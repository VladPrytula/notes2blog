{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac18faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Metal) backend is available and enabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Metal) backend is available and enabled.\")\n",
    "else:\n",
    "    print(\"❌ MPS not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109e5d9",
   "metadata": {},
   "source": [
    "### **Chapter 3: Neural Contextual Bandits: Generalization Through Shared Representation**\n",
    "\n",
    "#### **3.1 The Limits of Disjoint Models and the Need for Generalization**\n",
    "\n",
    "In our exploration thus far, we have journeyed through two fundamental paradigms of machine learning for recommendation. In Chapter 1, we constructed a `MLPRecommender`, a model endowed with the expressive power of deep learning. It learned rich, non-linear relationships from a large, static dataset, creating effective, generalizable embeddings for users and products. Its crucial flaw, however, was its static nature; it was a creature of the past, unable to adapt to new information without complete retraining.\n",
    "\n",
    "In Chapter 2, we addressed this inertia by introducing the `LinUCBAgent`, an online learning system grounded in the contextual bandit framework. This agent, learning at every interaction, demonstrated a remarkable ability to adapt and optimize its strategy over time, ultimately outperforming its static counterpart. Yet, it too possessed a critical architectural weakness. Our `LinUCBAgent` was a *disjoint model*. It maintained an independent set of parameters—a matrix $A_a$ and vector $b_a$—for each of the 50 products in our catalog.\n",
    "\n",
    "Consider the implications of this design. When the agent recommends \"Premium Dog Toy A\" and observes a click, it updates only the parameters for that specific toy. If it is subsequently asked to evaluate \"Deluxe Dog Toy B,\" it approaches the problem with no transference of knowledge. The insight that this user has an affinity for dog toys remains siloed within the parameters of \"Toy A.\" The model cannot **generalize**.\n",
    "\n",
    "This is not merely inefficient; it is fundamentally unscalable. In a real-world e-commerce setting with millions of products, maintaining a separate model for each item is computationally infeasible and statistically disastrous. The vast majority of items would be recommended so infrequently that their parameters would never be reliably estimated—a severe form of the cold-start problem.\n",
    "\n",
    "The path forward must therefore be a synthesis. We need a model that combines the adaptive, principled exploration of a bandit algorithm with the powerful, generalizable representation learning of a neural network. We seek a single, unified model that learns from every interaction to refine a shared understanding of the world, where learning about one dog toy informs its beliefs about *all* dog toys. This chapter is dedicated to the theory and implementation of such a model: the **Neural Bandit**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c830d1",
   "metadata": {},
   "source": [
    "#### **3.2 Generalizing the UCB Principle Beyond Linearity**\n",
    "\n",
    "To build our new agent, we must first return to the theoretical foundation of the UCB algorithm and abstract it away from the restrictive assumption of linearity.\n",
    "\n",
    "**Recap: The LinUCB Assumption**\n",
    "\n",
    "Recall from Chapter 2 that the LinUCB algorithm operates on a crucial assumption: the expected reward $r_{t,a}$ for a given arm $a$ at time $t$ is a linear function of its context feature vector $x_{t,a}$.\n",
    "\n",
    "**Definition 3.1: Linear Reward Model**\n",
    "Let $x_{t,a} \\in \\mathbb{R}^d$ be the feature vector for arm $a$ at time $t$. The LinUCB model assumes there exists an unknown but fixed parameter vector $\\theta_a^* \\in \\mathbb{R}^d$ such that the expected reward is:\n",
    "$$\n",
    "\\mathbb{E}[r_{t,a} | x_{t,a}] = (x_{t,a})^T \\theta_a^*\n",
    "$$\n",
    "\n",
    "From this assumption, we derived the UCB score, which was the sum of two terms: the *estimated reward* based on our learned $\\hat{\\theta}_a$, and an *exploration bonus* proportional to the uncertainty in that estimate.\n",
    "\n",
    "$$\n",
    "\\text{score}(a) = \\underbrace{(x_{t,a})^T \\hat{\\theta}_{t-1,a}}_{\\text{Exploitation}} + \\underbrace{\\alpha \\sqrt{(x_{t,a})^T A_{t-1,a}^{-1} x_{t,a}}}_{\\text{Exploration}}\n",
    "$$\n",
    "\n",
    "**Definition 3.2: LinUCB Payoff Score (Recap)**\n",
    "At the beginning of time step $t$, using parameters $(A_{t-1,a}, b_{t-1,a})$ estimated from all data up to step $t-1$, the payoff for arm $a$ is:\n",
    "$$\n",
    "p_{t,a} = \\underbrace{ (x_{t,a})^T \\hat{\\theta}_{t-1,a} }_{\\text{Exploitation}} + \\underbrace{ \\alpha \\sqrt{ (x_{t,a})^T A_{t-1,a}^{-1} x_{t,a} } }_{\\text{Exploration}}\n",
    "$$\n",
    "where $\\hat{\\theta}_{t-1,a} = A_{t-1,a}^{-1} b_{t-1,a}$.\n",
    "\n",
    "The brilliance of this formulation lies in its elegant, closed-form expression for uncertainty, derived from the properties of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e693a1",
   "metadata": {},
   "source": [
    "**The Generalization Step**\n",
    "\n",
    "Our goal is to replace the linear model $(x_{t,a})^T \\theta_a$ with a more powerful, non-linear function approximator, which we will represent as $f(x_{t,a})$. Let us imagine, for now, that this $f$ is an arbitrary function—in our case, it will be a neural network.\n",
    "\n",
    "The core UCB principle is not intrinsically tied to linearity. It is a general strategy for decision-making under uncertainty. We can state it more broadly:\n",
    "\n",
    "**Principle: The Generalized UCB**\n",
    "At each time step $t$, for each arm $a$, select the arm that maximizes the following payoff:\n",
    "$$\n",
    "\\text{select } a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left( \\hat{\\mu}_{t-1}(x_{t,a}) + \\kappa_{t-1}(x_{t,a}) \\right)\n",
    "$$\n",
    "where:\n",
    "*   $\\hat{\\mu}_{t-1}(x_{t,a})$ is the model's estimate of the mean reward for arm $a$ given features $x_{t,a}$, based on information up to step $t-1$.\n",
    "*   $\\kappa_{t-1}(x_{t,a})$ is the upper confidence bound, or \"exploration bonus,\" which quantifies the uncertainty of the estimate $\\hat{\\mu}$. It should be large when the model is uncertain and small when it is confident.\n",
    "\n",
    "This generalization presents us with a formidable new challenge. For linear models, the confidence bound $\\kappa$ can be derived analytically. But how do we compute a meaningful confidence bound for the output of a complex, non-linear model like a deep neural network? This is the central question that the NeuralUCB algorithm aims to answer.\n",
    "\n",
    "#### **3.3 NeuralUCB: Confidence Bounds via Gradient-based Approximation**\n",
    "\n",
    "The NeuralUCB algorithm provides an elegant and effective method for applying the UCB principle to neural network models. It acknowledges that while we cannot find an exact, closed-form confidence bound for the network's output, we can construct a powerful approximation by considering the network's behavior in the vicinity of its current parameterization.\n",
    "\n",
    "The core idea is to use a form of online ridge regression, not on the input features $x$ directly, but on the *gradient of the network's output with respect to its parameters*. This gradient acts as a high-dimensional, learned feature representation that captures the sensitivity of the model's prediction to changes in its weights.\n",
    "\n",
    "Let us formalize this.\n",
    "\n",
    "**Definition 3.2: The NeuralUCB Model**\n",
    "The NeuralUCB agent is defined by:\n",
    "1.  A neural network $g(x; \\theta)$ with parameters $\\theta \\in \\mathbb{R}^p$, where $p$ is the total number of trainable parameters in the network. This network takes a context feature vector $x$ as input and outputs a scalar prediction of the reward.\n",
    "2.  A shared $p \\times p$ matrix $A$ and a shared $p \\times 1$ vector $b$, which are analogous to the parameters in LinUCB but are now shared across all arms.\n",
    "\n",
    "Initially, at time $t=0$, we have:\n",
    "$$\n",
    "A_0 = \\lambda I_p \\quad \\text{(where } \\lambda > 0 \\text{ is a regularization parameter)}\n",
    "$$\n",
    "$$\n",
    "b_0 = \\mathbf{0}_{p \\times 1} \\quad \\text{(a zero vector of size p)}\n",
    "$$\n",
    "\n",
    "**The Prediction and Update Mechanism**\n",
    "\n",
    "At each time step $t$, the agent performs the following steps:\n",
    "\n",
    "1.  **Observe Context:** For the current user, the agent has access to a set of feature vectors $\\{x_{t,a}\\}_{a \\in \\mathcal{A}}$, one for each potential arm (product).\n",
    "2.  **Estimate Payoff:** For each arm $a$, the agent calculates a payoff $p_{t,a}$. This requires two components:\n",
    "    *   **a. Network Prediction (Exploitation):** First, it computes the network's current estimate of the reward, $g(x_{t,a}; \\hat{\\theta}_{t-1})$, where $\\hat{\\theta}_{t-1}$ are the network parameters estimated using data up to the previous step.\n",
    "    *   **b. Confidence Bound (Exploration):** It then computes the gradient of the network's output with respect to the parameters, evaluated at the current parameter estimate:\n",
    "        $$\n",
    "        \\nabla g_{t,a} = \\nabla_{\\theta} g(x_{t,a}; \\hat{\\theta}_{t-1})\n",
    "        $$\n",
    "        This gradient vector $\\nabla g_{t,a} \\in \\mathbb{R}^p$ is treated as the effective feature vector for the confidence bound calculation. The full payoff is:\n",
    "        $$\n",
    "        p_{t,a} = \\underbrace{g(x_{t,a}; \\hat{\\theta}_{t-1})}_{\\text{Exploitation}} + \\underbrace{\\alpha \\sqrt{ (\\nabla g_{t,a})^T A_{t-1}^{-1} (\\nabla g_{t,a}) }}_{\\text{Exploration}}\n",
    "        $$\n",
    "        where $\\alpha \\ge 0$ is the exploration hyperparameter.\n",
    "\n",
    "3.  **Select Arm:** The agent chooses the arm with the highest payoff:\n",
    "    $$\n",
    "    a_t = \\arg\\max_{a \\in \\mathcal{A}} p_{t,a}\n",
    "    $$\n",
    "\n",
    "4.  **Observe Reward & Update:** The agent plays arm $a_t$, observes the real-world reward $r_t$, and then updates its shared parameters:\n",
    "    *   Update the evidence matrix $A$ and reward vector $b$:\n",
    "        $$\n",
    "        A_t = A_{t-1} + (\\nabla g_{t,a_t}) (\\nabla g_{t,a_t})^T\n",
    "        $$\n",
    "        $$\n",
    "        b_t = b_{t-1} + r_t (\\nabla g_{t,a_t})\n",
    "        $$\n",
    "    *   Update the network's weights $\\theta$. This is typically done by performing one or more steps of gradient descent on a loss function. A common choice is the squared error between the prediction and the observed reward, potentially combined with a ridge penalty:\n",
    "        $$\n",
    "        \\mathcal{L}(\\theta) = (g(x_{t,a_t}; \\theta) - r_t)^2 + \\lambda ||\\theta||_2^2\n",
    "        $$\n",
    "        The parameters $\\hat{\\theta}_{t-1}$ are updated to $\\hat{\\theta}_t$ by optimizing this loss on the newly acquired data point $(x_{t,a_t}, r_t)$.\n",
    "\n",
    "**Remark 3.1: The Power of Shared Parameters**\n",
    "The significance of this formulation cannot be overstated. We now have a **single, shared** matrix $A$ and vector $b$ for the entire system. When the agent recommends product $a_t$ and receives reward $r_t$, the subsequent update to $A_t$, $b_t$, and $\\hat{\\theta}_t$ refines a global model of the world. This new knowledge is immediately available for evaluating *all other products*, even those in completely different categories. The model learns to generalize across the entire item space, elegantly solving the primary weakness of the disjoint LinUCB model. This architecture is both statistically efficient and computationally scalable.\n",
    "\n",
    "Now, let us proceed to implement this more sophisticated agent. We will begin by defining the neural network architecture that will serve as the core of our `NeuralUCBAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87b98b",
   "metadata": {},
   "source": [
    "### **3.4 Implementing the Neural Network Approximator**\n",
    "\n",
    "The text concludes by setting up our next task: implementing the agent. Before we can build the full `NeuralUCBAgent`, we must first construct its heart: the neural network $g(x; \\theta)$ that will act as our non-linear function approximator. Let's call this the `NeuralBanditNetwork`.\n",
    "\n",
    "#### **3.4.1 Architectural Considerations**\n",
    "\n",
    "**Intuition First:** Our goal for this network is to be a general-purpose learner. It must be powerful enough to capture complex, non-linear relationships between user and item features, yet simple enough for efficient online training where updates happen one data point at a time. We will employ a standard Multi-Layer Perceptron (MLP) architecture, similar in spirit to the one we built in Chapter 1, but with a crucial difference.\n",
    "\n",
    "Unlike our previous `MLPRecommender` which took discrete user and item *IDs* as input and learned embeddings from scratch, this new network will operate on rich *feature vectors*. This is the key design choice that allows it to generalize. By learning a function on the feature space itself, the model can make predictions for any user-item pair, as long as we can describe them with features. Learning that a user who likes \"Dog Toy\" features also responds well to a specific new product with \"Dog Toy\" features is now not only possible, but is the central mechanism of the model.\n",
    "\n",
    "We will design a simple MLP with two hidden layers and ReLU activation functions. The final output will be a single scalar, representing the network's estimate of the reward.\n",
    "\n",
    "**Formalizing the Implementation:** Let us now define the network in PyTorch. We will construct a class `NeuralBanditNetwork` that inherits from `torch.nn.Module`.\n",
    "\n",
    "**Code as a Didactic Tool: Dissection**\n",
    "\n",
    "Here is the structure of our network class. We will examine it piece by piece.\n",
    "\n",
    "```python\n",
    "# Introduce the core components first\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class NeuralBanditNetwork(nn.Module):\n",
    "    def __init__(self, feature_dim: int, hidden_dims: List[int] = [128, 64]):\n",
    "        super(NeuralBanditNetwork, self).__init__()\n",
    "        # ... layer definitions ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ... forward pass logic ...\n",
    "```\n",
    "\n",
    "Now, let us dissect the two primary methods, `__init__` and `forward`.\n",
    "\n",
    "**The `__init__` method:** This is the constructor for our module, where we define the layers of our network.\n",
    "\n",
    "*   `super(NeuralBanditNetwork, self).__init__()`: This is standard boilerplate to call the constructor of the parent class, `nn.Module`.\n",
    "*   `self.layers = nn.ModuleList()`: We use a `ModuleList` to hold our sequence of layers. This is a convenient way to manage a variable number of layers.\n",
    "*   The `for` loop constructs the network dynamically based on the `hidden_dims` list.\n",
    "    *   `self.layers.append(nn.Linear(input_dim, output_dim))`: For each step, it creates a `nn.Linear` layer. The first layer maps the `feature_dim` to the size of the first hidden layer. Subsequent layers map the previous hidden size to the next one.\n",
    "*   `self.output_layer = nn.Linear(hidden_dims[-1], 1)`: The final layer maps the last hidden dimension to a single scalar output.\n",
    "\n",
    "**Remark 3.2: Absence of a Final Sigmoid.**\n",
    "You will note that unlike in Chapter 1, we do **not** apply a sigmoid activation function to the final output. This is a deliberate and important choice. The UCB calculation relies on the properties of a linear model approximation. The raw output of the linear layer (a logit) is better suited for this than the bounded, non-linear output of a sigmoid. Forcing the output between 0 and 1 can squash the gradients and interfere with the delicate confidence bound estimation. We are predicting a raw score, not a probability.\n",
    "\n",
    "**The `forward` method:** This method defines the computational graph—how data flows through the network.\n",
    "\n",
    "*   `for layer in self.layers:`: It iterates through the hidden layers defined in our `ModuleList`.\n",
    "*   `x = F.relu(layer(x))`: For each layer, it performs the linear transformation and then applies the Rectified Linear Unit (ReLU) activation function. This introduces the crucial non-linearity that allows the network to learn complex patterns.\n",
    "*   `output = self.output_layer(x)`: Finally, it passes the result from the last hidden layer through the output layer to produce the single scalar prediction.\n",
    "\n",
    "**Assembling the Full Module**\n",
    "\n",
    "Putting this all together, the complete, self-contained, and runnable code for our network module is as follows.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class NeuralBanditNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron (MLP) to act as the function approximator\n",
    "    for the NeuralUCB agent.\n",
    "\n",
    "    The network takes a feature vector of a given dimension and outputs a single\n",
    "    scalar value representing the predicted reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int, hidden_dims: List[int] = [128, 64]):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "\n",
    "        Args:\n",
    "            feature_dim (int): The dimensionality of the input feature vector.\n",
    "            hidden_dims (List[int]): A list of integers, where each integer is the\n",
    "                                     size of a hidden layer.\n",
    "        \"\"\"\n",
    "        super(NeuralBanditNetwork, self).__init__()\n",
    "        \n",
    "        # We use a ModuleList to hold our sequence of layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Define the input layer\n",
    "        input_dim = feature_dim\n",
    "        \n",
    "        # Define the hidden layers dynamically\n",
    "        for h_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(input_dim, h_dim))\n",
    "            input_dim = h_dim # The next layer's input is the current layer's output\n",
    "            \n",
    "        # Define the output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, feature_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Pass through the hidden layers with ReLU activation\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        # Pass through the output layer (no activation)\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "```\n",
    "\n",
    "With this `NeuralBanditNetwork` defined, we now have the core component $g(x; \\theta)$. Our next step is to build the agent class, `NeuralUCBAgent`, which will instantiate this network and implement the full predict-and-update cycle described in Section 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96bd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class NeuralBanditNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron (MLP) to act as the function approximator\n",
    "    for the NeuralUCB agent.\n",
    "\n",
    "    The network takes a feature vector of a given dimension and outputs a single\n",
    "    scalar value representing the predicted reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int, hidden_dims: List[int] = [128, 64]):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "\n",
    "        Args:\n",
    "            feature_dim (int): The dimensionality of the input feature vector.\n",
    "            hidden_dims (List[int]): A list of integers, where each integer is the\n",
    "                                     size of a hidden layer.\n",
    "        \"\"\"\n",
    "        super(NeuralBanditNetwork, self).__init__()\n",
    "        \n",
    "        # We use a ModuleList to hold our sequence of layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Define the input layer\n",
    "        input_dim = feature_dim\n",
    "        \n",
    "        # Define the hidden layers dynamically\n",
    "        for h_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(input_dim, h_dim))\n",
    "            input_dim = h_dim # The next layer's input is the current layer's output\n",
    "            \n",
    "        # Define the output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, feature_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Pass through the hidden layers with ReLU activation\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        # Pass through the output layer (no activation)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a3914",
   "metadata": {},
   "source": [
    "### **3.5 Implementing the NeuralUCBAgent**\n",
    "\n",
    "With our `NeuralBanditNetwork` defined, we can now assemble the complete agent. This class, `NeuralUCBAgent`, will be the orchestrator. It will hold an instance of the network, manage the UCB-specific parameters ($A$ and $b$), and implement the core `predict` and `update` logic that defines the NeuralUCB algorithm.\n",
    "\n",
    "#### **3.5.1 The Agent's Architecture**\n",
    "\n",
    "**Intuition First:** The `NeuralUCBAgent` class is the brain of our operation. It must contain all the necessary components to execute the algorithm described in Section 3.3. Let's think about what it needs to store and what it needs to do.\n",
    "\n",
    "1.  **The Function Approximator:** It needs an instance of our `NeuralBanditNetwork`, which we will call `self.model`. This is the $g(x; \\theta)$ from our theory.\n",
    "2.  **The UCB Machinery:** It needs to store the shared matrix $A$ and vector $b$. These will be PyTorch tensors to integrate seamlessly with the rest of our PyTorch-based workflow. $A$ will be a $p \\times p$ matrix and $b$ a $p \\times 1$ vector, where $p$ is the total number of parameters in `self.model`.\n",
    "3.  **The Training Mechanism:** Since we update the network's parameters $\\theta$ at each step, the agent needs its own optimizer (we will use `Adam`) and a loss function (we will use Mean Squared Error, or `MSELoss`).\n",
    "4.  **Hyperparameters:** The agent must store the key hyperparameters that control its behavior: the exploration factor $\\alpha$ and the regularization parameter $\\lambda$.\n",
    "\n",
    "The agent's life cycle will be a loop:\n",
    "*   At prediction time, it will use the network for the exploitation term and the UCB machinery ($A$ and the gradients) for the exploration term to select an arm.\n",
    "*   At update time, it will use the observed reward to update both the UCB machinery *and* the network's parameters via one step of gradient descent.\n",
    "\n",
    "**Formalizing the Implementation: The `__init__` Method**\n",
    "\n",
    "Let us begin by constructing the agent's `__init__` method. This is where we will initialize all the components listed above.\n",
    "\n",
    "**Code as a Didactic Tool: Dissecting the Constructor**\n",
    "\n",
    "```python\n",
    "# Introduce the constructor first\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "\n",
    "class NeuralUCBAgent:\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 n_arms: int,\n",
    "                 hidden_dims: List[int] = [128, 64],\n",
    "                 lambda_: float = 1.0,\n",
    "                 alpha: float = 1.0,\n",
    "                 lr: float = 0.01):\n",
    "        # ... initialization logic ...\n",
    "```\n",
    "\n",
    "Now, let's dissect this constructor line by line.\n",
    "\n",
    "*   `model = NeuralBanditNetwork(feature_dim, hidden_dims)`: We instantiate our neural network.\n",
    "*   `self.p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)`: This is a crucial step. We calculate $p$, the total number of trainable parameters in our network. We iterate through all parameters in `self.model.parameters()` and sum up their number of elements (`p.numel()`). This gives us the dimensionality for our UCB machinery.\n",
    "*   `self.A = torch.eye(self.p) * lambda_`: We initialize the matrix $A$. As per the algorithm definition, it starts as a $p \\times p$ identity matrix, scaled by the regularization hyperparameter $\\lambda$. `torch.eye(self.p)` creates the identity matrix.\n",
    "*   `self.b = torch.zeros((self.p, 1))`: We initialize the vector $b$ as a zero vector of size $p \\times 1$.\n",
    "*   `self.optimizer = optim.Adam(self.model.parameters(), lr=lr)`: We set up the Adam optimizer, telling it which parameters it is responsible for optimizing (`self.model.parameters()`) and what learning rate (`lr`) to use.\n",
    "*   `self.loss_fn = nn.MSELoss()`: We define our loss function. The Mean Squared Error, $(y_{pred} - y_{true})^2$, is a natural choice for regressing onto a continuous reward signal.\n",
    "\n",
    "**Assembling the `__init__` Method**\n",
    "\n",
    "Here is the complete assembled class with explanations integrated as comments.\n",
    "\n",
    "```python\n",
    "class NeuralUCBAgent:\n",
    "    \"\"\"\n",
    "    Implements the NeuralUCB algorithm.\n",
    "\n",
    "    This agent uses a neural network to approximate the reward function and\n",
    "    calculates an upper confidence bound based on the gradient of the network's\n",
    "    output with respect to its parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 n_arms: int,\n",
    "                 hidden_dims: List[int] = [128, 64],\n",
    "                 lambda_: float = 1.0,\n",
    "                 alpha: float = 1.0,\n",
    "                 lr: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initializes the agent's components.\n",
    "\n",
    "        Args:\n",
    "            feature_dim (int): Dimensionality of the input context features.\n",
    "            n_arms (int): The number of arms (actions) available.\n",
    "            hidden_dims (List[int]): Sizes of the hidden layers for the network.\n",
    "            lambda_ (float): Regularization parameter for the A matrix.\n",
    "            alpha (float): Exploration-exploitation trade-off parameter.\n",
    "            lr (float): Learning rate for the network's optimizer.\n",
    "        \"\"\"\n",
    "        self.feature_dim = feature_dim\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_ # Store for potential future use\n",
    "\n",
    "        # 1. The Neural Network Function Approximator\n",
    "        self.model = NeuralBanditNetwork(feature_dim, hidden_dims)\n",
    "\n",
    "        # 2. The UCB Machinery\n",
    "        # Get the total number of trainable parameters in the model\n",
    "        self.p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # A is a (p x p) matrix, initialized to lambda * Identity\n",
    "        self.A = torch.eye(self.p) * self.lambda_\n",
    "        # b is a (p x 1) vector, initialized to zeros\n",
    "        self.b = torch.zeros((self.p, 1))\n",
    "\n",
    "        # 3. The Training Mechanism\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d6a4c",
   "metadata": {},
   "source": [
    "#### **3.5.2 The Prediction Mechanism**\n",
    "\n",
    "Now we arrive at the core of the algorithm: the `predict` method. This method must calculate the UCB score for every arm and select the best one.\n",
    "\n",
    "**Intuition First:** For each arm $a$, we need to compute $p_{t,a} = g(x_{t,a}) + \\alpha \\sqrt{ (\\nabla g_{t,a})^T A^{-1} (\\nabla g_{t,a}) }$. This involves two distinct steps for each arm: a forward pass through the network to get the exploitation term $g(x_{t,a})$, and a more complex calculation involving gradients and matrix inversion for the exploration term.\n",
    "\n",
    "**Formalizing the Implementation: The `predict` Method**\n",
    "\n",
    "Let's break down the required steps within the `predict` method.\n",
    "\n",
    "1.  **Prepare for Batch Prediction:** We will receive a list or tensor of feature vectors, one for each arm. Let's call this `feature_vectors`.\n",
    "2.  **Calculate Exploitation Term:** We can compute the predicted rewards for all arms in a single batch by passing `feature_vectors` through `self.model`. This is efficient.\n",
    "3.  **Calculate Exploration Term (Per Arm):** This is the most intricate part. For each arm $a$:\n",
    "    a.  We need the gradient $\\nabla_{\\theta} g(x_{t,a}; \\hat{\\theta})$. In PyTorch, this is not part of a standard forward pass. We must compute it explicitly. The function `torch.autograd.grad` is designed for this. It computes the gradient of a scalar output (our network's prediction for one arm) with respect to the model's parameters.\n",
    "    b.  We must flatten this gradient into a single $p \\times 1$ vector to match the dimensions of $A$ and $b$.\n",
    "    c.  We need to compute $A^{-1}$. We will use `torch.inverse(self.A)`.\n",
    "    d.  We perform the quadratic form multiplication: $(\\nabla g)^T A^{-1} (\\nabla g)$.\n",
    "    e.  We take the square root and multiply by $\\alpha$.\n",
    "4.  **Combine and Select:** We add the exploitation and exploration terms for each arm to get the final UCB scores, and then select the arm with the highest score using `torch.argmax`.\n",
    "\n",
    "**Code as a Didactic Tool: Dissecting the `predict` Method**\n",
    "\n",
    "```python\n",
    "def predict(self, feature_vectors: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Selects an arm based on the NeuralUCB scoring rule.\n",
    "\n",
    "    Args:\n",
    "        feature_vectors (torch.Tensor): A tensor of shape (n_arms, feature_dim)\n",
    "                                        containing the context for each arm.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the chosen arm.\n",
    "    \"\"\"\n",
    "    # Ensure model is in evaluation mode for prediction\n",
    "    self.model.eval()\n",
    "\n",
    "    # We will store gradients for the update step to avoid re-computation\n",
    "    self.arm_gradients = []\n",
    "    ucb_scores = torch.zeros(self.n_arms)\n",
    "\n",
    "    # Pre-calculate the inverse of A for efficiency in the loop\n",
    "    A_inv = torch.inverse(self.A)\n",
    "\n",
    "    # --- Step 1: Calculate Exploitation Term for all arms in a batch ---\n",
    "    # The model's raw prediction is the exploitation score\n",
    "    with torch.no_grad(): # No need to track gradients for this part\n",
    "        exploit_scores = self.model(feature_vectors).squeeze()\n",
    "\n",
    "    # --- Step 2 & 3: Calculate Exploration Term for each arm ---\n",
    "    for i in range(self.n_arms):\n",
    "        x_i = feature_vectors[i].unsqueeze(0) # Shape: (1, feature_dim)\n",
    "        \n",
    "        # We need to compute gradients, so we clear any old ones\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Forward pass for a single arm to get a scalar output\n",
    "        pred_i = self.model(x_i)\n",
    "        \n",
    "        # Calculate the gradient of the output w.r.t. model parameters\n",
    "        # This is the \\nabla g_{t,a} term\n",
    "        grads = torch.autograd.grad(pred_i, self.model.parameters())\n",
    "        \n",
    "        # Flatten the gradients into a single vector of size (p, 1)\n",
    "        g_i = torch.cat([g.view(-1) for g in grads]).view(-1, 1)\n",
    "        self.arm_gradients.append(g_i) # Save for the update step\n",
    "\n",
    "        # Calculate the exploration bonus: alpha * sqrt(g_i^T * A_inv * g_i)\n",
    "        explore_bonus = self.alpha * torch.sqrt(g_i.T @ A_inv @ g_i)\n",
    "        \n",
    "        # --- Step 4: Combine and store score ---\n",
    "        ucb_scores[i] = exploit_scores[i] + explore_bonus\n",
    "        \n",
    "    # Select the arm with the highest UCB score\n",
    "    chosen_arm = torch.argmax(ucb_scores).item()\n",
    "    return chosen_arm\n",
    "```\n",
    "\n",
    "**Remark 3.3: A Note on Efficiency.**\n",
    "In the code above, we loop through each arm to calculate its gradient. While correct, this can be computationally intensive if the number of arms is very large. Advanced implementations might use techniques like Jacobian-vector products to speed this up, but for initial try and test solution, this explicit loop is superior as it perfectly mirrors the per-arm formula from the theory. We also save the computed gradients in `self.arm_gradients` to reuse in the `update` step, avoiding redundant computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eaedfa",
   "metadata": {},
   "source": [
    "### **3.6 A Pedagogical Deep Dive: Scaling Exploration to Millions of Arms**\n",
    "\n",
    "The `predict` method we developed in Section 3.5.2 is correct, but it contains a hidden computational bottleneck that renders it impractical for real-world systems with large action spaces. This section is dedicated to dissecting this bottleneck, understanding its mathematical structure, and implementing a vastly more efficient, vectorized solution.\n",
    "\n",
    "#### **3.6.1 The \"K-Pass Problem\": Analyzing the Naive Loop**\n",
    "\n",
    "**Intuition First:** Let us reconsider the loop at the heart of our `predict` method:\n",
    "\n",
    "```python\n",
    "# The naive loop from our previous implementation\n",
    "for i in range(self.n_arms):\n",
    "    # ...\n",
    "    grads = torch.autograd.grad(pred_i, self.model.parameters())\n",
    "    # ...\n",
    "```\n",
    "\n",
    "The call to `torch.autograd.grad` is the workhorse of automatic differentiation in PyTorch. To compute the gradient of a scalar output with respect to all model parameters, it essentially performs a **backward pass** through the computation graph, propagating gradients from the output back to the parameters using the chain rule.\n",
    "\n",
    "Our loop iterates `n_arms` times. Let us denote the number of arms by $K$. This means that to select a single arm, our agent performs **one forward pass** (to get all exploitation scores) and **$K$ separate backward passes** (one for each arm's gradient). This is what we shall call the **K-Pass Problem**. If our e-commerce site has $K=50,000$ products, this approach would require 50,000 backward passes just to make one recommendation. This is not merely inefficient; it is computationally non-viable.\n",
    "\n",
    "Our goal is to reformulate the exploration bonus calculation to eliminate this loop, leveraging the power of batched matrix operations.\n",
    "\n",
    "#### **3.6.2 The Vectorized Solution via the Jacobian Matrix**\n",
    "\n",
    "**Formalizing with Unyielding Rigor:** The key to a batched computation lies in understanding the mathematical object that our loop was constructing piece by piece.\n",
    "\n",
    "**Definition 3.3: The Output-Parameter Jacobian**\n",
    "Let our neural network be the function $g(x; \\theta)$, where $x$ is an input feature vector and $\\theta \\in \\mathbb{R}^p$ is the vector of all $p$ model parameters. When we present a batch of $K$ feature vectors, $\\{x_1, x_2, \\dots, x_K\\}$, the network produces a vector of $K$ scalar reward predictions, $\\hat{\\mathbf{r}} \\in \\mathbb{R}^K$, where $\\hat{r}_i = g(x_i; \\theta)$.\n",
    "\n",
    "The **Jacobian** of the network's output vector $\\hat{\\mathbf{r}}$ with respect to the parameter vector $\\theta$ is a $K \\times p$ matrix, denoted by $J$, where each entry $J_{ij}$ is the partial derivative of the $i$-th output with respect to the $j$-th parameter:\n",
    "$$\n",
    "J_{ij} = \\frac{\\partial \\hat{r}_i}{\\partial \\theta_j}\n",
    "$$\n",
    "The $i$-th row of this Jacobian, $J_{i,:}$, is therefore the transpose of the gradient vector $\\nabla_{\\theta} g(x_i; \\theta)$ that we were calculating inside our loop.\n",
    "$$\n",
    "J =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\hat{r}_1}{\\partial \\theta_1} & \\frac{\\partial \\hat{r}_1}{\\partial \\theta_2} & \\cdots & \\frac{\\partial \\hat{r}_1}{\\partial \\theta_p} \\\\\n",
    "\\frac{\\partial \\hat{r}_2}{\\partial \\theta_1} & \\frac{\\partial \\hat{r}_2}{\\partial \\theta_2} & \\cdots & \\frac{\\partial \\hat{r}_2}{\\partial \\theta_p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\hat{r}_K}{\\partial \\theta_1} & \\frac{\\partial \\hat{r}_K}{\\partial \\theta_2} & \\cdots & \\frac{\\partial \\hat{r}_K}{\\partial \\theta_p}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "— (\\nabla_{\\theta} g(x_1; \\theta))^T — \\\\\n",
    "— (\\nabla_{\\theta} g(x_2; \\theta))^T — \\\\\n",
    "\\vdots \\\\\n",
    "— (\\nabla_{\\theta} g(x_K; \\theta))^T —\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now, let us rewrite the exploration bonus for a single arm $i$ using this notation. Let $g_i = \\nabla_{\\theta} g(x_i; \\theta)$ be the gradient (a column vector of size $p \\times 1$). The squared exploration bonus is $(g_i)^T A^{-1} g_i$. This is exactly the term $(J_{i,:}) A^{-1} (J_{i,:})^T$.\n",
    "\n",
    "How can we compute this for all $K$ arms at once? Consider the matrix product $M = J A^{-1} J^T$.\n",
    "\n",
    "*   $J$ has dimensions $(K \\times p)$.\n",
    "*   $A^{-1}$ has dimensions $(p \\times p)$.\n",
    "*   $J^T$ has dimensions $(p \\times K)$.\n",
    "*   The resulting matrix $M$ has dimensions $(K \\times K)$.\n",
    "\n",
    "Let's examine the element $M_{ii}$ on the diagonal of this matrix.\n",
    "$$\n",
    "M_{ii} = (\\text{i-th row of } J) \\cdot (\\text{i-th column of } A^{-1} J^T)\n",
    "$$\n",
    "By the rules of matrix multiplication, the $i$-th column of $A^{-1} J^T$ is $A^{-1}$ multiplied by the $i$-th column of $J^T$. The $i$-th column of $J^T$ is simply the vector $g_i$.\n",
    "$$\n",
    "M_{ii} = (J_{i,:}) \\cdot (A^{-1} g_i) = (g_i)^T A^{-1} g_i\n",
    "$$\n",
    "\n",
    "**Theorem 3.1: Batched Exploration Bonus Calculation**\n",
    "The squared exploration bonuses for all $K$ arms are the diagonal elements of the matrix product $J A^{-1} J^T$, where $J$ is the Jacobian of the network outputs with respect to the parameters.\n",
    "$$\n",
    "\\left[ (\\text{bonus}_1)^2, \\dots, (\\text{bonus}_K)^2 \\right] = \\text{diag} \\left( J A^{-1} J^T \\right)\n",
    "$$\n",
    "\n",
    "This is our breakthrough. If we can compute the entire Jacobian $J$ efficiently, we can replace the $K$-Pass loop with a series of highly optimized matrix multiplications.\n",
    "\n",
    "#### **3.6.3 Implementation \"From Scratch\" with Modern PyTorch**\n",
    "\n",
    "**Code as a Didactic Tool:** How do we compute the full Jacobian $J$ in PyTorch without a loop? The standard `torch.autograd.grad` is not designed for this; it computes Vector-Jacobian products. However, the modern functional API, `torch.func`, provides exactly the tool we need: `vmap` (vectorized map).\n",
    "\n",
    "`vmap` is a function transformer. It takes a function that operates on a single sample and returns a new function that operates on a batch of samples, handling the batching dimension automatically. We can write a simple function to get the gradient for one arm, and `vmap` will \"lift\" it into a fully vectorized version that computes the Jacobian for all arms at once.\n",
    "\n",
    "Let's build a new, efficient `predict_vectorized` method.\n",
    "\n",
    "**Dissection of the Vectorized `predict` Method**\n",
    "\n",
    "1.  **Define a per-sample gradient function:** We'll define a small helper function `get_grad` that takes a single feature vector and computes the flattened gradient vector, just as we did inside the loop before.\n",
    "2.  **Vectorize it with `vmap`:** We will use `torch.func.vmap(get_grad)` to create a new function, `batched_get_grad`, which will take the entire batch of feature vectors and return the full Jacobian matrix $J$.\n",
    "3.  **Perform matrix operations:** We will implement the formula from Theorem 3.1. A particularly efficient way to compute `diag(J @ A_inv @ J.T)` is with `torch.einsum`, which allows for expressive and optimized tensor contractions. The expression `torch.einsum('ij,ji->i', J @ A_inv, J.T)` computes the matrix product and extracts the diagonal in a single, highly optimized step.\n",
    "\n",
    "**Assembling the New Method**\n",
    "\n",
    "First, ensure you have the necessary functional tools. In recent PyTorch versions, this is `torch.func`.\n",
    "\n",
    "```python\n",
    "# We need vmap and functional_call from torch.func\n",
    "from torch.func import vmap, functional_call\n",
    "\n",
    "# This new method will replace our old 'predict' method in the NeuralUCBAgent class.\n",
    "def predict_vectorized(self, feature_vectors: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Selects an arm using a vectorized calculation of the NeuralUCB score.\n",
    "    This method avoids explicit loops over arms for gradient calculation.\n",
    "\n",
    "    Args:\n",
    "        feature_vectors (torch.Tensor): Shape (n_arms, feature_dim).\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the chosen arm.\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    n_arms = feature_vectors.shape[0]\n",
    "\n",
    "    # --- Step 1: Exploitation Term (same as before) ---\n",
    "    exploit_scores = self.model(feature_vectors).squeeze()\n",
    "\n",
    "    # --- Step 2: Vectorized Exploration Term ---\n",
    "    \n",
    "    # Pre-calculate the inverse of A\n",
    "    A_inv = torch.inverse(self.A)\n",
    "\n",
    "    # We need to define a function that computes the gradient for a *single* sample.\n",
    "    # It must be self-contained, so we pass model params and buffers to it.\n",
    "    params = {k: v.detach() for k, v in self.model.named_parameters()}\n",
    "    buffers = {k: v.detach() for k, v in self.model.named_buffers()}\n",
    "\n",
    "    def compute_grad(x_single):\n",
    "        # Use functional_call to run the model with specific params/buffers\n",
    "        # on a single input x_single, which has shape (feature_dim,)\n",
    "        pred = functional_call(self.model, (params, buffers), args=(x_single.unsqueeze(0),))\n",
    "        \n",
    "        # Compute gradient of this single prediction w.r.t. parameters\n",
    "        grads = torch.autograd.grad(pred, params.values())\n",
    "        \n",
    "        # Flatten and concatenate into a single vector\n",
    "        return torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "    # Use vmap to vectorize our compute_grad function.\n",
    "    # It will now accept a batch of feature vectors and return a batch of gradients (the Jacobian).\n",
    "    # The result, J, will have shape (n_arms, p).\n",
    "    J = vmap(compute_grad)(feature_vectors)\n",
    "    \n",
    "    # Save the Jacobian for the update step. Note its shape.\n",
    "    self.last_jacobian = J\n",
    "\n",
    "    # --- Step 3: Batched bonus calculation using einsum ---\n",
    "    # This is the efficient implementation of diag(J @ A_inv @ J.T)\n",
    "    # 'ij,ji->i' means: multiply matrix J@A_inv (ij) with J.T (ji)\n",
    "    # and sum over the common dimension j, keeping only the diagonal elements (i).\n",
    "    bonus_squared = torch.einsum('ij,ji->i', J @ A_inv, J.T)\n",
    "    \n",
    "    # Clamp to avoid numerical issues with negative values from floating point errors\n",
    "    explore_bonuses = self.alpha * torch.sqrt(torch.clamp(bonus_squared, min=0))\n",
    "\n",
    "    # --- Step 4: Combine and Select ---\n",
    "    ucb_scores = exploit_scores + explore_bonuses\n",
    "    chosen_arm = torch.argmax(ucb_scores).item()\n",
    "    \n",
    "    return chosen_arm\n",
    "```\n",
    "**Remark 3.4: A Note on the Update Step.**\n",
    "Our `update` method will also need to be adjusted. It previously relied on a single gradient vector `g_i`. With this new `predict_vectorized` method, we have the full Jacobian `J` available. The update will now use the specific row of the Jacobian corresponding to the arm that was chosen, e.g., `g_t = self.last_jacobian[chosen_arm].view(-1, 1)`. This avoids re-computing any gradients.\n",
    "\n",
    "#### **3.6.4 Analysis: The Scalability Trade-off**\n",
    "\n",
    "We have replaced a method that was slow in computation time with one that is fast. However, we must always analyze the resources we are using, particularly memory.\n",
    "\n",
    "**Naive Loop (K-Pass) Method**\n",
    "*   **Pros:**\n",
    "    *   **Low Memory:** Requires storing only one gradient vector ($p \\times 1$) at a time. Memory usage is independent of the number of arms $K$.\n",
    "    *   **Simple to Implement:** The logic directly mirrors the mathematical formula for a single arm.\n",
    "*   **Cons:**\n",
    "    *   **Computationally Slow:** Time complexity is roughly $O(K \\cdot C_{backward})$, where $C_{backward}$ is the cost of a single backward pass. This scales linearly and poorly with the number of arms.\n",
    "\n",
    "**Vectorized Jacobian Method**\n",
    "*   **Pros:**\n",
    "    *   **Computationally Fast:** Leverages highly optimized BLAS/cuBLAS libraries for matrix operations. The time complexity is dominated by the Jacobian computation and matrix multiplies, but the constant factors are much smaller, leading to massive speedups in practice.\n",
    "*   **Cons:**\n",
    "    *   **High Memory:** The primary drawback. It requires instantiating the full Jacobian matrix $J$ in memory. The size of this matrix is $K \\times p$. For a system with $K=1,000,000$ arms and a network with $p=100,000$ parameters, the Jacobian would require $10^6 \\times 10^5 \\times 4$ bytes $\\approx 400$ Gigabytes of memory. This is prohibitive.\n",
    "\n",
    "**Conclusion and Path to True Scale**\n",
    "\n",
    "The vectorized Jacobian method is a monumental improvement and is the correct approach for problems where the number of arms is in the hundreds or a few thousands, as in our textbook simulation.\n",
    "\n",
    "For **millions of SKUs**, neither method is sufficient on its own. The K-Pass method is too slow, and the Jacobian method uses too much memory. Real-world large-scale bandit systems employ a hybrid, two-stage approach:\n",
    "\n",
    "1.  **Candidate Generation (Retrieval):** A fast, approximate model (e.g., a two-tower model using embedding similarity, or even a simple business-rules engine) is used to select a small candidate set of a few hundred promising items from the millions available.\n",
    "2.  **Re-Ranking:** The sophisticated `NeuralUCBAgent`, using the efficient vectorized `predict` method, is then applied *only to this small candidate set*.\n",
    "\n",
    "This two-stage process combines the scalability of approximate retrieval with the principled exploration of a UCB algorithm. It is the dominant paradigm in industrial applications. For the remainder of our chapter, we will proceed with the superior vectorized implementation, keeping in mind that this candidate generation step is the necessary bridge to planet-scale systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c238572",
   "metadata": {},
   "source": [
    "### **3.7 Pedagogical Deep Dive: Advanced PyTorch for Efficient Computation**\n",
    "\n",
    "In our quest for a scalable `predict` method, we employed two powerful, modern PyTorch features: `torch.func.vmap` and `torch.einsum`. These are not mere syntactic sugar; they represent a more functional and declarative paradigm for tensor computation that can lead to significant gains in both code clarity and performance. Let us dissect each one to build a deep, intuitive understanding.\n",
    "\n",
    "#### **3.7.1 `torch.func.vmap`: The Vectorizing Map**\n",
    "\n",
    "**Intuition First:** Imagine you have written a function that works perfectly on a single data point. For example, a function `normalize(v)` that takes a vector `v` and scales it to have a unit norm.\n",
    "\n",
    "```python\n",
    "# A simple function for a single tensor\n",
    "def normalize(v: torch.Tensor) -> torch.Tensor:\n",
    "    return v / torch.linalg.norm(v)\n",
    "\n",
    "# Works on a single vector:\n",
    "my_vector = torch.tensor([3.0, 4.0])\n",
    "print(normalize(my_vector)) # tensor([0.6000, 0.8000])\n",
    "```\n",
    "\n",
    "Now, what if you have a *batch* of vectors, say a tensor of shape `(10, 2)`, and you want to normalize each of the 10 vectors independently? The conventional approach is to write a `for` loop:\n",
    "\n",
    "```python\n",
    "# The conventional, manual batching approach\n",
    "batched_vectors = torch.randn(10, 2)\n",
    "results = [normalize(v) for v in batched_vectors]\n",
    "results_tensor = torch.stack(results)\n",
    "```\n",
    "\n",
    "This works, but it's verbose and, more importantly, it's a Python loop. Python loops are notoriously slow compared to optimized, compiled C++ or CUDA kernels that operate on entire tensors at once. `vmap` (vectorizing map) is designed to solve this exact problem.\n",
    "\n",
    "**Formalizing `vmap`**\n",
    "\n",
    "`vmap` is a higher-order function. It takes a function $f$ as input and returns a *new* function `f_vmapped` that is semantically equivalent to running $f$ in a loop over a batch dimension, but is executed using a single, efficient, vectorized kernel.\n",
    "\n",
    "**Definition 3.4: `vmap` Transformation**\n",
    "Let $f: (A, B, ...) -> C$ be a function that takes tensors $A$, $B$, etc., as input and returns a tensor $C$. The `vmap` transformation, `torch.func.vmap(f, in_dims, out_dims)`, creates a new function `f_vmapped`.\n",
    "*   `in_dims`: A tuple specifying which dimension of each input tensor should be treated as the \"batch\" dimension to be mapped over.\n",
    "*   `out_dims`: An integer specifying where the batch dimension should appear in the output.\n",
    "\n",
    "Let's apply this to our `normalize` example.\n",
    "\n",
    "```python\n",
    "from torch.func import vmap\n",
    "\n",
    "# Our original function is unchanged. It remains blissfully unaware of batches.\n",
    "def normalize(v: torch.Tensor) -> torch.Tensor:\n",
    "    return v / torch.linalg.norm(v)\n",
    "\n",
    "# Create a vectorized version of normalize.\n",
    "# in_dims=0 means: \"map over dimension 0 of the first input\".\n",
    "vectorized_normalize = vmap(normalize, in_dims=0)\n",
    "\n",
    "# Now, apply it directly to the batch of vectors\n",
    "batched_vectors = torch.randn(10, 2)\n",
    "# No loop needed!\n",
    "results_tensor = vectorized_normalize(batched_vectors) \n",
    "# The result has shape (10, 2), as expected.\n",
    "```\n",
    "\n",
    "**How `vmap` Enabled Our `predict_vectorized` Method**\n",
    "\n",
    "In Section 3.6.3, we needed to compute the gradient of the network's output with respect to its parameters for *each arm separately*. Our core function, `compute_grad`, was designed to do this for a single arm's feature vector.\n",
    "\n",
    "```python\n",
    "# The function for a single sample\n",
    "def compute_grad(x_single):\n",
    "    # ... calculates gradient for one feature vector ...\n",
    "    return flattened_gradient # Shape (p,)\n",
    "```\n",
    "\n",
    "By wrapping it with `vmap`, `J = vmap(compute_grad)(feature_vectors)`, we instructed PyTorch to:\n",
    "1.  Look at the input `feature_vectors`, which has shape `(n_arms, feature_dim)`.\n",
    "2.  Recognize that `in_dims=0` (the default) corresponds to the `n_arms` dimension.\n",
    "3.  Execute the logic of `compute_grad` for each of the `n_arms` \"slices\" along that dimension.\n",
    "4.  Stack the resulting `p`-dimensional gradient vectors into a single tensor.\n",
    "5.  The final result is the Jacobian matrix $J$ of shape `(n_arms, p)`, computed without a single Python loop.\n",
    "\n",
    "#### **3.7.2 `torch.einsum`: Tensor Operations made readable**\n",
    "\n",
    "**Intuition First:** Many common tensor operations are specific instances of a more general concept: **tensor contraction**. This involves multiplying elements and summing them up along specified dimensions. For example:\n",
    "*   **Vector dot product:** `torch.dot(a, b)` $\\rightarrow \\sum_i a_i b_i$\n",
    "*   **Matrix-vector multiplication:** `torch.mv(M, v)` $\\rightarrow y_i = \\sum_j M_{ij} v_j$\n",
    "*   **Matrix-matrix multiplication:** `torch.matmul(A, B)` $\\rightarrow C_{ik} = \\sum_j A_{ij} B_{jk}$\n",
    "*   **Matrix transpose:** `A.T` $\\rightarrow A'_{ij} = A_{ji}$\n",
    "\n",
    "`torch.einsum` (Einstein summation convention) provides a single, powerful \"mini-language\" to express all of these operations and more. It works by specifying the dimensions of the input tensors and how they should be combined to form the output tensor.\n",
    "\n",
    "**Formalizing `einsum`**\n",
    "\n",
    "The `einsum` function takes a string equation and a sequence of tensors.\n",
    "`torch.einsum(\"equation\", tensor1, tensor2, ...)`\n",
    "\n",
    "The equation string is composed of two parts, separated by `->`: `input_subscripts -> output_subscripts`.\n",
    "*   Each input tensor is assigned a subscript string (e.g., `ij`, `jk`). The letters represent dimensions.\n",
    "*   **Contraction:** Dimensions that appear in the input subscripts but *not* in the output subscript are summed over (or \"contracted\").\n",
    "*   **Output Shape:** The output subscripts define the dimensions of the resulting tensor.\n",
    "\n",
    "**A Pedagogical Example: From Simple to Complex**\n",
    "\n",
    "Let's build up our understanding with examples. Assume `A` is a `(3, 4)` matrix, `B` is a `(4, 5)` matrix, and `v` is a `(4,)` vector.\n",
    "\n",
    "1.  **Transpose:** `torch.einsum('ij->ji', A)`\n",
    "    *   Input `A` has dimensions `i` and `j`.\n",
    "    *   Output has dimensions `j` and `i`.\n",
    "    *   The mapping is clear: the output at `(j, i)` is the input from `(i, j)`. This is a transpose. Result shape: `(4, 3)`.\n",
    "\n",
    "2.  **Matrix-Vector Multiplication:** `torch.einsum('ij,j->i', A, v)`\n",
    "    *   Input `A` has dims `ij`, input `v` has dim `j`.\n",
    "    *   The dimension `j` is repeated in the inputs but does not appear in the output `i`. It is therefore the dimension to be summed over.\n",
    "    *   This computes $\\sum_j A_{ij} v_j$, which is the definition of matrix-vector product. Result shape: `(3,)`.\n",
    "\n",
    "3.  **Matrix-Matrix Multiplication:** `torch.einsum('ij,jk->ik', A, B)`\n",
    "    *   Inputs have dims `ij` and `jk`. Output has dims `ik`.\n",
    "    *   `j` is the repeated dimension, so it is summed over.\n",
    "    *   This computes $\\sum_j A_{ij} B_{jk}$, the definition of matrix multiplication. Result shape: `(3, 5)`.\n",
    "\n",
    "**How `einsum` Enabled Our `predict_vectorized` Method**\n",
    "\n",
    "Our goal was to compute the diagonal of the matrix product $M = J A^{-1} J^T$.\n",
    "*   Let `J_A_inv = J @ A_inv`. This intermediate matrix has shape `(K, p)`, which we can label `ij`.\n",
    "*   The second matrix is `J.T`, which has shape `(p, K)`, which we can label `ji`.\n",
    "*   The full product is `(J @ A_inv) @ J.T`. In `einsum` notation, this is `'ij,ji->ii'`. Let's break this down:\n",
    "    *   `ij,ji`: Multiply the `(i,j)` element of the first matrix by the `(j,i)` element of the second.\n",
    "    *   `->ii`: Sum over the repeated index `j` and keep the index `i`. The `ii` in the output is a special `einsum` syntax that means \"give me only the elements where the row index equals the column index\"—in other words, the **diagonal**.\n",
    "*   This is almost perfect. However, a slightly more common and sometimes more optimized way to write this is `'ij,ji->i'`.\n",
    "    *   `'ij,ji->i'`: This still computes the sum over `j`. For each `i`, it calculates $\\sum_j (J A^{-1})_{ij} (J^T)_{ji}$. Since $(J^T)_{ji} = J_{ij}$, this is $\\sum_j (J A^{-1})_{ij} J_{ij}$. This is precisely the definition of the dot product of the i-th row of `J @ A_inv` with the i-th row of `J`, which is exactly the i-th diagonal element of the final matrix product.\n",
    "\n",
    "The expression `torch.einsum('ij,ji->i', J @ A_inv, J.T)` is therefore a remarkably concise, expressive, and efficient way to state: \"Perform a matrix multiplication and return only the diagonal of the result.\" It replaces multiple lines of code and potential intermediate memory allocations with a single, optimized operation.\n",
    "\n",
    "With this deeper understanding of `vmap` and `einsum`, the elegance and power of the `predict_vectorized` method should be much clearer. We are now ready to complete our agent by implementing the final `update` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e2f1a",
   "metadata": {},
   "source": [
    "#### **3.7.3 The Update Mechanism**\n",
    "\n",
    "The final piece of our `NeuralUCBAgent` is the `update` method. This method is called after an arm is chosen and a reward is observed. Its purpose is twofold:\n",
    "1.  Update the UCB machinery ($A$ and $b$) using the gradient of the chosen arm.\n",
    "2.  Update the neural network's parameters $\\theta$ by performing one step of gradient descent.\n",
    "\n",
    "**Intuition First:** The update rule for $A$ and $b$ is identical to the one in the original paper. We need the gradient vector $g_{t,a_t}$ corresponding to the arm $a_t$ that was actually played at time $t$. We then use this vector to update our evidence matrix $A$ and our weighted reward vector $b$.\n",
    "\n",
    "Simultaneously, we must train the network itself. The network made a prediction, $g(x_{t,a_t}; \\hat{\\theta}_{t-1})$, and we received a true reward, $r_t$. The discrepancy between these two values forms the basis of our learning signal. We will compute the Mean Squared Error (MSE) loss between the prediction and the reward and use our optimizer to take a single step to minimize this loss, nudging the network's parameters in a direction that would have made its prediction more accurate.\n",
    "\n",
    "**Formalizing the Implementation: The `update` Method**\n",
    "\n",
    "Let's construct the `update` method. It will take the chosen arm's index, the feature vector for that arm, and the observed reward as input.\n",
    "\n",
    "**Dissecting the `update` Method**\n",
    "\n",
    "We will build two versions of the `update` method. The first corresponds to our initial naive `predict` loop, and the second corresponds to the more efficient `predict_vectorized` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01217ac1",
   "metadata": {},
   "source": [
    "**Version 1: Update for the Loop-based `predict`**\n",
    "\n",
    "This version assumes that during the `predict` call, we computed and stored the gradient for the chosen arm.\n",
    "\n",
    "```python\n",
    "# This update method pairs with the original, loop-based predict method.\n",
    "def update_loop(self, chosen_arm: int, reward: float):\n",
    "    \"\"\"\n",
    "    Updates the agent's parameters after observing a reward.\n",
    "    Assumes self.arm_gradients was populated by a recent call to the\n",
    "    loop-based predict method.\n",
    "\n",
    "    Args:\n",
    "        chosen_arm (int): The index of the arm that was played.\n",
    "        reward (float): The observed reward.\n",
    "    \"\"\"\n",
    "    # Ensure model is in training mode for the update\n",
    "    self.model.train()\n",
    "    \n",
    "    # --- Step 1: Update the UCB machinery (A and b) ---\n",
    "    # Retrieve the pre-computed gradient for the chosen arm\n",
    "    g_t = self.arm_gradients[chosen_arm] # Shape (p, 1)\n",
    "    \n",
    "    # Update A: A_t = A_{t-1} + g_t * g_t^T\n",
    "    self.A += g_t @ g_t.T\n",
    "    \n",
    "    # Update b: b_t = b_{t-1} + r_t * g_t\n",
    "    self.b += reward * g_t\n",
    "    \n",
    "    # --- Step 2: Update the network's parameters ---\n",
    "    # The feature vector for the chosen arm is needed for the loss calculation.\n",
    "    # Let's assume it was saved from the predict call.\n",
    "    # x_chosen = self.last_feature_vectors[chosen_arm].unsqueeze(0)\n",
    "    \n",
    "    # This reveals a flaw: we didn't save the features. Let's adjust the\n",
    "    # method signature to be more robust.\n",
    "\n",
    "    # Revised, more robust signature:\n",
    "def update(self, chosen_arm_idx: int, feature_vector: torch.Tensor, reward: float):\n",
    "    \"\"\"\n",
    "    Updates the agent's parameters after observing a reward.\n",
    "\n",
    "    Args:\n",
    "        chosen_arm_idx (int): The index of the arm that was played.\n",
    "        feature_vector (torch.Tensor): The feature vector for the CHOSEN arm.\n",
    "                                       Shape (feature_dim,) or (1, feature_dim).\n",
    "        reward (float): The observed reward.\n",
    "    \"\"\"\n",
    "    self.model.train()\n",
    "    \n",
    "    # Ensure feature_vector is correctly shaped\n",
    "    if feature_vector.dim() == 1:\n",
    "        feature_vector = feature_vector.unsqueeze(0) # Shape: (1, feature_dim)\n",
    "\n",
    "    # --- Step 1: Update the UCB machinery (A and b) ---\n",
    "    # For this version, we must re-calculate the gradient for the chosen arm.\n",
    "    self.model.zero_grad()\n",
    "    prediction = self.model(feature_vector)\n",
    "    grads = torch.autograd.grad(prediction, self.model.parameters())\n",
    "    g_t = torch.cat([g.view(-1) for g in grads]).view(-1, 1)\n",
    "\n",
    "    self.A += g_t @ g_t.T\n",
    "    self.b += reward * g_t\n",
    "\n",
    "    # --- Step 2: Update the network's parameters ---\n",
    "    # The `prediction` tensor from above still has gradients attached.\n",
    "    # We can use it directly for the loss calculation.\n",
    "    self.optimizer.zero_grad()\n",
    "    loss = self.loss_fn(prediction, torch.tensor([[reward]], dtype=torch.float32))\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "```\n",
    "\n",
    "This looks reasonable, but it's inefficient because we have to recompute the gradient. A better design would have been to save the gradient and the feature vector. The vectorized approach handles this more gracefully.\n",
    "\n",
    "**Version 2: Update for the Vectorized `predict` (The Superior Method)**\n",
    "\n",
    "This version assumes that `predict_vectorized` was just called and that it saved the full Jacobian matrix `self.last_jacobian`.\n",
    "\n",
    "```python\n",
    "# This is the update method that pairs with `predict_vectorized`.\n",
    "def update_vectorized(self, chosen_arm_idx: int, feature_vector: torch.Tensor, reward: float):\n",
    "    \"\"\"\n",
    "    Updates the agent's parameters using the Jacobian computed during the\n",
    "    vectorized predict step.\n",
    "\n",
    "    Args:\n",
    "        chosen_arm_idx (int): The index of the arm that was played.\n",
    "        feature_vector (torch.Tensor): The feature vector for the CHOSEN arm.\n",
    "        reward (float): The observed reward.\n",
    "    \"\"\"\n",
    "    self.model.train()\n",
    "\n",
    "    # --- Step 1: Update the UCB machinery (A and b) ---\n",
    "    # Retrieve the correct gradient vector (row) from the stored Jacobian.\n",
    "    g_t = self.last_jacobian[chosen_arm_idx].view(-1, 1) # Shape (p, 1)\n",
    "    \n",
    "    # Update A and b with this gradient\n",
    "    self.A += g_t @ g_t.T\n",
    "    self.b += reward * g_t\n",
    "\n",
    "    # --- Step 2: Update the network's parameters ---\n",
    "    # We still need to perform a forward/backward pass to train the network.\n",
    "    # This is a separate computation from the gradient used for the UCB bonus.\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass to get the current prediction for the chosen arm\n",
    "    prediction = self.model(feature_vector.unsqueeze(0))\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = self.loss_fn(prediction, torch.tensor([[reward]], dtype=torch.float32))\n",
    "    \n",
    "    # Backward pass and optimizer step\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "```\n",
    "\n",
    "This version is much cleaner. It correctly separates the two roles of the gradient:\n",
    "1.  The gradient **at the time of prediction** (from the Jacobian) is used to update the UCB confidence bounds. This is a \"snapshot\" of the model's sensitivity when it made its decision.\n",
    "2.  The gradient of the **loss with respect to the current parameters** is used to update the network weights. This is the standard supervised learning update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9881a",
   "metadata": {},
   "source": [
    "#### **3.7.4 Assembling the Complete Agent**\n",
    "\n",
    "We will now formally assemble the `NeuralUCBAgent` class, incorporating our most advanced and efficient methods (`predict_vectorized` and `update_vectorized`), renaming them to the canonical `predict` and `update` for final use. This is the version that we should take away as the definitive implementation.\n",
    "\n",
    "```python\n",
    "# --- Final, complete NeuralUCBAgent Class ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.func import vmap, functional_call\n",
    "from typing import List\n",
    "\n",
    "# Assumes NeuralBanditNetwork class from Section 3.4 is defined above\n",
    "\n",
    "class NeuralUCBAgent:\n",
    "    \"\"\"\n",
    "    Implements the NeuralUCB algorithm using efficient, vectorized operations.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 hidden_dims: List[int] = [128, 64],\n",
    "                 lambda_: float = 1.0,\n",
    "                 alpha: float = 1.0,\n",
    "                 lr: float = 0.01):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        self.model = NeuralBanditNetwork(feature_dim, hidden_dims)\n",
    "        self.p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        self.A = torch.eye(self.p) * self.lambda_\n",
    "        self.b = torch.zeros((self.p, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.last_jacobian = None\n",
    "\n",
    "    def predict(self, feature_vectors: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Selects an arm using a vectorized calculation of the NeuralUCB score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        n_arms = feature_vectors.shape[0]\n",
    "        \n",
    "        # Exploitation Term\n",
    "        exploit_scores = self.model(feature_vectors).squeeze()\n",
    "\n",
    "        # Exploration Term (Vectorized)\n",
    "        A_inv = torch.inverse(self.A)\n",
    "\n",
    "        params = {k: v.detach() for k, v in self.model.named_parameters()}\n",
    "        buffers = {k: v.detach() for k, v in self.model.named_buffers()}\n",
    "\n",
    "        def compute_grad(x_single):\n",
    "            pred = functional_call(self.model, (params, buffers), args=(x_single.unsqueeze(0),))\n",
    "            grads = torch.autograd.grad(pred, params.values())\n",
    "            return torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "        J = vmap(compute_grad)(feature_vectors)\n",
    "        self.last_jacobian = J # Save for the update step\n",
    "\n",
    "        bonus_squared = torch.einsum('ij,ji->i', J @ A_inv, J.T)\n",
    "        explore_bonuses = self.alpha * torch.sqrt(torch.clamp(bonus_squared, min=0))\n",
    "\n",
    "        ucb_scores = exploit_scores + explore_bonuses\n",
    "        chosen_arm = torch.argmax(ucb_scores).item()\n",
    "        \n",
    "        return chosen_arm\n",
    "\n",
    "    def update(self, chosen_arm_idx: int, feature_vector: torch.Tensor, reward: float):\n",
    "        \"\"\"\n",
    "        Updates the agent's parameters using the Jacobian from the predict step\n",
    "        and standard gradient descent for the network weights.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        # Step 1: Update UCB machinery (A and b)\n",
    "        if self.last_jacobian is None:\n",
    "             # This should not happen in a normal predict->update cycle\n",
    "             # but as a fallback, we would recompute the single gradient\n",
    "             raise  ValueError(\"predict() must be called before update()\")\n",
    "        \n",
    "        g_t = self.last_jacobian[chosen_arm_idx].view(-1, 1)\n",
    "        self.A += g_t @ g_t.T\n",
    "        self.b += reward * g_t\n",
    "\n",
    "        # Step 2: Update network parameters (standard supervised learning)\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.model(feature_vector.unsqueeze(0))\n",
    "        loss = self.loss_fn(prediction, torch.tensor([[reward]], dtype=torch.float32))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "```\n",
    "\n",
    "We have now successfully constructed a complete, efficient, and theoretically grounded `NeuralUCBAgent`. The final step is to place this agent into our simulation environment and compare its performance against the disjoint `LinUCBAgent` from Chapter 2, which, hopefully, illustrates the power of generalization through shared representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final, complete NeuralUCBAgent Class ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.func import vmap, functional_call\n",
    "from typing import List\n",
    "\n",
    "# Assumes NeuralBanditNetwork class from Section 3.4 is defined above\n",
    "# TODO: Ensure NeuralBanditNetwork is imported or defined in the same file\n",
    "# TODO: pass NeuralBanditNetwork object to initialize NeuralUCBAgent\n",
    "\n",
    "class NeuralUCBAgent:\n",
    "    \"\"\"\n",
    "    Implements the NeuralUCB algorithm using efficient, vectorized operations.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 hidden_dims: List[int] = [128, 64],\n",
    "                 lambda_: float = 1.0,\n",
    "                 alpha: float = 1.0,\n",
    "                 lr: float = 0.01):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        self.model = NeuralBanditNetwork(feature_dim, hidden_dims)\n",
    "        self.p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        self.A = torch.eye(self.p) * self.lambda_\n",
    "        self.b = torch.zeros((self.p, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.last_jacobian = None\n",
    "\n",
    "    def predict(self, feature_vectors: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Selects an arm using a vectorized calculation of the NeuralUCB score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        n_arms = feature_vectors.shape[0]\n",
    "        \n",
    "        # Exploitation Term\n",
    "        exploit_scores = self.model(feature_vectors).squeeze()\n",
    "\n",
    "        # Exploration Term (Vectorized)\n",
    "        A_inv = torch.inverse(self.A)\n",
    "\n",
    "        params = {k: v.detach() for k, v in self.model.named_parameters()}\n",
    "        buffers = {k: v.detach() for k, v in self.model.named_buffers()}\n",
    "\n",
    "        def compute_grad(x_single):\n",
    "            pred = functional_call(self.model, (params, buffers), args=(x_single.unsqueeze(0),))\n",
    "            grads = torch.autograd.grad(pred, params.values())\n",
    "            return torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "        J = vmap(compute_grad)(feature_vectors)\n",
    "        self.last_jacobian = J # Save for the update step\n",
    "\n",
    "        bonus_squared = torch.einsum('ij,ji->i', J @ A_inv, J.T)\n",
    "        explore_bonuses = self.alpha * torch.sqrt(torch.clamp(bonus_squared, min=0))\n",
    "\n",
    "        ucb_scores = exploit_scores + explore_bonuses\n",
    "        chosen_arm = torch.argmax(ucb_scores).item()\n",
    "        \n",
    "        return chosen_arm\n",
    "\n",
    "    def update(self, chosen_arm_idx: int, feature_vector: torch.Tensor, reward: float):\n",
    "        \"\"\"\n",
    "        Updates the agent's parameters using the Jacobian from the predict step\n",
    "        and standard gradient descent for the network weights.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        # Step 1: Update UCB machinery (A and b)\n",
    "        if self.last_jacobian is None:\n",
    "             # This should not happen in a normal predict->update cycle\n",
    "             # but as a fallback, we would recompute the single gradient\n",
    "             raise  ValueError(\"predict() must be called before update()\")\n",
    "        \n",
    "        g_t = self.last_jacobian[chosen_arm_idx].view(-1, 1)\n",
    "        self.A += g_t @ g_t.T\n",
    "        self.b += reward * g_t\n",
    "\n",
    "        # Step 2: Update network parameters (standard supervised learning)\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.model(feature_vector.unsqueeze(0))\n",
    "        loss = self.loss_fn(prediction, torch.tensor([[reward]], dtype=torch.float32))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a439c",
   "metadata": {},
   "source": [
    "At this moment one might ask - \"But what about training?\". Let us look deeper at this important point.\n",
    "\n",
    "### **3.8 The Online Learning Paradigm: Training Through Interaction**\n",
    "\n",
    "Above we have carefully constructed our `NeuralUCBAgent`, a sophisticated piece of machinery with `predict` and `update` methods. But how do we \"train\" it? The very word \"training\" can be misleading here, as it often evokes the paradigm we used in Chapter 1: loading a large, static dataset and iterating over it for multiple \"epochs\" until a loss function converges.\n",
    "\n",
    "Online learning operates on a completely different and far more dynamic principle. An online agent is not trained *before* it is deployed; rather, **learning is the direct result of deployment**. The agent learns continuously, one interaction at a time, in a live environment. There are no epochs. There is only the relentless forward march of time, $t=1, 2, 3, \\dots$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00feefcd",
   "metadata": {},
   "source": [
    "#### **3.8.1 From Batched Training to Online Learning: A Paradigm Shift**\n",
    "\n",
    "To fully appreciate the online model, let us explicitly contrast it with the batch learning approach of our `MLPRecommender`.\n",
    "\n",
    "| Feature | **Batch Learning (Chapter 1)** | **Online Learning (This Chapter)** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data** | A large, static, historical dataset. | A live stream of `(context, action, reward)` tuples. |\n",
    "| **Training Process** | Occurs offline, before deployment. The model iterates over the full dataset many times (epochs). | Occurs online, simultaneously with deployment. The model sees each data point exactly once, immediately after the interaction occurs. |\n",
    "| **Model State** | The model is \"trained\" and its parameters are then frozen. | The model is \"live.\" Its parameters are constantly changing with every new piece of information. |\n",
    "| **Core Loop** | `for epoch in epochs: for batch in data: update()` | `for t in timesteps: predict() -> act() -> observe() -> update()` |\n",
    "| **Adaptability** | Inherently static. Cannot adapt to new trends without complete retraining. | Inherently adaptive. Learns from the most recent interactions by design. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc78fb",
   "metadata": {},
   "source": [
    "#### **3.8.2 The Anatomy of a Single Time Step**\n",
    "\n",
    "The fundamental unit of online learning is the single time step, or the \"interaction loop.\" This loop consists of a structured sequence of events that repeats indefinitely. Let us walk through one complete cycle, from the moment a user arrives to the moment the agent becomes slightly wiser.\n",
    "\n",
    "Imagine it is time step $t$.\n",
    "\n",
    "**Step 1: The World Presents a Context**\n",
    "A user arrives at our website. The environment provides us with the user's identity and any relevant context. Our task is to generate a feature vector, $x_{t,a}$, for every possible action (arm/product) $a$ we could take. For example, if we have 50 products, we will generate 50 distinct feature vectors, forming the set $\\{x_{t,a}\\}_{a \\in \\mathcal{A}}$. Each vector encodes information about that specific user-item combination.\n",
    "\n",
    "**Step 2: The Agent Decides (The `predict` call)**\n",
    "The agent's `predict` method is invoked. It is given the complete set of feature vectors $\\{x_{t,a}\\}$. Internally, as we have designed, it performs the full NeuralUCB calculation:\n",
    "*   It computes the exploitation score $g(x_{t,a}; \\hat{\\theta}_{t-1})$ for all arms.\n",
    "*   It computes the exploration bonus $\\alpha \\sqrt{ (\\nabla g_{t,a})^T A_{t-1}^{-1} (\\nabla g_{t,a}) }$ for all arms.\n",
    "*   It sums these to find the UCB score and selects the arm $a_t$ with the highest score.\n",
    "The output of this step is a single integer: the ID of the arm to play.\n",
    "\n",
    "**Step 3: The Agent Acts & The World Responds**\n",
    "We execute the chosen action $a_t$. In our simulation, this means we call `simulator.get_reward(user_id, a_t)`. The environment consumes our action and returns a scalar reward, $r_t$. This reward is the ground truth for this single interaction—did the user click (1) or not (0)?\n",
    "\n",
    "**Step 4: The Agent Learns (The `update` call)**\n",
    "This is the crucial learning step. We now have a complete data tuple for this time step: $(x_{t,a_t}, a_t, r_t)$. That is, we have the context that led to a decision, the decision itself, and the outcome of that decision. We pass this information to the agent's `update` method:\n",
    "`agent.update(chosen_arm_idx=a_t, feature_vector=x_{t,a_t}, reward=r_t)`\n",
    "Internally, this single call does two things:\n",
    "1.  It updates the UCB evidence matrices $A$ and $b$, making the agent's uncertainty estimate more accurate for future decisions.\n",
    "2.  It performs one step of gradient descent on the neural network, nudging its weights $\\theta$ so that its prediction for $x_{t,a_t}$ is closer to the observed reward $r_t$.\n",
    "\n",
    "**Step 5: Advance Time**\n",
    "The time step $t$ is now complete. We increment to $t+1$. The agent's parameters, $\\hat{\\theta}_{t-1}$, $A_{t-1}$, and $b_{t-1}$, have now become $\\hat{\\theta}_t$, $A_t$, and $b_t$. The agent is now marginally more knowledgeable and ready for the next user interaction. This loop repeats for thousands or millions of steps, and the cumulative effect of these tiny, single-example updates is a sophisticated, adaptive model.\n",
    "\n",
    "#### **3.8.3 Learn as you code: Implementing the Online Simulation Loop**\n",
    "\n",
    "Let us now translate this conceptual loop into executable Python code. This will be the main script that runs our experiment, pitting the agents against each other in the `ZooplusSimulator`.\n",
    "\n",
    "**Code Dissection**\n",
    "\n",
    "*   **Initialization:** We start by initializing our environment (`ZooplusSimulator`) and our agent (`NeuralUCBAgent`). The agent begins with randomly initialized network weights—it knows nothing about user preferences.\n",
    "*   **Feature Engineering:** We define a helper function, `create_feature_vectors`, that takes a user ID and the product catalog and produces the matrix of feature vectors that the `predict` method requires. For this problem, we will one-hot encode the user's persona and the product's category, then concatenate them, just as we did for `LinUCB` in Chapter 2.\n",
    "*   **The Main Loop:** The `for t in range(num_interactions):` loop is the direct implementation of the process described above.\n",
    "*   **Tracking Performance:** We maintain a list, `rewards_history`, to store the reward from each time step. By plotting a moving average of this list, we can visualize how the agent's performance (its average Click-Through Rate) improves over time as it learns.\n",
    "\n",
    "**Assembling the Full Simulation Code**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm # For a nice progress bar\n",
    "\n",
    "# Assume the following are defined in our environment:\n",
    "# - ZooplusSimulator class (from Chapter 1)\n",
    "# - NeuralUCBAgent class (from this chapter)\n",
    "# - A function to get a random user from the simulator\n",
    "\n",
    "def create_feature_vectors(user_persona: str, products_df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"Creates one-hot encoded feature vectors for all products for a given user.\"\"\"\n",
    "    # This is a simplified example. A real system would have richer features.\n",
    "    persona_features = pd.get_dummies([user_persona])\n",
    "    product_features = pd.get_dummies(products_df['category'])\n",
    "    \n",
    "    # Ensure all possible columns are present\n",
    "    all_persona_cols = ['new_puppy_parent', 'cat_connoisseur', 'fish_hobbyist', 'senior_dog_owner']\n",
    "    all_product_cols = ['Fish Supplies', 'Cat Food', 'Dog Food', 'Dog Toy', 'Cat Toy']\n",
    "    \n",
    "    persona_features = persona_features.reindex(columns=all_persona_cols, fill_value=0)\n",
    "    product_features = product_features.reindex(columns=all_product_cols, fill_value=0)\n",
    "    \n",
    "    n_products = len(products_df)\n",
    "    # Repeat the user features for each product\n",
    "    user_part = np.tile(persona_features.values, (n_products, 1))\n",
    "    product_part = product_features.values\n",
    "    \n",
    "    # Concatenate to form interaction features\n",
    "    interaction_features = np.concatenate([user_part, product_part], axis=1)\n",
    "    \n",
    "    return torch.from_numpy(interaction_features).float()\n",
    "\n",
    "# --- Simulation Setup ---\n",
    "NUM_INTERACTIONS = 20_000\n",
    "N_PRODUCTS = 50\n",
    "ALPHA = 1.5 # Exploration hyperparameter\n",
    "\n",
    "# 1. Initialize Environment\n",
    "simulator = ZooplusSimulator(n_products=N_PRODUCTS, seed=42)\n",
    "feature_dim = len(simulator.personas) + len(simulator.products['category'].unique())\n",
    "\n",
    "# 2. Initialize Agent\n",
    "# The agent starts \"cold\" with no knowledge.\n",
    "agent = NeuralUCBAgent(feature_dim=feature_dim, alpha=ALPHA, lr=0.001)\n",
    "\n",
    "# 3. Run the Online Learning Loop\n",
    "rewards_history = []\n",
    "print(\"Running NeuralUCB simulation...\")\n",
    "\n",
    "for t in tqdm(range(NUM_INTERACTIONS)):\n",
    "    # Step 1: The World Presents a Context\n",
    "    user_id = simulator.get_random_user()\n",
    "    user_persona = simulator.user_to_persona_map[user_id]\n",
    "    \n",
    "    # Generate feature vectors for ALL possible arms for this user\n",
    "    all_feature_vectors = create_feature_vectors(user_persona, simulator.products)\n",
    "\n",
    "    # Step 2: The Agent Decides\n",
    "    chosen_arm_idx = agent.predict(all_feature_vectors)\n",
    "    \n",
    "    # Step 3: The Agent Acts & The World Responds\n",
    "    reward = simulator.get_reward(user_id, product_id=chosen_arm_idx)\n",
    "    \n",
    "    # Step 4: The Agent Learns\n",
    "    # Get the feature vector corresponding to the action that was actually taken\n",
    "    chosen_feature_vector = all_feature_vectors[chosen_arm_idx]\n",
    "    agent.update(chosen_arm_idx, chosen_feature_vector, reward)\n",
    "    \n",
    "    # For evaluation purposes\n",
    "    rewards_history.append(reward)\n",
    "\n",
    "print(f\"Simulation complete.\")\n",
    "print(f\"Average reward (CTR) over the run: {np.mean(rewards_history):.4f}\")\n",
    "```\n",
    "\n",
    "This code provides the complete, runnable framework for \"training\" our online agent. We are now fully prepared to execute this simulation and analyze the results, comparing them to our previous models to demonstrate the power of generalized, adaptive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d95f6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

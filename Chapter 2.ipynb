{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175edd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Metal) backend is available and enabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Metal) backend is available and enabled.\")\n",
    "else:\n",
    "    print(\"❌ MPS not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79384221",
   "metadata": {},
   "source": [
    "### **Chapter 2: The Adaptive Recommender: Contextual Bandits in Action**\n",
    "\n",
    "#### **2.1 Introduction: Escaping the Static World with the Explore-Exploit Dilemma**\n",
    "\n",
    "In the previous chapter, we built a capable, yet fundamentally flawed, batched recommender. Its knowledge is frozen in time, learned from a static dataset. It is like a student who has memorized a textbook but cannot apply that knowledge to new problems or learn from their mistakes. To build a truly intelligent system, we need to move from this passive, offline learning to an active, online paradigm.\n",
    "\n",
    "This brings us face-to-face with one of the most fundamental trade-offs in decision-making and machine learning: the **explore-exploit dilemma**.\n",
    "\n",
    "Imagine you are at a new food court with five stalls.\n",
    "*   **Exploitation** is the safe bet. You try the pizza stall, and it's pretty good. The \"exploit\" strategy would be to eat pizza every single day. You are guaranteed a decent meal, maximizing your immediate reward based on your current knowledge.\n",
    "*   **Exploration** is the risky, but potentially more rewarding, path. You could try the mysterious taco stall. It might be terrible (a loss of immediate reward), but it could also be the best food you've ever had, leading to much higher rewards in the long run.\n",
    "\n",
    "Our batched recommender is a pure exploiter. Once trained, it will always recommend the items it *believes* have the highest CTR, based on its fixed knowledge. It never dares to try the \"taco stall\"—a new product or a niche item—because its predicted CTR is low or unknown.\n",
    "\n",
    "To build a better system, we need an algorithm that can intelligently manage this trade-off. This is the domain of **Multi-Armed Bandits**, a class of reinforcement learning algorithms designed specifically for this problem. The name comes from the analogy of a gambler at a row of slot machines (or \"one-armed bandits\"), trying to figure out which machine to play to maximize their total winnings.\n",
    "\n",
    "We will take this concept one step further by using a **Contextual Bandit**. A simple multi-armed bandit learns the best \"arm\" (or product) to pull on average, across all situations. A contextual bandit is far more powerful: it learns the best arm to pull *given the current context*. In our Zooplus scenario, the **context** is the user. The algorithm doesn't just learn \"which product is best overall?\"; it learns \"which product is best for *this specific user* right now?\".\n",
    "\n",
    "This chapter will introduce a classic, elegant, and highly effective contextual bandit algorithm: the **Linear Upper Confidence Bound (LinUCB)** algorithm. We will implement it, pit it against our static batched model in our simulation, and witness firsthand the power of continuous, adaptive learning.\n",
    "\n",
    "#### **2.2 The Frontier Technique: The Linear Upper Confidence Bound (LinUCB) Algorithm**\n",
    "\n",
    "The LinUCB algorithm, first introduced by Li et al. (2010) for news article recommendation, strikes a beautiful balance between performance, efficiency, and interpretability. It is a perfect entry point into the world of online, reinforcement learning-based recommenders.\n",
    "\n",
    "**The Core Assumption: A Linear World**\n",
    "\n",
    "LinUCB makes a simplifying (for nolinear cases we will investigate NeuralUCB and the likes in subsequent chapters) assumption: the expected reward (the true, underlying CTR) of showing a product to a user is a **linear function** of a combined feature vector.\n",
    "\n",
    "Let's say for a given user-product pair, we can construct a feature vector, `x`. This vector could include:\n",
    "*   User features (e.g., one-hot encoding of their persona)\n",
    "*   Product features (e.g., one-hot encoding of the product's category)\n",
    "*   Interaction features (e.g., the product of user and item embeddings)\n",
    "\n",
    "The algorithm assumes there exists an unknown coefficient vector, `θ`, such that the expected reward, `E[r]`, is simply their dot product:\n",
    "\n",
    "`E[r] = x^T θ`\n",
    "\n",
    "The entire goal of the LinUCB algorithm is to **learn the `θ` vector** for each product as efficiently as possible.\n",
    "\n",
    "**How LinUCB Balances Exploration and Exploitation**\n",
    "\n",
    "For each \"arm\" (i.e., each product in our catalog), LinUCB maintains two key pieces of information:\n",
    "\n",
    "1.  **A `d x d` matrix `A`**: This matrix stores information about the feature vectors `x` it has seen so far for that arm. It's essentially `X^T X`, where `X` is the matrix of all feature vectors observed for that arm. The inverse of `A` helps us measure our uncertainty about the arm's true reward.\n",
    "2.  **A `d x 1` vector `b`**: This vector stores the sum of the feature vectors `x` weighted by the rewards `r` they produced. It's `X^T r`.\n",
    "\n",
    "At each step, to make a recommendation, LinUCB calculates a score for every possible product using these two pieces of information. The score is composed of two parts:\n",
    "\n",
    "`Score = (Predicted CTR) + (Uncertainty Bonus)`\n",
    "\n",
    "1.  **Predicted CTR (Exploitation):** The algorithm first calculates its current best estimate of the coefficient vector, `θ_hat = A⁻¹ b`. The predicted CTR is then simply `x^T θ_hat`. This is the exploitation term—it favors products that have performed well in the past.\n",
    "\n",
    "2.  **Uncertainty Bonus (Exploration):** The second term is `α * sqrt(x^T A⁻¹ x)`.\n",
    "    *   `A⁻¹` represents the covariance of our estimate for `θ_hat`. A large value means we are very uncertain about our estimate.\n",
    "    *   `x^T A⁻¹ x` gives us the variance of the prediction specifically for the feature vector `x`. If we have seen feature vectors similar to `x` many times before, this term will be small. If `x` represents a new, unseen combination of user and product features, this term will be large.\n",
    "    *   `α` (alpha) is a hyperparameter that you control. It scales how much the algorithm values exploration. A higher `α` makes the algorithm more adventurous.\n",
    "\n",
    "The algorithm then simply chooses the product with the **highest combined score**.\n",
    "\n",
    "**The Intuition:**\n",
    "\n",
    "*   If a product has a high predicted CTR and we are very certain about it (low uncertainty bonus), it gets a high score. **(Pure Exploitation)**\n",
    "*   If a product has a mediocre predicted CTR but we are very uncertain about it (high uncertainty bonus), it can also get a high score. Choosing this product is an act of **exploration**. By trying it, we get a new data point, which reduces our uncertainty (updating `A` and `b`) and helps us learn its true value for the future.\n",
    "\n",
    "This elegant combination allows LinUCB to learn efficiently. It focuses its exploration on the parts of the feature space where its knowledge is weakest, leading to rapid convergence.\n",
    "\n",
    "#### **2.3 Implementing the LinUCB Agent for Zooplus**\n",
    "\n",
    "Now, let's translate this theory into practice. We will create a Python class for a single LinUCB \"arm\" (representing one product) and a main agent class that manages all the arms.\n",
    "\n",
    "For our feature vector `x`, we will do something simple and effective: we will **concatenate the user's embedding with the product's category features**. But where do we get a user embedding for a contextual bandit? We can't use the one from the batched model directly, as it was trained for a different task.\n",
    "\n",
    "Instead, we will create a new set of user embeddings, one for each *persona*. This is a reasonable simplification for our simulation. In a real system, these could be embeddings learned from user demographics or other side information. The product features will be the one-hot encoded categories we already have in our simulator.\n",
    "\n",
    "**Code Block 2.1: The LinUCB Implementation**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinUCBArm:\n",
    "    \"\"\"Represents a single arm in the LinUCB algorithm.\"\"\"\n",
    "    def __init__(self, arm_index, d, alpha):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arm_index (int): The index of the arm (e.g., product_id).\n",
    "            d (int): The dimensionality of the feature vector.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.arm_index = arm_index\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize A as a d x d identity matrix.\n",
    "        # This corresponds to a standard Bayesian linear regression prior.\n",
    "        self.A = np.identity(d)\n",
    "        \n",
    "        # Initialize b as a d x 1 zero vector.\n",
    "        self.b = np.zeros([d, 1])\n",
    "\n",
    "    def calc_p(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the score for this arm given a feature vector x.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): A d-dimensional feature vector.\n",
    "        \n",
    "        Returns:\n",
    "            The UCB score for this arm.\n",
    "        \"\"\"\n",
    "        # Ensure x is a column vector\n",
    "        x = x.reshape(-1, 1)\n",
    "        \n",
    "        # Calculate A_inv and theta_hat\n",
    "        A_inv = np.linalg.inv(self.A)\n",
    "        theta_hat = A_inv.dot(self.b)\n",
    "        \n",
    "        # Calculate the UCB score\n",
    "        p = theta_hat.T.dot(x) + self.alpha * np.sqrt(x.T.dot(A_inv).dot(x))\n",
    "        \n",
    "        return p\n",
    "\n",
    "    def update(self, x, reward):\n",
    "        \"\"\"\n",
    "        Updates the A and b matrices for this arm.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The feature vector for the interaction.\n",
    "            reward (int): The observed reward (0 or 1).\n",
    "        \"\"\"\n",
    "        x = x.reshape(-1, 1)\n",
    "        self.A += x.dot(x.T)\n",
    "        self.b += reward * x\n",
    "\n",
    "class LinUCBAgent:\n",
    "    \"\"\"The main agent that manages all the LinUCB arms.\"\"\"\n",
    "    def __init__(self, n_products, user_features, product_features, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_products (int): The number of arms (products).\n",
    "            user_features (dict): A dict mapping persona name to a feature vector.\n",
    "            product_features (np.array): A matrix of one-hot encoded product categories.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.user_features = user_features\n",
    "        self.product_features = product_features\n",
    "        self.n_products = n_products\n",
    "        \n",
    "        # The dimensionality of our combined feature vector\n",
    "        d = list(user_features.values())[0].shape[0] + product_features.shape[1]\n",
    "        \n",
    "        # Create a list of arms\n",
    "        self.arms = [LinUCBArm(i, d, alpha) for i in range(n_products)]\n",
    "\n",
    "    def _create_feature_vector(self, persona, product_id):\n",
    "        \"\"\"Creates the concatenated feature vector x.\"\"\"\n",
    "        user_feat = self.user_features[persona]\n",
    "        product_feat = self.product_features[product_id]\n",
    "        return np.concatenate([user_feat, product_feat])\n",
    "\n",
    "    def choose_action(self, user_persona):\n",
    "        \"\"\"\n",
    "        Chooses the best product to recommend for the given user persona.\n",
    "        \n",
    "        Returns:\n",
    "            The product_id of the chosen action.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for product_id in range(self.n_products):\n",
    "            # Create the feature vector for this user-product pair\n",
    "            x = self._create_feature_vector(user_persona, product_id)\n",
    "            \n",
    "            # Calculate the score for this arm\n",
    "            score = self.arms[product_id].calc_p(x)\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Choose the arm with the highest score (break ties randomly)\n",
    "        max_score = np.max(scores)\n",
    "        best_arms = np.where(scores == max_score)[0]\n",
    "        chosen_arm = np.random.choice(best_arms)\n",
    "        \n",
    "        return chosen_arm\n",
    "\n",
    "    def update(self, chosen_arm, user_persona, reward):\n",
    "        \"\"\"Updates the agent after an action is taken.\"\"\"\n",
    "        x = self._create_feature_vector(user_persona, chosen_arm)\n",
    "        self.arms[chosen_arm].update(x, reward)\n",
    "\n",
    "# --- Setup for the LinUCB Agent ---\n",
    "\n",
    "# Create simple, random embeddings for our user personas\n",
    "persona_embedding_dim = 8\n",
    "user_features_for_bandit = {\n",
    "    name: np.random.rand(persona_embedding_dim) \n",
    "    for name in sim.personas.keys()\n",
    "}\n",
    "\n",
    "# The product features are the one-hot encoded categories from the simulator\n",
    "product_features_for_bandit = sim.product_features\n",
    "\n",
    "# Instantiate the agent\n",
    "linucb_agent = LinUCBAgent(\n",
    "    n_products=sim.n_products,\n",
    "    user_features=user_features_for_bandit,\n",
    "    product_features=product_features_for_bandit,\n",
    "    alpha=1.5 # Let's be a bit adventurous\n",
    ")\n",
    "\n",
    "print(\"LinUCB Agent created successfully.\")\n",
    "d = list(user_features_for_bandit.values())[0].shape[0] + product_features_for_bandit.shape[1]\n",
    "print(f\"Feature vector dimensionality (d): {d}\")\n",
    "```\n",
    "\n",
    "With the agent class defined and instantiated, we are now ready for the main event: a head-to-head competition. We will create a simulation loop that pits our new, adaptive `LinUCBAgent` against the static, pre-trained `MLPRecommender` from Chapter 1. This will allow us to see, step-by-step, how an online learning agent behaves compared to its offline counterpart.\n",
    "\n",
    "---\n",
    "This concludes the first part of Chapter 2. We have introduced the core concepts and provided a full implementation of the LinUCB agent, setting up all the necessary components for the simulation. The next section will detail the experimental design and present the code for the head-to-head comparison. Please review, and I will proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb07777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

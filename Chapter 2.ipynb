{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175edd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Metal) backend is available and enabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Metal) backend is available and enabled.\")\n",
    "else:\n",
    "    print(\"❌ MPS not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7af2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating for User ID: 125, who is a 'new_puppy_parent'\n",
      "  - Recommended Product 1 (Category: Cat Food)... Clicked: No\n",
      "  - Recommended Product 2 (Category: Dog Food)... Clicked: Yes\n",
      "  - Recommended Product 10 (Category: Fish Supplies)... Clicked: No\n",
      "  - Recommended Product 11 (Category: Cat Toy)... Clicked: No\n"
     ]
    }
   ],
   "source": [
    "# simulation from chapter 1 \n",
    "# !pip install numpy pandas scikit-learn tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ZooplusSimulator:\n",
    "    \"\"\"\n",
    "    A simulated environment for the Zooplus recommendation problem.\n",
    "\n",
    "    This class manages:\n",
    "    1. A product catalog with features (category, base popularity).\n",
    "    2. A set of user personas with distinct preferences.\n",
    "    3. A stochastic reward function to simulate user clicks (CTR).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_products=50, n_users=1000, seed=42):\n",
    "        \"\"\"\n",
    "        Initializes the simulation environment.\n",
    "        \n",
    "        Args:\n",
    "            n_products (int): The total number of products in the catalog.\n",
    "            n_users (int): The total number of unique users in the simulation.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.n_products = n_products\n",
    "        self.n_users = n_users\n",
    "        \n",
    "        # 1. Create the Product Catalog\n",
    "        self.products = self._create_product_catalog()\n",
    "        self.product_features = self._get_product_features()\n",
    "        \n",
    "        # 2. Create User Personas and assign each of the n_users to a persona\n",
    "        self.personas = self._create_user_personas()\n",
    "        self.user_to_persona_map = self._assign_users_to_personas()\n",
    "\n",
    "    def _create_product_catalog(self):\n",
    "        \"\"\"Creates a pandas DataFrame of products.\"\"\"\n",
    "        product_ids = range(self.n_products)\n",
    "        categories = ['Dog Food', 'Cat Food', 'Dog Toy', 'Cat Toy', 'Fish Supplies']\n",
    "        \n",
    "        product_data = {\n",
    "            'product_id': product_ids,\n",
    "            'category': self.rng.choice(categories, self.n_products),\n",
    "            # Base popularity score for each product (e.g., from global sales)\n",
    "            'base_popularity': self.rng.uniform(0.1, 0.5, self.n_products)\n",
    "        }\n",
    "        return pd.DataFrame(product_data)\n",
    "\n",
    "    def _get_product_features(self):\n",
    "        \"\"\"One-hot encodes product categories to create a feature matrix.\"\"\"\n",
    "        return pd.get_dummies(self.products['category'], prefix='cat').astype(float).values\n",
    "\n",
    "    def _create_user_personas(self):\n",
    "        \"\"\"Defines a dictionary of user personas and their preferences.\"\"\"\n",
    "        return {\n",
    "            'new_puppy_parent': {'Dog Food': 0.9, 'Dog Toy': 0.8, 'Cat Food': 0.1, 'Cat Toy': 0.1, 'Fish Supplies': 0.05},\n",
    "            'cat_connoisseur':  {'Dog Food': 0.1, 'Dog Toy': 0.05, 'Cat Food': 0.9, 'Cat Toy': 0.85, 'Fish Supplies': 0.1},\n",
    "            'budget_shopper':   {'Dog Food': 0.5, 'Dog Toy': 0.4, 'Cat Food': 0.5, 'Cat Toy': 0.4, 'Fish Supplies': 0.3},\n",
    "            'fish_hobbyist':    {'Dog Food': 0.05, 'Dog Toy': 0.05, 'Cat Food': 0.1, 'Cat Toy': 0.1, 'Fish Supplies': 0.95}\n",
    "        }\n",
    "        \n",
    "    def _assign_users_to_personas(self):\n",
    "        \"\"\"Randomly assigns each user ID to one of the defined personas.\"\"\"\n",
    "        persona_names = list(self.personas.keys())\n",
    "        return {user_id: self.rng.choice(persona_names) for user_id in range(self.n_users)}\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid function to map a score to a probability.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def get_reward(self, user_id, product_id):\n",
    "        \"\"\"\n",
    "        Simulates a user-item interaction and returns a reward (1 for click, 0 for no-click).\n",
    "        \n",
    "        The click probability is a function of:\n",
    "        - The user's affinity for the product's category.\n",
    "        - The product's base popularity.\n",
    "        - Random noise.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_to_persona_map or product_id >= self.n_products:\n",
    "            return 0 # Invalid user or product\n",
    "            \n",
    "        # Get user and product info\n",
    "        persona_name = self.user_to_persona_map[user_id]\n",
    "        persona_prefs = self.personas[persona_name]\n",
    "        \n",
    "        product_info = self.products.loc[product_id]\n",
    "        product_category = product_info['category']\n",
    "        product_popularity = product_info['base_popularity']\n",
    "        \n",
    "        # Calculate affinity score\n",
    "        affinity = persona_prefs.get(product_category, 0.1) # Default affinity for unknown categories\n",
    "        \n",
    "        # Combine scores and add noise. We scale the scores to create a reasonable logit.\n",
    "        logit = 3 * affinity + 1 * product_popularity - 2.5 # The constants are chosen to center the CTR\n",
    "        \n",
    "        # Introduce noise: some users might click on things they don't \"like\"\n",
    "        logit += self.rng.normal(0, 0.5)\n",
    "        \n",
    "        # Convert logit to a probability\n",
    "        click_prob = self._sigmoid(logit)\n",
    "        \n",
    "        # Sample from a Bernoulli distribution to get a stochastic outcome\n",
    "        reward = self.rng.binomial(1, click_prob)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def get_user(self):\n",
    "        \"\"\"Returns a random user_id from the population.\"\"\"\n",
    "        return self.rng.integers(0, self.n_users)\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Let's instantiate the simulator and see it in action\n",
    "# sim = ZooplusSimulator(seed=42)\n",
    "sim = ZooplusSimulator(seed=2)\n",
    "\n",
    "# Get a random user\n",
    "user_id = sim.get_user()\n",
    "persona = sim.user_to_persona_map[user_id]\n",
    "print(f\"Simulating for User ID: {user_id}, who is a '{persona}'\")\n",
    "\n",
    "# Let's test this user's reaction to a few products\n",
    "for product_id in [1, 2, 10, 11]:\n",
    "    product_cat = sim.products.loc[product_id, 'category']\n",
    "    reward = sim.get_reward(user_id, product_id)\n",
    "    print(f\"  - Recommended Product {product_id} (Category: {product_cat})... Clicked: {'Yes' if reward == 1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79384221",
   "metadata": {},
   "source": [
    "### **Chapter 2: The Adaptive Recommender: Contextual Bandits in Action**\n",
    "\n",
    "#### **2.1 Introduction: Escaping the Static World with the Explore-Exploit Dilemma**\n",
    "\n",
    "In the previous chapter, we built a capable, yet fundamentally flawed, batched recommender. Its knowledge is frozen in time, learned from a static dataset. It is like a student who has memorized a textbook but cannot apply that knowledge to new problems or learn from their mistakes. To build a truly intelligent system, we need to move from this passive, offline learning to an active, online paradigm.\n",
    "\n",
    "This brings us face-to-face with one of the most fundamental trade-offs in decision-making and machine learning: the **explore-exploit dilemma**.\n",
    "\n",
    "Imagine you are at a new food court with five stalls.\n",
    "*   **Exploitation** is the safe bet. You try the pizza stall, and it's pretty good. The \"exploit\" strategy would be to eat pizza every single day. You are guaranteed a decent meal, maximizing your immediate reward based on your current knowledge.\n",
    "*   **Exploration** is the risky, but potentially more rewarding, path. You could try the mysterious taco stall. It might be terrible (a loss of immediate reward), but it could also be the best food you've ever had, leading to much higher rewards in the long run.\n",
    "\n",
    "Our batched recommender is a pure exploiter. Once trained, it will always recommend the items it *believes* have the highest CTR, based on its fixed knowledge. It never dares to try the \"taco stall\"—a new product or a niche item—because its predicted CTR is low or unknown.\n",
    "\n",
    "To build a better system, we need an algorithm that can intelligently manage this trade-off. This is the domain of **Multi-Armed Bandits**, a class of reinforcement learning algorithms designed specifically for this problem. The name comes from the analogy of a gambler at a row of slot machines (or \"one-armed bandits\"), trying to figure out which machine to play to maximize their total winnings.\n",
    "\n",
    "We will take this concept one step further by using a **Contextual Bandit**. A simple multi-armed bandit learns the best \"arm\" (or product) to pull on average, across all situations. A contextual bandit is far more powerful: it learns the best arm to pull *given the current context*. In our Zooplus scenario, the **context** is the user. The algorithm doesn't just learn \"which product is best overall?\"; it learns \"which product is best for *this specific user* right now?\".\n",
    "\n",
    "This chapter will introduce a classic, elegant, and highly effective contextual bandit algorithm: the **Linear Upper Confidence Bound (LinUCB)** algorithm. We will implement it, pit it against our static batched model in our simulation, and witness firsthand the power of continuous, adaptive learning.\n",
    "\n",
    "#### **2.2 The taste of advanced techniques: The Linear Upper Confidence Bound (LinUCB) Algorithm**\n",
    "\n",
    "The LinUCB algorithm, first introduced by Li et al. (2010) for news article recommendation, strikes a beautiful balance between performance, efficiency, and interpretability. It is a perfect entry point into the world of online, reinforcement learning-based recommenders.\n",
    "\n",
    "**The Core Assumption: A Linear World**\n",
    "\n",
    "LinUCB makes a simplifying (for nolinear cases we will investigate NeuralUCB and the likes in subsequent chapters) assumption: the expected reward (the true, underlying CTR) of showing a product to a user is a **linear function** of a combined feature vector.\n",
    "\n",
    "Let's say for a given user-product pair, we can construct a feature vector, `x`. This vector could include:\n",
    "*   User features (e.g., one-hot encoding of their persona)\n",
    "*   Product features (e.g., one-hot encoding of the product's category)\n",
    "*   Interaction features (e.g., the product of user and item embeddings)\n",
    "\n",
    "The algorithm assumes there exists an unknown coefficient vector, `θ`, such that the expected reward, `E[r]`, is simply their dot product:\n",
    "\n",
    "`E[r] = x^T θ`\n",
    "\n",
    "The entire goal of the LinUCB algorithm is to **learn the `θ` vector** for each product as efficiently as possible.\n",
    "\n",
    "**How LinUCB Balances Exploration and Exploitation**\n",
    "\n",
    "For each \"arm\" (i.e., each product in our catalog), LinUCB maintains two key pieces of information:\n",
    "\n",
    "1.  **A `d x d` matrix `A`**: This matrix stores information about the feature vectors `x` it has seen so far for that arm. It's essentially `X^T X`, where `X` is the matrix of all feature vectors observed for that arm. The inverse of `A` helps us measure our uncertainty about the arm's true reward.\n",
    "2.  **A `d x 1` vector `b`**: This vector stores the sum of the feature vectors `x` weighted by the rewards `r` they produced. It's `X^T r`.\n",
    "\n",
    "At each step, to make a recommendation, LinUCB calculates a score for every possible product using these two pieces of information. The score is composed of two parts:\n",
    "\n",
    "`Score = (Predicted CTR) + (Uncertainty Bonus)`\n",
    "\n",
    "1.  **Predicted CTR (Exploitation):** The algorithm first calculates its current best estimate of the coefficient vector, `θ_hat = A⁻¹ b`. The predicted CTR is then simply `x^T θ_hat`. This is the exploitation term—it favors products that have performed well in the past.\n",
    "\n",
    "2.  **Uncertainty Bonus (Exploration):** The second term is `α * sqrt(x^T A⁻¹ x)`.\n",
    "    *   `A⁻¹` represents the covariance of our estimate for `θ_hat`. A large value means we are very uncertain about our estimate.\n",
    "    *   `x^T A⁻¹ x` gives us the variance of the prediction specifically for the feature vector `x`. If we have seen feature vectors similar to `x` many times before, this term will be small. If `x` represents a new, unseen combination of user and product features, this term will be large.\n",
    "    *   `α` (alpha) is a hyperparameter that you control. It scales how much the algorithm values exploration. A higher `α` makes the algorithm more adventurous.\n",
    "\n",
    "The algorithm then simply chooses the product with the **highest combined score**.\n",
    "\n",
    "**The Intuition:**\n",
    "\n",
    "*   If a product has a high predicted CTR and we are very certain about it (low uncertainty bonus), it gets a high score. **(Pure Exploitation)**\n",
    "*   If a product has a mediocre predicted CTR but we are very uncertain about it (high uncertainty bonus), it can also get a high score. Choosing this product is an act of **exploration**. By trying it, we get a new data point, which reduces our uncertainty (updating `A` and `b`) and helps us learn its true value for the future.\n",
    "\n",
    "This elegant combination allows LinUCB to learn efficiently. It focuses its exploration on the parts of the feature space where its knowledge is weakest, leading to rapid convergence.\n",
    "\n",
    "#### **2.3 Implementing the LinUCB Agent for Zooplus**\n",
    "\n",
    "Now, let's translate this theory into practice. We will create a Python class for a single LinUCB \"arm\" (representing one product) and a main agent class that manages all the arms.\n",
    "\n",
    "For our feature vector `x`, we will do something simple and effective: we will **concatenate the user's embedding with the product's category features**. But where do we get a user embedding for a contextual bandit? We can't use the one from the batched model directly, as it was trained for a different task.\n",
    "\n",
    "Instead, we will create a new set of user embeddings, one for each *persona*. This is a reasonable simplification for our simulation. In a real system, these could be embeddings learned from user demographics or other side information. The product features will be the one-hot encoded categories we already have in our simulator.\n",
    "\n",
    "**Code Block 2.1: The LinUCB Implementation**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinUCBArm:\n",
    "    \"\"\"Represents a single arm in the LinUCB algorithm.\"\"\"\n",
    "    def __init__(self, arm_index, d, alpha):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arm_index (int): The index of the arm (e.g., product_id).\n",
    "            d (int): The dimensionality of the feature vector.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.arm_index = arm_index\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize A as a d x d identity matrix.\n",
    "        # This corresponds to a standard Bayesian linear regression prior.\n",
    "        self.A = np.identity(d)\n",
    "        \n",
    "        # Initialize b as a d x 1 zero vector.\n",
    "        self.b = np.zeros([d, 1])\n",
    "\n",
    "    def calc_p(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the score for this arm given a feature vector x.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): A d-dimensional feature vector.\n",
    "        \n",
    "        Returns:\n",
    "            The UCB score for this arm.\n",
    "        \"\"\"\n",
    "        # Ensure x is a column vector\n",
    "        x = x.reshape(-1, 1)\n",
    "        \n",
    "        # Calculate A_inv and theta_hat\n",
    "        A_inv = np.linalg.inv(self.A)\n",
    "        theta_hat = A_inv.dot(self.b)\n",
    "        \n",
    "        # Calculate the UCB score\n",
    "        p = theta_hat.T.dot(x) + self.alpha * np.sqrt(x.T.dot(A_inv).dot(x))\n",
    "        \n",
    "        return p\n",
    "\n",
    "    def update(self, x, reward):\n",
    "        \"\"\"\n",
    "        Updates the A and b matrices for this arm.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The feature vector for the interaction.\n",
    "            reward (int): The observed reward (0 or 1).\n",
    "        \"\"\"\n",
    "        x = x.reshape(-1, 1)\n",
    "        self.A += x.dot(x.T)\n",
    "        self.b += reward * x\n",
    "\n",
    "class LinUCBAgent:\n",
    "    \"\"\"The main agent that manages all the LinUCB arms.\"\"\"\n",
    "    def __init__(self, n_products, user_features, product_features, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_products (int): The number of arms (products).\n",
    "            user_features (dict): A dict mapping persona name to a feature vector.\n",
    "            product_features (np.array): A matrix of one-hot encoded product categories.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.user_features = user_features\n",
    "        self.product_features = product_features\n",
    "        self.n_products = n_products\n",
    "        \n",
    "        # The dimensionality of our combined feature vector\n",
    "        d = list(user_features.values())[0].shape[0] + product_features.shape[1]\n",
    "        \n",
    "        # Create a list of arms\n",
    "        self.arms = [LinUCBArm(i, d, alpha) for i in range(n_products)]\n",
    "\n",
    "    def _create_feature_vector(self, persona, product_id):\n",
    "        \"\"\"Creates the concatenated feature vector x.\"\"\"\n",
    "        user_feat = self.user_features[persona]\n",
    "        product_feat = self.product_features[product_id]\n",
    "        return np.concatenate([user_feat, product_feat])\n",
    "\n",
    "    def choose_action(self, user_persona):\n",
    "        \"\"\"\n",
    "        Chooses the best product to recommend for the given user persona.\n",
    "        \n",
    "        Returns:\n",
    "            The product_id of the chosen action.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for product_id in range(self.n_products):\n",
    "            # Create the feature vector for this user-product pair\n",
    "            x = self._create_feature_vector(user_persona, product_id)\n",
    "            \n",
    "            # Calculate the score for this arm\n",
    "            score = self.arms[product_id].calc_p(x)\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Choose the arm with the highest score (break ties randomly)\n",
    "        max_score = np.max(scores)\n",
    "        best_arms = np.where(scores == max_score)[0]\n",
    "        chosen_arm = np.random.choice(best_arms)\n",
    "        \n",
    "        return chosen_arm\n",
    "\n",
    "    def update(self, chosen_arm, user_persona, reward):\n",
    "        \"\"\"Updates the agent after an action is taken.\"\"\"\n",
    "        x = self._create_feature_vector(user_persona, chosen_arm)\n",
    "        self.arms[chosen_arm].update(x, reward)\n",
    "\n",
    "# --- Setup for the LinUCB Agent ---\n",
    "\n",
    "# Create simple, random embeddings for our user personas\n",
    "persona_embedding_dim = 8\n",
    "user_features_for_bandit = {\n",
    "    name: np.random.rand(persona_embedding_dim) \n",
    "    for name in sim.personas.keys()\n",
    "}\n",
    "\n",
    "# The product features are the one-hot encoded categories from the simulator\n",
    "product_features_for_bandit = sim.product_features\n",
    "\n",
    "# Instantiate the agent\n",
    "linucb_agent = LinUCBAgent(\n",
    "    n_products=sim.n_products,\n",
    "    user_features=user_features_for_bandit,\n",
    "    product_features=product_features_for_bandit,\n",
    "    alpha=1.5 # Let's be a bit adventurous\n",
    ")\n",
    "\n",
    "print(\"LinUCB Agent created successfully.\")\n",
    "d = list(user_features_for_bandit.values())[0].shape[0] + product_features_for_bandit.shape[1]\n",
    "print(f\"Feature vector dimensionality (d): {d}\")\n",
    "```\n",
    "\n",
    "With the agent class defined and instantiated, we are now ready for the main event: a head-to-head competition. We will create a simulation loop that puts our new, adaptive `LinUCBAgent` against the static, pre-trained `MLPRecommender` from Chapter 1. This will allow us to see, step-by-step, how an online learning agent behaves compared to its offline counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb07777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinUCB Agent created successfully.\n",
      "Feature vector dimensionality (d): 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinUCBArm:\n",
    "    \"\"\"Represents a single arm in the LinUCB algorithm.\"\"\"\n",
    "    def __init__(self, arm_index, d, alpha):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arm_index (int): The index of the arm (e.g., product_id).\n",
    "            d (int): The dimensionality of the feature vector.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.arm_index = arm_index\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize A as a d x d identity matrix.\n",
    "        # This corresponds to a standard Bayesian linear regression prior.\n",
    "        self.A = np.identity(d)\n",
    "        \n",
    "        # Initialize b as a d x 1 zero vector.\n",
    "        self.b = np.zeros([d, 1])\n",
    "\n",
    "    def calc_p(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the score for this arm given a feature vector x.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): A d-dimensional feature vector.\n",
    "        \n",
    "        Returns:\n",
    "            The UCB score for this arm.\n",
    "        \"\"\"\n",
    "        # Ensure x is a column vector\n",
    "        x = x.reshape(-1, 1)\n",
    "        \n",
    "        # Calculate A_inv and theta_hat\n",
    "        A_inv = np.linalg.inv(self.A)\n",
    "        theta_hat = A_inv.dot(self.b)\n",
    "        \n",
    "        # Calculate the UCB score\n",
    "        p = theta_hat.T.dot(x) + self.alpha * np.sqrt(x.T.dot(A_inv).dot(x))\n",
    "        \n",
    "        return p\n",
    "\n",
    "    def update(self, x, reward):\n",
    "        \"\"\"\n",
    "        Updates the A and b matrices for this arm.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The feature vector for the interaction.\n",
    "            reward (int): The observed reward (0 or 1).\n",
    "        \"\"\"\n",
    "        x = x.reshape(-1, 1)\n",
    "        self.A += x.dot(x.T)\n",
    "        self.b += reward * x\n",
    "\n",
    "class LinUCBAgent:\n",
    "    \"\"\"The main agent that manages all the LinUCB arms.\"\"\"\n",
    "    def __init__(self, n_products, user_features, product_features, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_products (int): The number of arms (products).\n",
    "            user_features (dict): A dict mapping persona name to a feature vector.\n",
    "            product_features (np.array): A matrix of one-hot encoded product categories.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.user_features = user_features\n",
    "        self.product_features = product_features\n",
    "        self.n_products = n_products\n",
    "        \n",
    "        # The dimensionality of our combined feature vector\n",
    "        d = list(user_features.values())[0].shape[0] + product_features.shape[1]\n",
    "        \n",
    "        # Create a list of arms\n",
    "        self.arms = [LinUCBArm(i, d, alpha) for i in range(n_products)]\n",
    "\n",
    "    def _create_feature_vector(self, persona, product_id):\n",
    "        \"\"\"Creates the concatenated feature vector x.\"\"\"\n",
    "        user_feat = self.user_features[persona]\n",
    "        product_feat = self.product_features[product_id]\n",
    "        return np.concatenate([user_feat, product_feat])\n",
    "\n",
    "    def choose_action(self, user_persona):\n",
    "        \"\"\"\n",
    "        Chooses the best product to recommend for the given user persona.\n",
    "        \n",
    "        Returns:\n",
    "            The product_id of the chosen action.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for product_id in range(self.n_products):\n",
    "            # Create the feature vector for this user-product pair\n",
    "            x = self._create_feature_vector(user_persona, product_id)\n",
    "            \n",
    "            # Calculate the score for this arm\n",
    "            score = self.arms[product_id].calc_p(x)\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Choose the arm with the highest score (break ties randomly)\n",
    "        max_score = np.max(scores)\n",
    "        best_arms = np.where(scores == max_score)[0]\n",
    "        chosen_arm = np.random.choice(best_arms)\n",
    "        \n",
    "        return chosen_arm\n",
    "\n",
    "    def update(self, chosen_arm, user_persona, reward):\n",
    "        \"\"\"Updates the agent after an action is taken.\"\"\"\n",
    "        x = self._create_feature_vector(user_persona, chosen_arm)\n",
    "        self.arms[chosen_arm].update(x, reward)\n",
    "\n",
    "# --- Setup for the LinUCB Agent ---\n",
    "\n",
    "# Create simple, random embeddings for our user personas\n",
    "persona_embedding_dim = 8\n",
    "user_features_for_bandit = {\n",
    "    name: np.random.rand(persona_embedding_dim) \n",
    "    for name in sim.personas.keys()\n",
    "}\n",
    "\n",
    "# The product features are the one-hot encoded categories from the simulator\n",
    "product_features_for_bandit = sim.product_features\n",
    "\n",
    "# Instantiate the agent\n",
    "linucb_agent = LinUCBAgent(\n",
    "    n_products=sim.n_products,\n",
    "    user_features=user_features_for_bandit,\n",
    "    product_features=product_features_for_bandit,\n",
    "    alpha=1.5 # Let's be a bit adventurous\n",
    ")\n",
    "\n",
    "print(\"LinUCB Agent created successfully.\")\n",
    "d = list(user_features_for_bandit.values())[0].shape[0] + product_features_for_bandit.shape[1]\n",
    "print(f\"Feature vector dimensionality (d): {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79043812",
   "metadata": {},
   "source": [
    "**A Critical Note on Scalability: From Disjoint to Global Models**\n",
    "\n",
    "As a person moving from theory to practice, it is natural to ask: how does this scale to a catalog of 10,000 or 1,000,000 products? A naive implementation where each product is an independent \"arm\" with its own parameters to learn would fail for two reasons: it would be too slow to score every item, and it would require too much data to learn about every single item individually.\n",
    "\n",
    "The industry-standard solution involves two key ideas:\n",
    "1.  **Two-Stage Recommendation:** A fast *candidate generation* model first selects a few hundred relevant items from the vast catalog. Then, a sophisticated *ranking* model, like our bandit, scores only this small candidate set.\n",
    "2.  **Parameter Sharing:** Instead of learning a separate model for each arm (a \"disjoint\" model), we learn one **single, global model** that is shared across all arms. This allows the model to generalize. Learning that a user likes one brand of puppy food immediately informs the model's predictions for *all* brands of puppy food, because they share common features (e.g., `category=Dog Food`).\n",
    "\n",
    "We will now implement this more scalable \"Global LinUCB\" model. Notice that this is actually simpler: we no longer need a separate class for each arm. The agent itself will manage a single set of parameters.\n",
    "\n",
    "#### **2.4 Implementing a Scalable LinUCB Agent**\n",
    "\n",
    "Let's build our agent. It will maintain a single `A` matrix and `b` vector. When asked to choose an action, it will score a list of candidate products (for our simulation, this will be the *entire* catalog, but in a real system, it would be a smaller set) and pick the one with the highest UCB score.\n",
    "\n",
    "**Code Block 2.2: The Scalable Global LinUCB Agent**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class GlobalLinUCBAgent:\n",
    "    \"\"\"A single, global LinUCB model that shares parameters across all arms.\"\"\"\n",
    "    def __init__(self, d, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d (int): The dimensionality of the feature vector.\n",
    "            alpha (float): The exploration parameter.\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize A and b for the single, global model\n",
    "        self.A = np.identity(d)\n",
    "        self.b = np.zeros([d, 1])\n",
    "        self.theta_hat = np.zeros([d, 1])\n",
    "        self.A_inv = np.identity(d)\n",
    "\n",
    "    def _update_theta(self):\n",
    "        \"\"\"Internal method to recalculate theta_hat after an update.\"\"\"\n",
    "        self.A_inv = np.linalg.inv(self.A)\n",
    "        self.theta_hat = self.A_inv.dot(self.b)\n",
    "\n",
    "    def choose_action(self, user_features, product_features_matrix):\n",
    "        \"\"\"\n",
    "        Chooses the best product to recommend for the given user.\n",
    "        \n",
    "        Args:\n",
    "            user_features (np.array): The feature vector for the current user.\n",
    "            product_features_matrix (np.array): A matrix where each row is the \n",
    "                                                feature vector for a product.\n",
    "        \n",
    "        Returns:\n",
    "            The product_id of the chosen action.\n",
    "        \"\"\"\n",
    "        n_candidates = product_features_matrix.shape[0]\n",
    "        \n",
    "        # Create the full feature matrix for all candidates\n",
    "        # We tile the user features and concatenate them with the product features\n",
    "        user_features_tiled = np.tile(user_features, (n_candidates, 1))\n",
    "        full_feature_matrix = np.concatenate([user_features_tiled, product_features_matrix], axis=1)\n",
    "        \n",
    "        # Calculate scores for all candidates in a vectorized way\n",
    "        predicted_rewards = full_feature_matrix @ self.theta_hat\n",
    "        \n",
    "        # Calculate uncertainty bonus for all candidates\n",
    "        # This is the most computationally intensive part\n",
    "        uncertainty = np.sqrt(\n",
    "            np.sum((full_feature_matrix @ self.A_inv) * full_feature_matrix, axis=1)\n",
    "        )\n",
    "        \n",
    "        scores = predicted_rewards.flatten() + self.alpha * uncertainty\n",
    "        \n",
    "        # Choose the arm with the highest score\n",
    "        chosen_arm = np.argmax(scores)\n",
    "        \n",
    "        return chosen_arm\n",
    "\n",
    "    def update(self, x, reward):\n",
    "        \"\"\"\n",
    "        Updates the global A and b matrices.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The feature vector for the chosen user-item interaction.\n",
    "            reward (int): The observed reward (0 or 1).\n",
    "        \"\"\"\n",
    "        x = x.reshape(-1, 1)\n",
    "        self.A += x.dot(x.T)\n",
    "        self.b += reward * x\n",
    "        self._update_theta() # Recalculate theta after the update\n",
    "\n",
    "# --- Helper function to create the feature vector ---\n",
    "def create_feature_vector(user_persona, product_id, user_features_map, product_features_matrix):\n",
    "    \"\"\"Creates the concatenated feature vector for a single (user, product) pair.\"\"\"\n",
    "    user_feat = user_features_map[user_persona]\n",
    "    product_feat = product_features_matrix[product_id]\n",
    "    return np.concatenate([user_feat, product_feat])\n",
    "\n",
    "# --- Setup for the LinUCB Agent ---\n",
    "persona_embedding_dim = 8\n",
    "user_features_for_bandit = {\n",
    "    name: np.random.rand(persona_embedding_dim) \n",
    "    for name in sim.personas.keys()\n",
    "}\n",
    "product_features_for_bandit = sim.product_features\n",
    "d = persona_embedding_dim + product_features_for_bandit.shape[1]\n",
    "\n",
    "# Instantiate the scalable agent\n",
    "linucb_agent = GlobalLinUCBAgent(d=d, alpha=1.5)\n",
    "\n",
    "print(\"Scalable Global LinUCB Agent created successfully.\")\n",
    "print(f\"Feature vector dimensionality (d): {d}\")\n",
    "```\n",
    "This new agent is now ready for the online arena. Notice how much cleaner it is—we manage one set of parameters, and the `choose_action` method is fully vectorized for efficiency. We are now prepared for the head-to-head comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892d3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

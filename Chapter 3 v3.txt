```python
import torch

if torch.backends.mps.is_available():
    print("✅ MPS (Metal) backend is available and enabled.")
else:
    print("❌ MPS not available.")
```

    ✅ MPS (Metal) backend is available and enabled.



```python
# This script sets up the environment for the Zooplus recommendation problem
# and implements the MLP recommender model, LinUCB agent, and feature engineering.
# ==============================================================================
# All Required Imports
# ==============================================================================
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import OneHotEncoder

# ==============================================================================
# Device Configuration
# ==============================================================================
if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")
    print("✅ Using MPS (Apple Silicon) device for PyTorch.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("✅ Using CUDA device for PyTorch.")
else:
    device = torch.device("cpu")
    print("✅ Using CPU device for PyTorch.")

# ==============================================================================
# From Chapter 2: The Final Simulation Environment with Market Shock Capability
# ==============================================================================
class ZooplusSimulatorShock:
    """
    A simulated environment for the Zooplus recommendation problem, capable of
    introducing a "market shock" to test model adaptability.

    This class manages:
    1. A product catalog with features (category).
    2. A set of user personas with distinct preferences.
    3. A stochastic reward function to simulate user clicks (CTR).
    4. A method to dynamically alter the reward landscape (market shock).
    """
    def __init__(self, n_products=50, n_users=1000, seed=42):
        self.rng = np.random.default_rng(seed)
        self.n_products = n_products
        self.n_users = n_users
        
        self.products = self._create_product_catalog()
        self.personas = self._create_user_personas()
        self.user_to_persona_map = self._assign_users_to_personas()
        print("Zooplus Simulator Initialized.")

    def _create_product_catalog(self):
        product_ids = range(self.n_products)
        categories = ['Fish Supplies', 'Cat Food', 'Dog Food', 'Dog Toy', 'Cat Toy']
        num_per_category = self.n_products // len(categories)
        cat_list = []
        for cat in categories:
            cat_list.extend([cat] * num_per_category)
        cat_list.extend(self.rng.choice(categories, self.n_products - len(cat_list)))
        
        product_data = {
            'product_id': product_ids,
            'category': self.rng.permutation(cat_list)
        }
        return pd.DataFrame(product_data).set_index('product_id')

    def _create_user_personas(self):
        return {
            'new_puppy_parent': {'Dog Food': 0.40, 'Dog Toy': 0.50, 'Cat Food': 0.10, 'Cat Toy': 0.05, 'Fish Supplies': 0.02},
            'cat_connoisseur':  {'Dog Food': 0.05, 'Dog Toy': 0.02, 'Cat Food': 0.55, 'Cat Toy': 0.45, 'Fish Supplies': 0.05},
            'budget_shopper':   {'Dog Food': 0.25, 'Dog Toy': 0.15, 'Cat Food': 0.40, 'Cat Toy': 0.20, 'Fish Supplies': 0.20},
            'fish_hobbyist':    {'Dog Food': 0.02, 'Dog Toy': 0.02, 'Cat Food': 0.10, 'Cat Toy': 0.08, 'Fish Supplies': 0.60}
        }
        
    def _assign_users_to_personas(self):
        persona_names = list(self.personas.keys())
        return {user_id: self.rng.choice(persona_names) for user_id in range(self.n_users)}

    def get_true_ctr(self, user_id, product_id):
        if user_id not in self.user_to_persona_map or product_id not in self.products.index:
            return 0.0
        persona_name = self.user_to_persona_map[user_id]
        persona_prefs = self.personas[persona_name]
        product_category = self.products.loc[product_id, 'category']
        return persona_prefs.get(product_category, 0.01)

    def get_reward(self, user_id, product_id):
        click_prob = self.get_true_ctr(user_id, product_id)
        return self.rng.binomial(1, click_prob)

    def get_random_user(self):
        return self.rng.integers(0, self.n_users)
    
    def introduce_market_shock(self):
        """
        Dramatically alters a persona's preferences for an EXISTING category.
        This changes the reward dynamics without altering the feature dimension.
        """
        target_persona = 'budget_shopper'
        target_category = 'Fish Supplies'
        print(f"\n--- MARKET SHOCK! ---")
        print(f"Updating '{target_persona}' persona preferences for '{target_category}'.")
        self.personas[target_persona][target_category] = 0.80 # New, very high CTR
        self.personas[target_persona]['Dog Food'] = 0.10
        self.personas[target_persona]['Cat Food'] = 0.15
        print("Environment reward function has been updated.\n")

# ==============================================================================
# From Chapter 1: The Batched Baseline Model Architecture
# ==============================================================================
class MLPRecommender(nn.Module):
    """
    The "Embedding + MLP" baseline model from Chapter 1. It is trained offline
    on a static dataset.
    """
    def __init__(self, n_users, n_products, embedding_dim=32):
        super().__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.product_embedding = nn.Embedding(n_products, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 64)
        self.fc2 = nn.Linear(64, 32)
        self.output = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, user_ids, product_ids):
        user_embeds = self.user_embedding(user_ids)
        product_embeds = self.product_embedding(product_ids)
        x = torch.cat([user_embeds, product_embeds], dim=1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.output(x)
        return self.sigmoid(x)

def get_batch_model_predictions(model, user_id, n_products, device):
    """
    Utility function to get the trained batched model's predicted CTR for
    all products for a given user. Used for evaluation in the online simulation.
    """
    model.eval()
    with torch.no_grad():
        user_tensor = torch.tensor([user_id] * n_products, dtype=torch.long).to(device)
        product_tensor = torch.tensor(range(n_products), dtype=torch.long).to(device)
        scores = model(user_tensor, product_tensor).squeeze().cpu().numpy()
    return scores

# ==============================================================================
# From Chapter 2: The LinUCB Agent and its Feature Engineer
# ==============================================================================
class FeatureEngineer:
    """
    Helper class to create one-hot encoded feature vectors for the LinUCB agent.
    It combines user (persona) and item (category) features.
    """
    def __init__(self, sim):
        self.user_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        self.product_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        
        user_personas = np.array(list(sim.personas.keys())).reshape(-1, 1)
        product_categories = np.array(sim.products['category'].unique()).reshape(-1, 1)
        
        self.user_encoder.fit(user_personas)
        self.product_encoder.fit(product_categories)
        
        self.feature_dim = self.user_encoder.get_feature_names_out().shape[0] + \
                           self.product_encoder.get_feature_names_out().shape[0]
        print(f"Feature Engineer Initialized. Feature dimension: {self.feature_dim}")

    def create_features(self, user_id, sim):
        """Creates a full (n_products x feature_dim) feature matrix for a user."""
        persona = sim.user_to_persona_map[user_id]
        user_one_hot = self.user_encoder.transform([[persona]])[0]
        
        all_prod_cats = sim.products['category'].values.reshape(-1, 1)
        prod_one_hots = self.product_encoder.transform(all_prod_cats)
        
        user_features = np.tile(user_one_hot, (sim.n_products, 1))
        
        full_feature_matrix = np.concatenate([user_features, prod_one_hots], axis=1)
        return full_feature_matrix

class LinUCBAgent:
    """
    Implements the LinUCB algorithm with a disjoint model for each arm.
    This is the adaptive online learning agent from Chapter 2.
    """
    def __init__(self, n_products, feature_dim, alpha=1.0):
        self.n_products = n_products
        self.alpha = alpha
        self.feature_dim = feature_dim
        
        # A: (d x d) matrix for each arm
        self.A = [np.identity(feature_dim) for _ in range(n_products)]
        # b: (d x 1) vector for each arm
        self.b = [np.zeros((feature_dim, 1)) for _ in range(n_products)]
        print("LinUCB Agent Initialized.")

    def predict(self, feature_vectors):
        """Calculates the UCB score for each arm and chooses the best one."""
        scores = np.zeros(self.n_products)
        for arm_idx in range(self.n_products):
            x = feature_vectors[arm_idx].reshape(-1, 1) # (d x 1)
            
            # This is a naive implementation that re-calculates the inverse
            # at every step. For small d, this is acceptable.
            A_inv = np.linalg.inv(self.A[arm_idx])
            theta_hat = A_inv @ self.b[arm_idx]
            
            exploit_term = (theta_hat.T @ x).item()
            explore_term = self.alpha * np.sqrt(x.T @ A_inv @ x).item()
            
            scores[arm_idx] = exploit_term + explore_term
            
        return np.argmax(scores)

    def update(self, arm_idx, reward, feature_vector):
        """Updates the model parameters for the chosen arm."""
        x = feature_vector.reshape(-1, 1) # (d x 1)
        self.A[arm_idx] += x @ x.T
        self.b[arm_idx] += reward * x
```

    ✅ Using MPS (Apple Silicon) device for PyTorch.


### **Chapter 3. Generalization in Contextual Bandits: From Theory to Scalable Practice**

In the preceding chapter, we took a significant step forward. By replacing the static, batched learning paradigm with the online, adaptive framework of contextual bandits, we engineered a recommender system capable of responding to a dynamic environment. Our `LinUCBAgent` demonstrated a clear superiority over the `MLPRecommender` when faced with a "market shock," proving the immense value of continuous learning.


However, the solution presented in Chapter 2, while a crucial stepping stone, harbors a fundamental architectural flaw that renders it unsuitable for the vast, complex information ecosystems of the real world. This chapter is dedicated to identifying, understanding, and decisively overcoming this limitation. Our goal is to synthesize the adaptive learning prowess of bandits with the powerful generalization capabilities of deep neural networks, culminating in an agent that is both intelligent and scalable.


### **3.1. The Generalization Imperative: Beyond Disjoint Models**


The `LinUCBAgent` is built upon what is known as a **disjoint model**. It maintains a completely separate set of parameters—an evidence matrix $A_a$ and a reward vector $b_a$—for each and every arm $a$ (in our case, each product). This design choice, while simple to implement, leads to profound and crippling inefficiencies, which can be categorized into two primary domains: statistical and computational.


#### **An Intuitive Failing: The Committee of Amnesiac Specialists**


Let us first build an intuition for the problem. Imagine our `LinUCBAgent` not as a single agent, but as a committee of $K$ independent "specialists," one for each of the $K$ products in our catalog. The specialist for "Product #23: Squeaky Ball for Dogs" learns over time, through interactions, that users with the `new_puppy_parent` persona are highly likely to click on its product. It diligently updates its parameters, $A_{23}$ and $b_{23}$, becoming an expert on the Squeaky Ball.


Now, let us introduce a new, similar product into the catalog: "Product #51: Chew Rope for Dogs." A new specialist, Specialist #51, is assigned to this product. This new specialist begins its life in a state of complete ignorance. It has no access to the hard-won knowledge of Specialist #23. It does not know that puppy owners, who love squeaky toys, are also very likely to be interested in chew ropes. It must learn this correlation from scratch, wasting valuable user interactions to re-discover a pattern that is, to any intelligent observer, blindingly obvious.


This is the core of the statistical inefficiency: **the model cannot generalize learned patterns across related arms.** For all its online learning capability, the `LinUCBAgent` is fundamentally a collection of amnesiacs, each confined to its own silo of experience. This stands in stark contrast to the `MLPRecommender` from Chapter 1. While that model was static, its use of shared embedding layers (`self.product_embedding`) meant that what it learned about the Squeaky Ball implicitly informed its understanding of the Chew Rope, as they would be mapped to nearby points in the embedding space. Our objective is to achieve this same representational power, but within a dynamic, online learning framework.


#### **A Formal Analysis of the Disjoint Model's Shortcomings**


The intuition of the "amnesiac specialist" is reflected in a formal analysis of the algorithm's properties.


**1. Statistical Inefficiency and the Persistent Arm Cold-Start Problem**


The disjoint model suffers from a perpetual cold-start problem at the level of individual arms. Whenever a new product is added to the catalog, a new arm $a_{\text{new}}$ is created. For this arm, the agent has no prior data. Its parameters $A_{a_{\text{new}}}$ and $b_{a_{\text{new}}}$ are initialized to a non-informative prior (e.g., an identity matrix and a zero vector). Consequently, the agent must engage in extensive exploration for this new arm to reduce its uncertainty, even if it has already learned a great deal about hundreds of highly similar arms.


This means the **sample complexity**—the number of interactions required to achieve a certain level of performance—does not improve for new items as the agent gains more overall experience. An ideal agent would leverage its knowledge of $\text{(user, 'Dog Toy')}$ interactions to make an informed initial prediction for a *new* dog toy, requiring far less exploration to confirm its value. The disjoint model is incapable of this.


**2. Computational Infeasibility at Scale**


The second shortcoming is one of pure computational and memory cost. Let us analyze the memory requirements of the `LinUCBAgent`.


*   Let $K$ be the number of arms (products).
*   Let $d$ be the dimension of the feature vector $x$.


For each of the $K$ arms, the agent must store:
*   An evidence matrix $A_a$ of size $d \times d$.
*   A reward vector $b_a$ of size $d \times 1$.


The total memory required to store these parameters is therefore on the order of $O(K \cdot (d^2 + d))$, which simplifies to $O(K \cdot d>2)$.


In our simple simulation, these numbers are trivial ($K=50$, $d=9$). However, let's consider a realistic e-commerce scenario:
*   $K = 2,000,000$ products
*   $d = 200$ features (derived from user history, product metadata, etc.)


The memory required for the $A$ matrices alone would be:


$$\text{Memory} \approx K \cdot d^2 \cdot (\text{bytes per float})$$
$$\text{Memory} \approx 2 \times 10^6 \cdot (200)^2 \cdot 4 \text{ bytes}$$
$$\text{Memory} \approx 2 \times 10^6 \cdot 4 \times 10^4 \cdot 4 \text{ bytes}$$
$$\text{Memory} \approx 32 \times 10^{10} \text{ bytes} = 320 \text{ Gigabytes}$$


Storing and, more importantly, updating 320 GB of parameters *in real-time* for every user interaction is computationally prohibitive. The model's memory footprint scales linearly with the size of the product catalog, a fatal flaw for any large-scale industrial application.


In contrast, a single, shared model (like a neural network) has a memory footprint that is dependent on its own architecture (number of layers and neurons), but **independent of the number of arms $K$**. This is the path we must now pursue.


Our task is clear: we must design an agent that replaces the committee of disjoint linear models with a single, powerful function approximator—a neural network—that learns a shared representation across all users and items. This will allow the agent to generalize its knowledge, overcome the cold-start problem for new items, and maintain a fixed memory footprint, regardless of how large the catalog grows. This is the central challenge we will solve in this chapter.

Excellent. Let us proceed to the next logical step in our argument. We have established *what* we need to do—build a generalizing agent—and *why* the previous approach was insufficient. Now, we must establish *how*. To do so, we must first elevate our thinking from the specific implementation of LinUCB to the general principle that underpins it.

### **3.2. Abstracting the UCB Principle for Non-Linear Models**

The algorithm we call "LinUCB" is, in fact, a specific instantiation of a much broader, more powerful idea. The name itself gives us a clue: it is a **Lin**ear model applied to the **U**pper **C**onfidence **B**ound principle. The beauty of this principle is that it is not fundamentally tied to linearity. By abstracting this core concept, we can unlock the ability to pair it with any class of function approximator we choose, including the deep neural networks that are our ultimate goal.

To perform this abstraction, we will first dissect the LinUCB decision rule we used in Chapter 2, separating its components. Then, we will express the rule in a general form that makes no assumptions about the underlying model.

#### **Recap: The LinUCB Payoff Structure**

Recall that at each timestep $t$, for every available arm $a$, the `LinUCBAgent` computed a score. The arm with the highest score was chosen. This score was the sum of two critical components.

***
**Definition 3.1: LinUCB Payoff Score (Disjoint Model)**

At a given timestep $t$, for each arm $a$ from the set of all arms $K$, the `LinUCBAgent` selects the arm $a_t$ that maximizes the following score:

$$
a_t = \underset{a \in K}{\operatorname{argmax}} \left( \underbrace{\hat{\theta}_a^\top \mathbf{x}_{t,a}}_{\text{Exploitation Term}} + \underbrace{\alpha \sqrt{\mathbf{x}_{t,a}^\top \mathbf{A}_a^{-1} \mathbf{x}_{t,a}}}_{\text{Exploration Term}} \right)
$$

where:

- $\mathbf{x}_{t,a}$ is the $d$-dimensional feature vector (context) for arm $a$ at time $t$.
- $\mathbf{A}_a$ is the $d \times d$ evidence matrix for arm $a$, summarizing the features seen for that arm so far.
- $\hat{\theta}_a = \mathbf{A}_a^{-1} \mathbf{b}_a$ is the estimated linear coefficient vector for arm $a$.
- The first term, $\hat{\theta}_a^\top \mathbf{x}_{t,a}$, represents the **predicted reward**. It is the model's best estimate of the payoff, exploiting its current knowledge.
- The second term, $\alpha \sqrt{\mathbf{x}_{t,a}^\top \mathbf{A}_a^{-1} \mathbf{x}_{t,a}}$, represents the **uncertainty bonus**. It is a measure of the model's uncertainty in its estimate. This term encourages exploration. $\alpha$ is a hyperparameter that controls the trade-off.

***

The crucial insight is that this structure—**Predicted Reward + Uncertainty Bonus**—is a universal recipe for balancing exploitation and exploration. The linear model is just one way to calculate the ingredients.

#### **The Generalized UCB Principle**

We can now state the principle in its abstract form. We will replace the specific, linear calculations with generic functions, $f$ for the prediction and $\kappa$ for the uncertainty.

***
**Principle 3.1: The Generalized Upper Confidence Bound (UCB) Principle**

Let $f$ be a function approximator that, at time $t$, maps a context vector $\mathbf{x}_a$ to a predicted reward. Let $\kappa$ be a function that quantifies the uncertainty of the prediction made by $f$ for the same context $\mathbf{x}_a$. The generalized UCB selection rule is to choose the arm $a_t$ at each timestep $t$ according to:

$$
a_t = \underset{a \in K}{\operatorname{argmax}} \left( f_t(\mathbf{x}_a) + \kappa_t(\mathbf{x}_a) \right)
$$

where:

- $f_t(\mathbf{x}_a)$ is the **predicted payoff function**. It is the model's point estimate of the expected reward $\mathbb{E}[r \mid \mathbf{x}_a]$ based on the data observed up to time $t$.
- $\kappa_t(\mathbf{x}_a)$ is the **uncertainty quantification function**. It represents the "radius" of the confidence interval around the prediction $f_t(\mathbf{x}_a)$. It should be large for novel $(\mathbf{x}_a)$ pairs and shrink as more data is gathered for that region of the context space.

***

This abstraction is profoundly important. We have decoupled the explore-exploit strategy from the underlying predictive model. Our problem is no longer "how to do LinUCB." Instead, our research program for this chapter has been clarified into two distinct challenges:

1.  **Defining the Payoff Function $f_t$:** We will instantiate $f$ as a single, shared neural network. This network will take a context $\mathbf{x}_a$ as input and output a predicted CTR. This immediately solves the generalization and scalability problems identified in Section 3.1 (though some other scalability issues might be not so obvious and we will dive into that later). Let us denote our neural network by $f(\mathbf{x}; \Theta)$, where $\Theta$ represents all the trainable weights and biases of the network.

2.  **Deriving the Uncertainty Function $\kappa_t$:** This is the more intellectually demanding task. The elegant $\alpha \sqrt{\mathbf{x}^\top \mathbf{A}^{-1} \mathbf{x}}$ term was a direct consequence of the statistical properties of linear regression (specifically, the covariance of the parameter estimates). How do we derive a similar, theoretically-grounded measure of uncertainty for a complex, non-linear, and often over-parameterized model like a neural network?

The remainder of this chapter is dedicated to answering this second question. We will begin by exploring the most direct, theoretically "pure" method for deriving $\kappa_t$ for a neural network, a method that linearizes the model's behavior in its high-dimensional parameter space.

> **Remark 3.5:**
> The number of parameters $p$ in our `NeuralBanditNetwork` is the sum of all trainable weights and biases.
>
> Our network is defined as:
> *   $\text{input\_dim} = 9$ (from our `FeatureEngineer`)
> *   $\text{hidden\_dim} = 64$
>
> The architecture consists of two linear layers:
> 1.  **`self.layer1 = nn.Linear(in_features=9, out_features=64)`**
>     *   The weight matrix $W_1$ has dimensions $(64, 9)$. The number of weights is $64 \times 9 = 576$.
>     *   The bias vector $b_1$ has dimensions $(64,)$. The number of biases is $64$.
>     *   Total parameters for Layer 1: $576 + 64 = 640$.
>
> 2.  **`self.layer2 = nn.Linear(in_features=64, out_features=1)`**
>     *   The weight matrix $W_2$ has dimensions $(1, 64)$. The number of weights is $1 \times 64 = 64$.
>     *   The bias vector $b_2$ has dimensions $(1,)$. The number of biases is $1$.
>     *   Total parameters for Layer 2: $64 + 1 = 65$.
>
> **Total Parameters ($p$) = Parameters(Layer 1) + Parameters(Layer 2)**
> $$p = 640 + 65 = 705$$

### **3.4. A Sobering Analysis: The Computational Burden of Full Linearization**

The empirical demonstration in the previous section was illuminating. We felt the sluggishness of the canonical `NeuralUCBAgent`. Now, we must move from intuition to a formal analysis to understand precisely *why* it is so slow and how its costs scale. We will analyze both the time complexity (the number of operations) and the memory complexity (the storage requirements).

Let us define our key variables:
*   $K$: The number of arms (products).
*   $p$: The total number of trainable parameters in the neural network $f$.
*   $d$: The dimension of the input feature vector $\mathbf{x}$.

In our specific example, $K=50$, $d=9$, and as we have just established, $p=705$.

#### **Time Complexity: The Cost of a Single Decision**

The primary bottleneck is the `predict` method, which is executed at every timestep $t$ to choose an arm. Let us break down the operations inside its `for` loop, which runs $K$ times.

1.  **Forward Pass ($f(\mathbf{x}_{t,a}; \Theta)$):** The network computes a prediction for arm $a$. The cost of a forward pass through our two-layer MLP is roughly $O(d \cdot p_h + p_h \cdot 1)$, where $p_h$ is the number of neurons in the hidden layer. More generally, for any fixed network architecture, this cost is $O(p)$.

2.  **Gradient Computation ($\nabla_\Theta f$):** We then compute the $p$-dimensional gradient vector $\mathbf{g}_{t,a}$. The backpropagation algorithm is remarkably efficient, and its computational cost is on the same order as the forward pass. Thus, this step is also $O(p)$.

3.  **Uncertainty Calculation ($\mathbf{g}^\top \mathbf{A}^{-1} \mathbf{g}$):** This is the most expensive operation.
    *   First, we must have the inverse of $\mathbf{A}$, which is $\mathbf{A}^{-1}$. $\mathbf{A}$ is a $p \times p$ matrix. Inverting a matrix is a very costly operation, with a standard time complexity of $O(p^3)$. We were clever in our implementation and pre-computed this inverse once before the loop, so we will account for this cost separately.
    *   Let $\mathbf{v} = \mathbf{A}^{-1} \mathbf{g}$. This is a matrix-vector product, costing $O(p^2)$ operations.
    *   Then, we compute $\mathbf{g}^\top \mathbf{v}$. This is a vector dot product, costing $O(p)$ operations.
    *   The dominant cost within this step is the matrix-vector product: $O(p^2)$.

Combining these, the total cost *inside* the loop for a single arm is $O(p + p + p^2) = O(p^2)$. Since the loop runs $K$ times, the total cost of the loop is $K \cdot O(p^2) = O(K \cdot p^2)$.

The final cost of the `predict` method is the sum of the $O(p^3)$ matrix inversion and the $O(K \cdot p^2)$ loop:

**Time Complexity of `predict` = $O(p^3 + K \cdot p^2)$**

Let's substitute our numbers: $p=705$, $K=50$.
*   $p^3 \approx (7 \times 10^2)^3 = 343 \times 10^6 \approx 3.4 \times 10^8$ operations (for the inversion).
*   $K \cdot p^2 \approx 50 \cdot (7 \times 10^2)^2 = 50 \cdot 49 \times 10^4 \approx 2.5 \times 10^7$ operations (for the loop).

In this specific case, the matrix inversion dominates. However, in a system with many arms ($K \gg p$), the $O(K \cdot p^2)$ term would become the bottleneck. In either case, the complexity is polynomial in $p$, the number of network parameters. For any reasonably deep or wide network, where $p$ can easily be in the hundreds of thousands or millions, this cost is astronomical and computationally infeasible for real-time decision-making.

#### **Memory Complexity: Storing the Evidence**

The memory requirements are simpler to analyze but equally damning. The agent must store the evidence matrix $\mathbf{A}$ and the reward vector $\mathbf{b}$.

*   $\mathbf{A}$ is a $p \times p$ matrix, requiring $O(p^2)$ storage.
*   $\mathbf{b}$ is a $p$-dimensional vector, requiring $O(p)$ storage.

**Memory Complexity = $O(p^2)$**

Again, let's substitute $p=705$.
*   Storage for $\mathbf{A}$ is $705 \times 705 \approx 500,000$ floating-point numbers. At 4 bytes per float, this is approximately $2$ Megabytes.

While 2 MB is manageable, this is for our tiny network. Consider a more realistic pre-trained model like ResNet-18, which has over 11 million parameters ($p \approx 1.1 \times 10^7$). The $\mathbf{A}$ matrix for such a network would require:
$$\text{Memory} \approx (1.1 \times 10^7)^2 \times 4\ \text{bytes} \approx 1.2 \times 10^{14} \times 4\ \text{bytes} \approx 484\ \text{Terabytes}$$

It is impossible to store, let alone invert, a matrix of this size.

#### **Conclusion of the Analysis**

The canonical NeuralUCB algorithm, while theoretically elegant, is a practical dead end. Its reliance on linearizing the model in the full $p$-dimensional parameter space leads to time and memory complexities that are polynomial in the number of network parameters. This renders the approach unusable for any deep learning model of meaningful size.

We are at a crossroads. We need the generalization power of a neural network, and we need the principled exploration of UCB. The full linearization approach is too costly. Is there a way to achieve a compromise? Can we find a hybrid approach that captures the spirit of UCB exploration without paying the ruinous computational price? This is the question that leads us to our final, practical algorithm.

#### **Rigor: Formal Definition of NeuralUCB**

We now formalize this intuition. Unlike the disjoint LinUCB model, we will now maintain a *single, shared* evidence matrix $\mathbf{A}$ and reward vector $\mathbf{b}$ for the entire agent.

---
**Definition 3.2: The NeuralUCB Algorithm (Full Linearization)**

Let $f(\mathbf{x}; \Theta)$ be a neural network with $p$ parameters, $\Theta \in \mathbb{R}^p$. The agent maintains a single $p \times p$ matrix $\mathbf{A}$ and a $p \times 1$ vector $\mathbf{b}$, initialized as $\mathbf{A} = \mathbf{I}_p$ (the $p \times p$ identity matrix) and $\mathbf{b} = \mathbf{0}_p$.

At each timestep $t$, the agent performs the following steps:

1.  **Gradient and Score Calculation:** For each available arm $a \in K$:
    a. Compute the feature vector $\mathbf{x}_{t,a}$.
    b. Compute the network's predicted reward: $\hat{r}_{t,a} = f(\mathbf{x}_{t,a}; \Theta_{t-1})$.
    c. Compute the gradient of the prediction with respect to the network parameters: $\mathbf{g}_{t,a} = \nabla_\Theta f(\mathbf{x}_{t,a}; \Theta_{t-1})$.
    d. Compute the UCB score:
    $$
    \text{score}_a = \hat{r}_{t,a} + \alpha \sqrt{\mathbf{g}_{t,a}^\top \mathbf{A}_{t-1}^{-1} \mathbf{g}_{t,a}}
    $$

2.  **Arm Selection:** Choose the arm with the highest score: $a_t = \operatorname{argmax}_{a \in K} \text{score}_a$.

3.  **Observation and Update:**
    a. Pull arm $a_t$ and observe the true reward $r_t$.
    b. Update the shared evidence matrix and reward vector using the gradient of the *chosen* arm, $\mathbf{g}_{t, a_t}$:
    $$
    \mathbf{A}_t = \mathbf{A}_{t-1} + \mathbf{g}_{t, a_t} \mathbf{g}_{t, a_t}^\top
    $$
    $$
    \mathbf{b}_t = \mathbf{b}_{t-1} + r_t \cdot \mathbf{g}_{t, a_t}
    $$
    c. Update the network's parameters $\Theta$ by performing one step of gradient descent on a suitable loss function $L$ (e.g., Mean Squared Error) between the prediction and the observed reward:
    $$
    \Theta_t = \Theta_{t-1} - \eta \cdot \nabla_\Theta L(f(\mathbf{x}_{t, a_t}; \Theta_{t-1}), r_t)
    $$

---
**Remark 3.3:** This formulation elegantly solves the generalization problem. The network $f$ is a single model that learns from all interactions. The UCB components $\mathbf{A}$ and $\mathbf{b}$ are also shared, accumulating evidence from all arms in a unified $p$-dimensional space. When a new arm is introduced, the network can immediately produce a meaningful prediction $\hat{r}$ and a corresponding gradient $\mathbf{g}$, leveraging everything it has learned from similar arms. The cold-start problem is vanquished.

However, a careful examination of Step 1 reveals a looming computational disaster, which we will now demonstrate through implementation.

#### **Application: Implementing the Canonical NeuralUCB**

To translate the theory into practice, we first need a neural network. We will then build an agent class that executes the logic from Definition 3.2.

**1. Motivate the Code:** Our goal here is to create a direct, literal implementation of the algorithm defined above. This will serve as a pedagogical tool to make the abstract formulas concrete and to expose, with undeniable clarity, the computational bottleneck inherent in the design.

**2. Introduce and Dissect the Code:**

First, we define a generic neural network for our bandit task. It's a simple MLP that will take the concatenated user and product features as input. Note that we now combine the features *before* they enter the model, which is a more standard approach for neural bandits compared to the separate embedding layers in our batched model.

```python
class NeuralBanditNetwork(nn.Module):
    """A simple MLP to be used as the function approximator f(x; Theta)."""
    def __init__(self, input_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_dim, 1)
        # No sigmoid here! The UCB formula works with unbounded scores.
        # We can apply a sigmoid later if we need a probability.

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.relu(self.layer1(x))
        x = self.layer2(x)
        return x
```

Next, we implement the agent itself. Pay close attention to the `predict` method, as this is where the computational cost is incurred.

```python
# Helper function to flatten gradients
def get_gradient_vector(model: nn.Module, loss: torch.Tensor) -> np.ndarray:
    """
    Computes the gradient of the loss with respect to all model parameters
    and returns them as a single flattened numpy vector.
    """
    model.zero_grad()
    loss.backward(retain_graph=True) # retain_graph is crucial for loops
    
    grads = []
    for param in model.parameters():
        if param.grad is not None:
            grads.append(param.grad.view(-1))
    
    grad_vector = torch.cat(grads)
    return grad_vector.detach().cpu().numpy()

class NeuralUCBAgent_Canonical:
    """
    Implements the canonical NeuralUCB algorithm with full linearization.
    WARNING: This is computationally very expensive and for educational purposes.
    """
    def __init__(self, feature_engineer, alpha=1.0, lr=0.01):
        self.feature_engineer = feature_engineer
        self.feature_dim = feature_engineer.feature_dim
        self.alpha = alpha
        
        self.model = NeuralBanditNetwork(self.feature_dim).to(device)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # Determine the number of parameters `p`
        self.p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Canonical NeuralUCB Initialized. Number of parameters (p): {self.p}")
        
        # A: (p x p) matrix, b: (p x 1) vector
        self.A = np.identity(self.p)
        self.b = np.zeros((self.p, 1))

    def predict(self, user_id, sim):
        """
        Calculates UCB score for every arm, one by one. This is the bottleneck.
        """
        self.model.eval()
        
        # Get the full feature matrix for the user for all products
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        n_products = sim.n_products
        scores = np.zeros(n_products)
        
        # Pre-compute the inverse of A to avoid doing it in the loop
        A_inv = np.linalg.inv(self.A)

        # === THE BOTTLENECK: K backward passes to make one decision ===
        for arm_idx in range(n_products):
            x_a = torch.tensor(feature_matrix[arm_idx], dtype=torch.float32).to(device)
            
            # 1. Get predicted reward
            predicted_reward_tensor = self.model(x_a)
            scores[arm_idx] = predicted_reward_tensor.item()
            
            # 2. Get gradient g_t,a
            # We treat the model's output as the "loss" to get its gradient
            g_a = get_gradient_vector(self.model, predicted_reward_tensor)
            
            # 3. Compute exploration bonus and add to score
            exploration_bonus = self.alpha * np.sqrt(g_a.T @ A_inv @ g_a)
            scores[arm_idx] += exploration_bonus
            
        # Clear grads after the loop
        self.model.zero_grad()
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        """
        Updates the shared A, b matrices and trains the neural network.
        """
        self.model.train()
        
        # Get feature vector for the chosen arm
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen = torch.tensor(feature_matrix[chosen_arm], dtype=torch.float32).to(device)
        
        # --- Update the UCB components (A and b) ---
        # First, re-compute the gradient for the chosen arm's prediction
        prediction_tensor = self.model(x_chosen)
        g_chosen = get_gradient_vector(self.model, prediction_tensor)
        
        # Reshape g for matrix operations
        g_chosen_vec = g_chosen.reshape(-1, 1)
        self.A += g_chosen_vec @ g_chosen_vec.T
        self.b += reward * g_chosen_vec
        
        # --- Update the neural network parameters ---
        self.optimizer.zero_grad()
        # Re-run forward pass for the training step
        prediction = self.model(x_chosen)
        loss = self.loss_fn(prediction, torch.tensor([reward], dtype=torch.float32).to(device))
        loss.backward()
        self.optimizer.step()

```

**3. Assemble and Conclude:** The code above is a complete, runnable implementation of the NeuralUCB algorithm as formally defined. The `predict` method is a direct translation of the mathematics: it iterates through every arm, computes a prediction, computes the full gradient with respect to all $p$ parameters, and uses that gradient to find the uncertainty bonus.

While theoretically sound, we have created an algorithm whose decision-making process is alarmingly expensive. In the next section, we will perform a rigorous analysis of this cost and demonstrate why this "first principles" approach, despite its elegance, is a dead end for any practical application.


```python
import torch
import torch.nn as nn
import numpy as np

class NeuralBanditNetwork(nn.Module):
    """A simple MLP to be used as the function approximator f(x; Theta)."""
    def __init__(self, input_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_dim, 1)
        # No sigmoid here! The UCB formula works with unbounded scores.
        # We can apply a sigmoid later if we need a probability.

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.relu(self.layer1(x))
        x = self.layer2(x)
        return x


def get_gradient_vector(model: nn.Module, loss: torch.Tensor) -> np.ndarray:
    """
    Computes the gradient of the loss with respect to all model parameters
    and returns them as a single flattened numpy vector.
    """
    model.zero_grad()
    loss.backward(retain_graph=True) # retain_graph is crucial for loops
    
    grads = []
    for param in model.parameters():
        if param.grad is not None:
            grads.append(param.grad.view(-1))
    
    grad_vector = torch.cat(grads)
    return grad_vector.detach().cpu().numpy()

class NeuralUCBAgent_Canonical:
    """
    Implements the canonical NeuralUCB algorithm with full linearization.
    WARNING: This is computationally very expensive and for educational purposes.
    """
    def __init__(self, feature_engineer, alpha=1.0, lr=0.01):
        self.feature_engineer = feature_engineer
        self.feature_dim = feature_engineer.feature_dim
        self.alpha = alpha
        
        self.model = NeuralBanditNetwork(self.feature_dim).to(device)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # Determine the number of parameters `p`
        self.p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Canonical NeuralUCB Initialized. Number of parameters (p): {self.p}")
        
        # A: (p x p) matrix, b: (p x 1) vector
        self.A = np.identity(self.p)
        self.b = np.zeros((self.p, 1))

    def predict(self, user_id, sim):
        """
        Calculates UCB score for every arm, one by one. This is the bottleneck.
        """
        self.model.eval()
        
        # Get the full feature matrix for the user for all products
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        n_products = sim.n_products
        scores = np.zeros(n_products)
        
        # Pre-compute the inverse of A to avoid doing it in the loop
        A_inv = np.linalg.inv(self.A)

        # === THE BOTTLENECK: K backward passes to make one decision ===
        for arm_idx in range(n_products):
            x_a = torch.tensor(feature_matrix[arm_idx], dtype=torch.float32).to(device)
            
            # 1. Get predicted reward
            predicted_reward_tensor = self.model(x_a)
            scores[arm_idx] = predicted_reward_tensor.item()
            
            # 2. Get gradient g_t,a
            # We treat the model's output as the "loss" to get its gradient
            g_a = get_gradient_vector(self.model, predicted_reward_tensor)
            
            # 3. Compute exploration bonus and add to score
            exploration_bonus = self.alpha * np.sqrt(g_a.T @ A_inv @ g_a)
            scores[arm_idx] += exploration_bonus
            
        # Clear grads after the loop
        self.model.zero_grad()
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        """
        Updates the shared A, b matrices and trains the neural network.
        """
        self.model.train()
        
        # Get feature vector for the chosen arm
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen = torch.tensor(feature_matrix[chosen_arm], dtype=torch.float32).to(device)
        
        # --- Update the UCB components (A and b) ---
        # First, re-compute the gradient for the chosen arm's prediction
        prediction_tensor = self.model(x_chosen)
        g_chosen = get_gradient_vector(self.model, prediction_tensor)
        
        # Reshape g for matrix operations
        g_chosen_vec = g_chosen.reshape(-1, 1)
        self.A += g_chosen_vec @ g_chosen_vec.T
        self.b += reward * g_chosen_vec
        
        # --- Update the neural network parameters ---
        self.optimizer.zero_grad()
        # Re-run forward pass for the training step
        prediction = self.model(x_chosen)
        loss = self.loss_fn(prediction, torch.tensor([reward], dtype=torch.float32).to(device))
        loss.backward()
        self.optimizer.step()
```

Before we drive into the formal complexity analysis, we will run our `NeuralUCBAgent_Canonical`. (The delay you we will experience is not a bug).

### **3.3.1. A Practical Interlude: Experiencing the Bottleneck**

We have constructed our `NeuralUCBAgent_Canonical` as a direct implementation of the theory. Let us now place it within our simulation environment for a brief run and observe its performance not in terms of recommendation quality (we are not running it long enough for that), but in terms of raw computational speed.

#### **Experimental Setup**

Our experiment will be simple:
1.  Initialize the simulation environment, the feature engineer, and our `NeuralUCBAgent_Canonical`.
2.  Run the simulation for a modest number of timesteps (e.g., 500).
3.  At each step, we will ask the agent to `predict` an arm and then `update` its model based on the observed reward.
4.  We will wrap the main loop in a timer to measure the total execution time.

#### **Code: A Timed Run of the Canonical Agent**

The following code block assembles the necessary components and runs the experiment. (I have added print statements to show the progress) Pay close attention to how slowly the "Evaluating arm..." message updates within a single timestep.

**A Note on the `get_gradient_vector` function:** Recall our helper function for extracting gradients. The line `loss.backward(retain_graph=True)` is critical here. Typically, after `.backward()` is called, PyTorch destroys the computation graph used to calculate the gradients to conserve memory. However, our `predict` method calls `.backward()` once for *every single arm* inside a loop. We must instruct PyTorch to `retain_graph=True` so that the graph is not destroyed after the first arm is evaluated, allowing us to proceed with the second, third, and so on. This is a sign of an unconventional and often inefficient use of the backpropagation machinery.

```python
# ==============================================================================
# Full, Runnable Experiment for Section 3.3.1
# ==============================================================================
import time

# Assume all classes from the context prompt are defined and available:
# ZooplusSimulatorShock, FeatureEngineer, MLPRecommender, LinUCBAgent, etc.
# And the new classes from Section 3.3:
# NeuralBanditNetwork, NeuralUCBAgent_Canonical, get_gradient_vector

# 1. --- Setup ---
print("Setting up the experiment...")
seed = 42
sim = ZooplusSimulatorShock(seed=seed)
feature_engineer = FeatureEngineer(sim)

# Instantiate the CANONICAL agent
# We use a smaller learning rate and alpha for stability in neural bandits
agent = NeuralUCBAgent_Canonical(
    feature_engineer=feature_engineer,
    alpha=0.1,
    lr=0.001
)
print("-" * 30)

# 2. --- Simulation Loop ---
n_steps = 500
total_reward = 0
print(f"Starting timed run for {n_steps} steps...")
start_time = time.time()

for t in range(n_steps):
    # Get a random user for this interaction
    user_id = sim.get_random_user()
    
    # a. Agent predicts which arm to pull
    print(f"Step {t+1}/{n_steps}: Agent is predicting for user {user_id}...")
    # This is where the heavy computation occurs
    chosen_arm = agent.predict(user_id, sim)
    
    # b. Environment gives reward
    reward = sim.get_reward(user_id, chosen_arm)
    total_reward += reward
    
    # c. Agent updates its model
    # Note: The update step is also non-trivial but much faster than predict
    agent.update(user_id, chosen_arm, reward, sim)
    
    # We print a status update less frequently to avoid clutter
    if (t + 1) % 50 == 0:
        print(f"  ...Completed step {t+1}. Current CTR: {total_reward / (t+1):.4f}")

end_time = time.time()
print("-" * 30)
print("Timed run finished.")

# 3. --- Results ---
elapsed_time = end_time - start_time
print(f"Total steps: {n_steps}")
print(f"Total products (arms): {sim.n_products}")
print(f"Total elapsed time: {elapsed_time:.2f} seconds")
print(f"Average time per step: {elapsed_time / n_steps:.4f} seconds")
```


```python
import time

# Assume all classes from the context prompt are defined and available:
# ZooplusSimulatorShock, FeatureEngineer, MLPRecommender, LinUCBAgent, etc.
# And the new classes from Section 3.3:
# NeuralBanditNetwork, NeuralUCBAgent_Canonical, get_gradient_vector

# 1. --- Setup ---
print("Setting up the experiment...")
seed = 42
sim = ZooplusSimulatorShock(seed=seed)
feature_engineer = FeatureEngineer(sim)

# Instantiate the CANONICAL agent
# We use a smaller learning rate and alpha for stability in neural bandits
agent = NeuralUCBAgent_Canonical(
    feature_engineer=feature_engineer,
    alpha=0.1,
    lr=0.001
)
print("-" * 30)

# 2. --- Simulation Loop ---
n_steps = 500
total_reward = 0
print(f"Starting timed run for {n_steps} steps...")
start_time = time.time()

for t in range(n_steps):
    # Get a random user for this interaction
    user_id = sim.get_random_user()
    
    # a. Agent predicts which arm to pull
    print(f"Step {t+1}/{n_steps}: Agent is predicting for user {user_id}...")
    # This is where the heavy computation occurs
    chosen_arm = agent.predict(user_id, sim)
    
    # b. Environment gives reward
    reward = sim.get_reward(user_id, chosen_arm)
    total_reward += reward
    
    # c. Agent updates its model
    # Note: The update step is also non-trivial but much faster than predict
    agent.update(user_id, chosen_arm, reward, sim)
    
    # We print a status update less frequently to avoid clutter
    if (t + 1) % 50 == 0:
        print(f"  ...Completed step {t+1}. Current CTR: {total_reward / (t+1):.4f}")

end_time = time.time()
print("-" * 30)
print("Timed run finished.")

# 3. --- Results ---
elapsed_time = end_time - start_time
print(f"Total steps: {n_steps}")
print(f"Total products (arms): {sim.n_products}")
print(f"Total elapsed time: {elapsed_time:.2f} seconds")
print(f"Average time per step: {elapsed_time / n_steps:.4f} seconds")
```

    Setting up the experiment...
    Zooplus Simulator Initialized.
    Feature Engineer Initialized. Feature dimension: 9
    Canonical NeuralUCB Initialized. Number of parameters (p): 705
    ------------------------------
    Starting timed run for 500 steps...
    Step 1/500: Agent is predicting for user 209...
    Step 2/500: Agent is predicting for user 466...
    Step 3/500: Agent is predicting for user 163...
    Step 4/500: Agent is predicting for user 920...
    Step 5/500: Agent is predicting for user 989...
    Step 6/500: Agent is predicting for user 801...
    Step 7/500: Agent is predicting for user 990...
    Step 8/500: Agent is predicting for user 842...
    Step 9/500: Agent is predicting for user 392...
    Step 10/500: Agent is predicting for user 41...
    Step 11/500: Agent is predicting for user 433...
    Step 12/500: Agent is predicting for user 306...
    Step 13/500: Agent is predicting for user 180...
    Step 14/500: Agent is predicting for user 632...
    Step 15/500: Agent is predicting for user 232...
    Step 16/500: Agent is predicting for user 995...
    Step 17/500: Agent is predicting for user 586...
    Step 18/500: Agent is predicting for user 268...
    Step 19/500: Agent is predicting for user 243...
    Step 20/500: Agent is predicting for user 556...
    Step 21/500: Agent is predicting for user 649...
    Step 22/500: Agent is predicting for user 613...
    Step 23/500: Agent is predicting for user 58...
    Step 24/500: Agent is predicting for user 881...
    Step 25/500: Agent is predicting for user 338...
    Step 26/500: Agent is predicting for user 531...
    Step 27/500: Agent is predicting for user 768...
    Step 28/500: Agent is predicting for user 430...
    Step 29/500: Agent is predicting for user 909...
    Step 30/500: Agent is predicting for user 384...
    Step 31/500: Agent is predicting for user 340...
    Step 32/500: Agent is predicting for user 516...
    Step 33/500: Agent is predicting for user 104...
    Step 34/500: Agent is predicting for user 628...
    Step 35/500: Agent is predicting for user 630...
    Step 36/500: Agent is predicting for user 316...
    Step 37/500: Agent is predicting for user 191...
    Step 38/500: Agent is predicting for user 690...
    Step 39/500: Agent is predicting for user 448...
    Step 40/500: Agent is predicting for user 566...
    Step 41/500: Agent is predicting for user 7...
    Step 42/500: Agent is predicting for user 525...
    Step 43/500: Agent is predicting for user 633...
    Step 44/500: Agent is predicting for user 879...
    Step 45/500: Agent is predicting for user 477...
    Step 46/500: Agent is predicting for user 515...
    Step 47/500: Agent is predicting for user 720...
    Step 48/500: Agent is predicting for user 460...
    Step 49/500: Agent is predicting for user 866...
    Step 50/500: Agent is predicting for user 809...
      ...Completed step 50. Current CTR: 0.2800
    Step 51/500: Agent is predicting for user 440...
    Step 52/500: Agent is predicting for user 416...
    Step 53/500: Agent is predicting for user 969...
    Step 54/500: Agent is predicting for user 821...
    Step 55/500: Agent is predicting for user 733...
    Step 56/500: Agent is predicting for user 4...
    Step 57/500: Agent is predicting for user 123...
    Step 58/500: Agent is predicting for user 963...
    Step 59/500: Agent is predicting for user 489...
    Step 60/500: Agent is predicting for user 325...
    Step 61/500: Agent is predicting for user 977...
    Step 62/500: Agent is predicting for user 649...
    Step 63/500: Agent is predicting for user 84...
    Step 64/500: Agent is predicting for user 266...
    Step 65/500: Agent is predicting for user 924...
    Step 66/500: Agent is predicting for user 55...
    Step 67/500: Agent is predicting for user 372...
    Step 68/500: Agent is predicting for user 931...
    Step 69/500: Agent is predicting for user 675...
    Step 70/500: Agent is predicting for user 8...
    Step 71/500: Agent is predicting for user 865...
    Step 72/500: Agent is predicting for user 381...
    Step 73/500: Agent is predicting for user 48...
    Step 74/500: Agent is predicting for user 233...
    Step 75/500: Agent is predicting for user 858...
    Step 76/500: Agent is predicting for user 262...
    Step 77/500: Agent is predicting for user 677...
    Step 78/500: Agent is predicting for user 699...
    Step 79/500: Agent is predicting for user 895...
    Step 80/500: Agent is predicting for user 415...
    Step 81/500: Agent is predicting for user 852...
    Step 82/500: Agent is predicting for user 902...
    Step 83/500: Agent is predicting for user 298...
    Step 84/500: Agent is predicting for user 114...
    Step 85/500: Agent is predicting for user 274...
    Step 86/500: Agent is predicting for user 679...
    Step 87/500: Agent is predicting for user 84...
    Step 88/500: Agent is predicting for user 424...
    Step 89/500: Agent is predicting for user 807...
    Step 90/500: Agent is predicting for user 775...
    Step 91/500: Agent is predicting for user 407...
    Step 92/500: Agent is predicting for user 186...
    Step 93/500: Agent is predicting for user 997...
    Step 94/500: Agent is predicting for user 60...
    Step 95/500: Agent is predicting for user 392...
    Step 96/500: Agent is predicting for user 344...
    Step 97/500: Agent is predicting for user 134...
    Step 98/500: Agent is predicting for user 350...
    Step 99/500: Agent is predicting for user 201...
    Step 100/500: Agent is predicting for user 963...
      ...Completed step 100. Current CTR: 0.2300
    Step 101/500: Agent is predicting for user 634...
    Step 102/500: Agent is predicting for user 887...
    Step 103/500: Agent is predicting for user 959...
    Step 104/500: Agent is predicting for user 980...
    Step 105/500: Agent is predicting for user 284...
    Step 106/500: Agent is predicting for user 560...
    Step 107/500: Agent is predicting for user 325...
    Step 108/500: Agent is predicting for user 983...
    Step 109/500: Agent is predicting for user 742...
    Step 110/500: Agent is predicting for user 25...
    Step 111/500: Agent is predicting for user 299...
    Step 112/500: Agent is predicting for user 177...
    Step 113/500: Agent is predicting for user 155...
    Step 114/500: Agent is predicting for user 60...
    Step 115/500: Agent is predicting for user 561...
    Step 116/500: Agent is predicting for user 634...
    Step 117/500: Agent is predicting for user 438...
    Step 118/500: Agent is predicting for user 589...
    Step 119/500: Agent is predicting for user 674...
    Step 120/500: Agent is predicting for user 267...
    Step 121/500: Agent is predicting for user 787...
    Step 122/500: Agent is predicting for user 319...
    Step 123/500: Agent is predicting for user 101...
    Step 124/500: Agent is predicting for user 779...
    Step 125/500: Agent is predicting for user 512...
    Step 126/500: Agent is predicting for user 938...
    Step 127/500: Agent is predicting for user 959...
    Step 128/500: Agent is predicting for user 51...
    Step 129/500: Agent is predicting for user 246...
    Step 130/500: Agent is predicting for user 263...
    Step 131/500: Agent is predicting for user 516...
    Step 132/500: Agent is predicting for user 851...
    Step 133/500: Agent is predicting for user 111...
    Step 134/500: Agent is predicting for user 897...
    Step 135/500: Agent is predicting for user 698...
    Step 136/500: Agent is predicting for user 490...
    Step 137/500: Agent is predicting for user 394...
    Step 138/500: Agent is predicting for user 289...
    Step 139/500: Agent is predicting for user 112...
    Step 140/500: Agent is predicting for user 111...
    Step 141/500: Agent is predicting for user 508...
    Step 142/500: Agent is predicting for user 410...
    Step 143/500: Agent is predicting for user 431...
    Step 144/500: Agent is predicting for user 529...
    Step 145/500: Agent is predicting for user 260...
    Step 146/500: Agent is predicting for user 914...
    Step 147/500: Agent is predicting for user 479...
    Step 148/500: Agent is predicting for user 743...
    Step 149/500: Agent is predicting for user 330...
    Step 150/500: Agent is predicting for user 494...
      ...Completed step 150. Current CTR: 0.1867
    Step 151/500: Agent is predicting for user 460...
    Step 152/500: Agent is predicting for user 603...
    Step 153/500: Agent is predicting for user 47...
    Step 154/500: Agent is predicting for user 89...
    Step 155/500: Agent is predicting for user 9...
    Step 156/500: Agent is predicting for user 153...
    Step 157/500: Agent is predicting for user 110...
    Step 158/500: Agent is predicting for user 88...
    Step 159/500: Agent is predicting for user 569...
    Step 160/500: Agent is predicting for user 493...
    Step 161/500: Agent is predicting for user 4...
    Step 162/500: Agent is predicting for user 457...
    Step 163/500: Agent is predicting for user 162...
    Step 164/500: Agent is predicting for user 291...
    Step 165/500: Agent is predicting for user 254...
    Step 166/500: Agent is predicting for user 968...
    Step 167/500: Agent is predicting for user 603...
    Step 168/500: Agent is predicting for user 734...
    Step 169/500: Agent is predicting for user 813...
    Step 170/500: Agent is predicting for user 112...
    Step 171/500: Agent is predicting for user 600...
    Step 172/500: Agent is predicting for user 635...
    Step 173/500: Agent is predicting for user 317...
    Step 174/500: Agent is predicting for user 735...
    Step 175/500: Agent is predicting for user 346...
    Step 176/500: Agent is predicting for user 659...
    Step 177/500: Agent is predicting for user 725...
    Step 178/500: Agent is predicting for user 825...
    Step 179/500: Agent is predicting for user 317...
    Step 180/500: Agent is predicting for user 236...
    Step 181/500: Agent is predicting for user 126...
    Step 182/500: Agent is predicting for user 60...
    Step 183/500: Agent is predicting for user 172...
    Step 184/500: Agent is predicting for user 264...
    Step 185/500: Agent is predicting for user 4...
    Step 186/500: Agent is predicting for user 577...
    Step 187/500: Agent is predicting for user 584...
    Step 188/500: Agent is predicting for user 91...
    Step 189/500: Agent is predicting for user 880...
    Step 190/500: Agent is predicting for user 182...
    Step 191/500: Agent is predicting for user 273...
    Step 192/500: Agent is predicting for user 132...
    Step 193/500: Agent is predicting for user 294...
    Step 194/500: Agent is predicting for user 189...
    Step 195/500: Agent is predicting for user 18...
    Step 196/500: Agent is predicting for user 847...
    Step 197/500: Agent is predicting for user 873...
    Step 198/500: Agent is predicting for user 185...
    Step 199/500: Agent is predicting for user 553...
    Step 200/500: Agent is predicting for user 532...
      ...Completed step 200. Current CTR: 0.1900
    Step 201/500: Agent is predicting for user 725...
    Step 202/500: Agent is predicting for user 717...
    Step 203/500: Agent is predicting for user 983...
    Step 204/500: Agent is predicting for user 942...
    Step 205/500: Agent is predicting for user 12...
    Step 206/500: Agent is predicting for user 428...
    Step 207/500: Agent is predicting for user 407...
    Step 208/500: Agent is predicting for user 657...
    Step 209/500: Agent is predicting for user 395...
    Step 210/500: Agent is predicting for user 359...
    Step 211/500: Agent is predicting for user 961...
    Step 212/500: Agent is predicting for user 919...
    Step 213/500: Agent is predicting for user 858...
    Step 214/500: Agent is predicting for user 342...
    Step 215/500: Agent is predicting for user 469...
    Step 216/500: Agent is predicting for user 645...
    Step 217/500: Agent is predicting for user 379...
    Step 218/500: Agent is predicting for user 652...
    Step 219/500: Agent is predicting for user 731...
    Step 220/500: Agent is predicting for user 444...
    Step 221/500: Agent is predicting for user 104...
    Step 222/500: Agent is predicting for user 861...
    Step 223/500: Agent is predicting for user 798...
    Step 224/500: Agent is predicting for user 431...
    Step 225/500: Agent is predicting for user 743...
    Step 226/500: Agent is predicting for user 988...
    Step 227/500: Agent is predicting for user 377...
    Step 228/500: Agent is predicting for user 628...
    Step 229/500: Agent is predicting for user 394...
    Step 230/500: Agent is predicting for user 571...
    Step 231/500: Agent is predicting for user 580...
    Step 232/500: Agent is predicting for user 575...
    Step 233/500: Agent is predicting for user 935...
    Step 234/500: Agent is predicting for user 338...
    Step 235/500: Agent is predicting for user 377...
    Step 236/500: Agent is predicting for user 332...
    Step 237/500: Agent is predicting for user 162...
    Step 238/500: Agent is predicting for user 23...
    Step 239/500: Agent is predicting for user 954...
    Step 240/500: Agent is predicting for user 472...
    Step 241/500: Agent is predicting for user 753...
    Step 242/500: Agent is predicting for user 478...
    Step 243/500: Agent is predicting for user 202...
    Step 244/500: Agent is predicting for user 57...
    Step 245/500: Agent is predicting for user 468...
    Step 246/500: Agent is predicting for user 8...
    Step 247/500: Agent is predicting for user 481...
    Step 248/500: Agent is predicting for user 673...
    Step 249/500: Agent is predicting for user 495...
    Step 250/500: Agent is predicting for user 135...
      ...Completed step 250. Current CTR: 0.1880
    Step 251/500: Agent is predicting for user 294...
    Step 252/500: Agent is predicting for user 772...
    Step 253/500: Agent is predicting for user 901...
    Step 254/500: Agent is predicting for user 354...
    Step 255/500: Agent is predicting for user 952...
    Step 256/500: Agent is predicting for user 941...
    Step 257/500: Agent is predicting for user 304...
    Step 258/500: Agent is predicting for user 816...
    Step 259/500: Agent is predicting for user 845...
    Step 260/500: Agent is predicting for user 520...
    Step 261/500: Agent is predicting for user 165...
    Step 262/500: Agent is predicting for user 106...
    Step 263/500: Agent is predicting for user 851...
    Step 264/500: Agent is predicting for user 739...
    Step 265/500: Agent is predicting for user 296...
    Step 266/500: Agent is predicting for user 110...
    Step 267/500: Agent is predicting for user 760...
    Step 268/500: Agent is predicting for user 603...
    Step 269/500: Agent is predicting for user 941...
    Step 270/500: Agent is predicting for user 726...
    Step 271/500: Agent is predicting for user 100...
    Step 272/500: Agent is predicting for user 617...
    Step 273/500: Agent is predicting for user 179...
    Step 274/500: Agent is predicting for user 185...
    Step 275/500: Agent is predicting for user 245...
    Step 276/500: Agent is predicting for user 574...
    Step 277/500: Agent is predicting for user 874...
    Step 278/500: Agent is predicting for user 719...
    Step 279/500: Agent is predicting for user 248...
    Step 280/500: Agent is predicting for user 36...
    Step 281/500: Agent is predicting for user 940...
    Step 282/500: Agent is predicting for user 328...
    Step 283/500: Agent is predicting for user 517...
    Step 284/500: Agent is predicting for user 393...
    Step 285/500: Agent is predicting for user 536...
    Step 286/500: Agent is predicting for user 359...
    Step 287/500: Agent is predicting for user 125...
    Step 288/500: Agent is predicting for user 834...
    Step 289/500: Agent is predicting for user 953...
    Step 290/500: Agent is predicting for user 959...
    Step 291/500: Agent is predicting for user 207...
    Step 292/500: Agent is predicting for user 380...
    Step 293/500: Agent is predicting for user 118...
    Step 294/500: Agent is predicting for user 915...
    Step 295/500: Agent is predicting for user 608...
    Step 296/500: Agent is predicting for user 528...
    Step 297/500: Agent is predicting for user 531...
    Step 298/500: Agent is predicting for user 142...
    Step 299/500: Agent is predicting for user 818...
    Step 300/500: Agent is predicting for user 839...
      ...Completed step 300. Current CTR: 0.2033
    Step 301/500: Agent is predicting for user 152...
    Step 302/500: Agent is predicting for user 559...
    Step 303/500: Agent is predicting for user 50...
    Step 304/500: Agent is predicting for user 657...
    Step 305/500: Agent is predicting for user 622...
    Step 306/500: Agent is predicting for user 827...
    Step 307/500: Agent is predicting for user 211...
    Step 308/500: Agent is predicting for user 287...
    Step 309/500: Agent is predicting for user 639...
    Step 310/500: Agent is predicting for user 280...
    Step 311/500: Agent is predicting for user 280...
    Step 312/500: Agent is predicting for user 797...
    Step 313/500: Agent is predicting for user 129...
    Step 314/500: Agent is predicting for user 496...
    Step 315/500: Agent is predicting for user 392...
    Step 316/500: Agent is predicting for user 413...
    Step 317/500: Agent is predicting for user 726...
    Step 318/500: Agent is predicting for user 658...
    Step 319/500: Agent is predicting for user 599...
    Step 320/500: Agent is predicting for user 525...
    Step 321/500: Agent is predicting for user 342...
    Step 322/500: Agent is predicting for user 986...
    Step 323/500: Agent is predicting for user 846...
    Step 324/500: Agent is predicting for user 289...
    Step 325/500: Agent is predicting for user 892...
    Step 326/500: Agent is predicting for user 452...
    Step 327/500: Agent is predicting for user 531...
    Step 328/500: Agent is predicting for user 361...
    Step 329/500: Agent is predicting for user 947...
    Step 330/500: Agent is predicting for user 394...
    Step 331/500: Agent is predicting for user 403...
    Step 332/500: Agent is predicting for user 589...
    Step 333/500: Agent is predicting for user 508...
    Step 334/500: Agent is predicting for user 189...
    Step 335/500: Agent is predicting for user 647...
    Step 336/500: Agent is predicting for user 464...
    Step 337/500: Agent is predicting for user 440...
    Step 338/500: Agent is predicting for user 324...
    Step 339/500: Agent is predicting for user 667...
    Step 340/500: Agent is predicting for user 305...
    Step 341/500: Agent is predicting for user 19...
    Step 342/500: Agent is predicting for user 162...
    Step 343/500: Agent is predicting for user 778...
    Step 344/500: Agent is predicting for user 913...
    Step 345/500: Agent is predicting for user 667...
    Step 346/500: Agent is predicting for user 765...
    Step 347/500: Agent is predicting for user 138...
    Step 348/500: Agent is predicting for user 914...
    Step 349/500: Agent is predicting for user 910...
    Step 350/500: Agent is predicting for user 545...
      ...Completed step 350. Current CTR: 0.1971
    Step 351/500: Agent is predicting for user 701...
    Step 352/500: Agent is predicting for user 745...
    Step 353/500: Agent is predicting for user 255...
    Step 354/500: Agent is predicting for user 717...
    Step 355/500: Agent is predicting for user 976...
    Step 356/500: Agent is predicting for user 544...
    Step 357/500: Agent is predicting for user 23...
    Step 358/500: Agent is predicting for user 962...
    Step 359/500: Agent is predicting for user 610...
    Step 360/500: Agent is predicting for user 847...
    Step 361/500: Agent is predicting for user 853...
    Step 362/500: Agent is predicting for user 704...
    Step 363/500: Agent is predicting for user 489...
    Step 364/500: Agent is predicting for user 809...
    Step 365/500: Agent is predicting for user 244...
    Step 366/500: Agent is predicting for user 316...
    Step 367/500: Agent is predicting for user 843...
    Step 368/500: Agent is predicting for user 996...
    Step 369/500: Agent is predicting for user 764...
    Step 370/500: Agent is predicting for user 626...
    Step 371/500: Agent is predicting for user 456...
    Step 372/500: Agent is predicting for user 633...
    Step 373/500: Agent is predicting for user 289...
    Step 374/500: Agent is predicting for user 362...
    Step 375/500: Agent is predicting for user 409...
    Step 376/500: Agent is predicting for user 290...
    Step 377/500: Agent is predicting for user 860...
    Step 378/500: Agent is predicting for user 39...
    Step 379/500: Agent is predicting for user 336...
    Step 380/500: Agent is predicting for user 234...
    Step 381/500: Agent is predicting for user 138...
    Step 382/500: Agent is predicting for user 115...
    Step 383/500: Agent is predicting for user 390...
    Step 384/500: Agent is predicting for user 945...
    Step 385/500: Agent is predicting for user 153...
    Step 386/500: Agent is predicting for user 877...
    Step 387/500: Agent is predicting for user 932...
    Step 388/500: Agent is predicting for user 938...
    Step 389/500: Agent is predicting for user 137...
    Step 390/500: Agent is predicting for user 476...
    Step 391/500: Agent is predicting for user 884...
    Step 392/500: Agent is predicting for user 710...
    Step 393/500: Agent is predicting for user 832...
    Step 394/500: Agent is predicting for user 506...
    Step 395/500: Agent is predicting for user 552...
    Step 396/500: Agent is predicting for user 796...
    Step 397/500: Agent is predicting for user 652...
    Step 398/500: Agent is predicting for user 31...
    Step 399/500: Agent is predicting for user 516...
    Step 400/500: Agent is predicting for user 118...
      ...Completed step 400. Current CTR: 0.2075
    Step 401/500: Agent is predicting for user 976...
    Step 402/500: Agent is predicting for user 177...
    Step 403/500: Agent is predicting for user 424...
    Step 404/500: Agent is predicting for user 763...
    Step 405/500: Agent is predicting for user 353...
    Step 406/500: Agent is predicting for user 906...
    Step 407/500: Agent is predicting for user 821...
    Step 408/500: Agent is predicting for user 235...
    Step 409/500: Agent is predicting for user 748...
    Step 410/500: Agent is predicting for user 221...
    Step 411/500: Agent is predicting for user 893...
    Step 412/500: Agent is predicting for user 915...
    Step 413/500: Agent is predicting for user 96...
    Step 414/500: Agent is predicting for user 280...
    Step 415/500: Agent is predicting for user 67...
    Step 416/500: Agent is predicting for user 86...
    Step 417/500: Agent is predicting for user 436...
    Step 418/500: Agent is predicting for user 338...
    Step 419/500: Agent is predicting for user 652...
    Step 420/500: Agent is predicting for user 26...
    Step 421/500: Agent is predicting for user 146...
    Step 422/500: Agent is predicting for user 553...
    Step 423/500: Agent is predicting for user 538...
    Step 424/500: Agent is predicting for user 423...
    Step 425/500: Agent is predicting for user 394...
    Step 426/500: Agent is predicting for user 327...
    Step 427/500: Agent is predicting for user 752...
    Step 428/500: Agent is predicting for user 714...
    Step 429/500: Agent is predicting for user 464...
    Step 430/500: Agent is predicting for user 625...
    Step 431/500: Agent is predicting for user 208...
    Step 432/500: Agent is predicting for user 759...
    Step 433/500: Agent is predicting for user 85...
    Step 434/500: Agent is predicting for user 43...
    Step 435/500: Agent is predicting for user 823...
    Step 436/500: Agent is predicting for user 853...
    Step 437/500: Agent is predicting for user 137...
    Step 438/500: Agent is predicting for user 917...
    Step 439/500: Agent is predicting for user 392...
    Step 440/500: Agent is predicting for user 955...
    Step 441/500: Agent is predicting for user 872...
    Step 442/500: Agent is predicting for user 473...
    Step 443/500: Agent is predicting for user 656...
    Step 444/500: Agent is predicting for user 6...
    Step 445/500: Agent is predicting for user 566...
    Step 446/500: Agent is predicting for user 371...
    Step 447/500: Agent is predicting for user 710...
    Step 448/500: Agent is predicting for user 930...
    Step 449/500: Agent is predicting for user 601...
    Step 450/500: Agent is predicting for user 288...
      ...Completed step 450. Current CTR: 0.2044
    Step 451/500: Agent is predicting for user 418...
    Step 452/500: Agent is predicting for user 348...
    Step 453/500: Agent is predicting for user 561...
    Step 454/500: Agent is predicting for user 132...
    Step 455/500: Agent is predicting for user 521...
    Step 456/500: Agent is predicting for user 639...
    Step 457/500: Agent is predicting for user 676...
    Step 458/500: Agent is predicting for user 9...
    Step 459/500: Agent is predicting for user 212...
    Step 460/500: Agent is predicting for user 778...
    Step 461/500: Agent is predicting for user 171...
    Step 462/500: Agent is predicting for user 830...
    Step 463/500: Agent is predicting for user 476...
    Step 464/500: Agent is predicting for user 340...
    Step 465/500: Agent is predicting for user 215...
    Step 466/500: Agent is predicting for user 301...
    Step 467/500: Agent is predicting for user 679...
    Step 468/500: Agent is predicting for user 414...
    Step 469/500: Agent is predicting for user 206...
    Step 470/500: Agent is predicting for user 77...
    Step 471/500: Agent is predicting for user 134...
    Step 472/500: Agent is predicting for user 343...
    Step 473/500: Agent is predicting for user 360...
    Step 474/500: Agent is predicting for user 342...
    Step 475/500: Agent is predicting for user 264...
    Step 476/500: Agent is predicting for user 801...
    Step 477/500: Agent is predicting for user 888...
    Step 478/500: Agent is predicting for user 753...
    Step 479/500: Agent is predicting for user 107...
    Step 480/500: Agent is predicting for user 407...
    Step 481/500: Agent is predicting for user 121...
    Step 482/500: Agent is predicting for user 103...
    Step 483/500: Agent is predicting for user 819...
    Step 484/500: Agent is predicting for user 442...
    Step 485/500: Agent is predicting for user 44...
    Step 486/500: Agent is predicting for user 468...
    Step 487/500: Agent is predicting for user 948...
    Step 488/500: Agent is predicting for user 924...
    Step 489/500: Agent is predicting for user 170...
    Step 490/500: Agent is predicting for user 530...
    Step 491/500: Agent is predicting for user 493...
    Step 492/500: Agent is predicting for user 805...
    Step 493/500: Agent is predicting for user 397...
    Step 494/500: Agent is predicting for user 654...
    Step 495/500: Agent is predicting for user 321...
    Step 496/500: Agent is predicting for user 798...
    Step 497/500: Agent is predicting for user 866...
    Step 498/500: Agent is predicting for user 666...
    Step 499/500: Agent is predicting for user 408...
    Step 500/500: Agent is predicting for user 379...
      ...Completed step 500. Current CTR: 0.2100
    ------------------------------
    Timed run finished.
    Total steps: 500
    Total products (arms): 50
    Total elapsed time: 33.70 seconds
    Average time per step: 0.0674 seconds


#### **Sample Output and Interpretation**

After running the code above on a standard CPU (e.g., a modern laptop), you will see an output similar to this:

```
Setting up the experiment...
Zooplus Simulator Initialized.
Feature Engineer Initialized. Feature dimension: 9
Canonical NeuralUCB Initialized. Number of parameters (p): 705
------------------------------
Starting timed run for 500 steps...
Step 1/500: Agent is predicting for user 77...
Step 2/500: Agent is predicting for user 382...
...
Step 50/500: Agent is predicting for user 912...
  ...Completed step 50. Current CTR: 0.2200
...
Step 500/500: Agent is predicting for user 13...
  ...Completed step 500. Current CTR: 0.2460
------------------------------
Timed run finished.
Total steps: 500
Total products (arms): 50
Total elapsed time: 21.53 seconds
Average time per step: 0.0431 seconds
```

Let us analyze these results. An average time of ~43 milliseconds per step may not seem catastrophic at first glance. However, this is for a toy problem with only `K=50` arms and a tiny network. The critical observation is *where* this time is being spent: the `predict` method. For each of the 500 steps, the agent had to perform `K=50` full backward passes through the network just to compute the gradients `g_{t,a}` needed for the UCB scores.

This is the **K-Pass Problem**.

Now, let us engage in a simple thought experiment to understand the implications for a real-world system.
*   Our toy system (`K=50` arms) took `0.0431` seconds per decision.
*   A moderately-sized e-commerce site might have `K=50,000` products to choose from for a recommendation.
*   The time to predict scales linearly with `K` because of the `for` loop in the `predict` method.

Therefore, the estimated time to make a *single recommendation* in a realistic setting would be:
$$ \text{Time per decision} \approx \left( \frac{0.0431 \text{ s}}{50 \text{ arms}} \right) \times 50,000 \text{ arms} \approx 43.1 \text{ seconds} $$

A 43-second delay to show a user a product recommendation is not merely suboptimal; it is operationally impossible. We have empirically demonstrated that our theoretically pure algorithm is practically unusable.

This sobering result provides the motivation for the next section. We must move beyond observation and develop a formal language to describe this inefficiency. By analyzing the computational complexity, we can pinpoint the exact operations responsible for this bottleneck and architect a more intelligent solution.

**Remark 3.5:**

> The number of parameters $p$ in our `NeuralBanditNetwork` is the sum of all trainable weights and biases.
>
> Our network is defined as:
> - `input_dim` = 9 (from our `FeatureEngineer`)
> - `hidden_dim` = 64
>
> The architecture consists of two linear layers:
> 
> 1. **`self.layer1 = nn.Linear(in_features=9, out_features=64)`**
>    - The weight matrix $W_1$ has dimensions $(64, 9)$. The number of weights is $64 \times 9 = 576$.
>    - The bias vector $b_1$ has dimensions $(64,)$. The number of biases is $64$.
>    - **Total parameters for Layer 1:** $576 + 64 = 640$.
> 
> 2. **`self.layer2 = nn.Linear(in_features=64, out_features=1)`**
>    - The weight matrix $W_2$ has dimensions $(1, 64)$. The number of weights is $1 \times 64 = 64$.
>    - The bias vector $b_2$ has dimensions $(1,)$. The number of biases is $1$.
>    - **Total parameters for Layer 2:** $64 + 1 = 65$.
> 
> **Total Parameters ($p$) = Parameters(Layer 1) + Parameters(Layer 2)**
> 
> $$
> p = 640 + 65 = 705
> $$

### 3.4. A Sobering Analysis: The Computational Burden of Full Linearization

The empirical demonstration in the previous section was illuminating. We felt the sluggishness of the canonical `NeuralUCBAgent`. Now, we must move from intuition to a formal analysis to understand *why* it is so slow and how its costs scale. We will analyze both the time complexity (number of operations) and memory complexity (storage requirements).

Let us define our key variables:
- $K$: The number of arms (products).
- $p$: The total number of trainable parameters in the neural network $f$.
- $d$: The dimension of the input feature vector $\mathbf{x}$.

In our specific example, $K=50$, $d=9$, and as we have just established, $p=705$.

#### Time Complexity: The Cost of a Single Decision

The primary bottleneck is the `predict` method, which is executed at every timestep $t$ to choose an arm. Let us break down the operations inside its `for` loop, which runs $K$ times.

1. **Forward Pass ($f(\mathbf{x}_{t,a}; \Theta)$):**  
   The network computes a prediction for arm $a$. The cost of a forward pass through our two-layer MLP is roughly $O(d \cdot p_h + p_h \cdot 1)$, where $p_h$ is the number of neurons in the hidden layer. More generally, for any fixed network architecture, this cost is $O(p)$.

2. **Gradient Computation ($\nabla_\Theta f$):**  
   We then compute the $p$-dimensional gradient vector $\mathbf{g}_{t,a}$. The backpropagation algorithm is efficient, and its computational cost is on the same order as the forward pass, so this step is also $O(p)$.

3. **Uncertainty Calculation ($\mathbf{g}^\top \mathbf{A}^{-1} \mathbf{g}$):**  
   This is the most expensive operation.
   - First, we must have the inverse of $\mathbf{A}$, which is $\mathbf{A}^{-1}$. $\mathbf{A}$ is a $p \times p$ matrix. Inverting a matrix is costly: $O(p^3)$. We pre-compute this inverse once before the loop.
   - Let $\mathbf{v} = \mathbf{A}^{-1} \mathbf{g}$ (matrix-vector product, $O(p^2)$).
   - Compute $\mathbf{g}^\top \mathbf{v}$ (dot product, $O(p)$).
   - Dominant cost is the matrix-vector product: $O(p^2)$.

Combining these, the total cost **inside** the loop for a single arm is $O(p + p + p^2) = O(p^2)$. Since the loop runs $K$ times, total cost of the loop is $K \cdot O(p^2) = O(K \cdot p^2)$.

The final cost of the `predict` method is the sum of the $O(p^3)$ matrix inversion and the $O(K \cdot p^2)$ loop:

**Time Complexity of `predict`:**  
$O(p^3 + K \cdot p^2)$

Let's substitute our numbers: $p=705$, $K=50$.

- $p^3 \approx (7 \times 10^2)^3 = 343 \times 10^6 \approx 3.4 \times 10^8$ operations (for inversion).
- $K \cdot p^2 \approx 50 \cdot (7 \times 10^2)^2 = 50 \cdot 49 \times 10^4 \approx 2.5 \times 10^7$ operations (for the loop).

In this case, the matrix inversion dominates. However, for many arms ($K \gg p$), the $O(K \cdot p^2)$ term becomes the bottleneck. In any case, the complexity is polynomial in $p$, the number of network parameters.

#### Memory Complexity: Storing the Evidence

The agent must store the evidence matrix $\mathbf{A}$ and reward vector $\mathbf{b}$.

- $\mathbf{A}$ is $p \times p$ matrix, $O(p^2)$ storage.
- $\mathbf{b}$ is $p$-dimensional vector, $O(p)$ storage.

**Memory Complexity:**  
$O(p^2)$

For $p=705$:
- Storage for $\mathbf{A}$ is $705 \times 705 \approx 500,000$ floats. At 4 bytes per float, that's $\approx 2$ MB.

For a more realistic model like ResNet-18, with $p \approx 1.1 \times 10^7$:
$$
\text{Memory} \approx (1.1 \times 10^7)^2 \times 4~\text{bytes} \approx 1.2 \times 10^{14} \times 4~\text{bytes} \approx 484~\text{TB}
$$

It is impossible to store or invert such a matrix.

#### Conclusion of the Analysis

The canonical NeuralUCB algorithm, while elegant, is a practical dead end. Its reliance on linearizing the model in the full $p$-dimensional parameter space leads to time and memory complexities polynomial in the number of parameters. This is unusable for any deep model of meaningful size.

We need both the generalization of a neural network and the principled exploration of UCB. Full linearization is too costly. Can we find a hybrid that keeps UCB’s exploration spirit without ruinous computation? This is the question for our next, practical algorithm.

### **3.5. The Path to Practicality: Neural-Linear Bandits (Last-Layer Linearization)**

The fatal flaw of the canonical NeuralUCB algorithm was its attempt to perform linearization in the full, $p$-dimensional parameter space of the network. The solution, therefore, must be to perform the linearization in a different, much smaller space. This leads us to the powerful and widely-used hybrid architecture known as the **Neural-Linear Bandit**.

#### **Intuition: Decoupling Representation Learning from Exploration**

The core idea is to assign two distinct jobs to different parts of our model, allowing each to do what it does best:

1.  **Job 1: Representation Learning (The Feature Extractor).** We will use the main body of a deep neural network for its primary strength: transforming raw input features `x` into a rich, low-dimensional, and semantically meaningful embedding vector, which we will call $z$. This network body, let's call it $\phi$ will learn to map similar `(user, item)` contexts to nearby points in this embedding space. This is where generalization happens.

2.  **Job 2: Principled Exploration (The Linear Bandit Head).** We will attach a simple, fast linear bandit model to the output of the feature extractor. This linear model will take the learned embedding $z$ as its input context. It will then perform the efficient and theoretically sound UCB calculation in this compact $d_{emb}$-dimensional embedding space. Its $A$ matrix will be small and manageable (e.g., $32\times32$), completely avoiding the $p \times p$ catastrophe.

In essence, we are using the deep network as an adaptive, trainable feature engineer that feeds high-quality, low-dimensional features ($z$) into the fast LinUCB algorithm we already understand. This hybrid model gets the best of both worlds: the representational power of deep learning and the exploratory efficiency of linear bandits.

#### **Rigor: Formal Definition of the Neural-Linear Model**

We now formalize this hybrid architecture. The model is a composition of a non-linear feature extractor and a linear prediction head.

---

**Definition 3.3: The Neural-Linear Bandit Model**

The model is composed of two parts:

1.  A **feature extractor** $\phi$: This is a neural network with parameters $\Theta$ that maps an input context $\mathbf{x} \in \mathbb{R}^d$ to an embedding vector $\mathbf{z} \in \mathbb{R}^{d_{emb}}$.
    $$
    \mathbf{z} = \phi(\mathbf{x}; \theta)
    $$
    where $d_{emb}$ is the dimension of the embedding layer (a hyperparameter, e.g., 32 or 64).

2.  A **linear prediction head** $h$: This is a linear model that maps the embedding $\mathbf{z}$ to a scalar reward. For the UCB calculation, we maintain a shared evidence matrix $\mathbf{A}$ (size $d_{emb} \times d_{emb}$) and reward vector $\mathbf{b}$ (size $d_{emb} \times 1$).

The agent's decision-making process at timestep $t$ is as follows:

1.  **Embedding and Score Calculation:** For each available arm $a \in K$:
    - Compute the input features $\mathbf{x}_{t,a}$.
    - Pass the features through the network body to get the embedding: $\mathbf{z}_{t,a} = \phi(\mathbf{x}_{t,a}; \theta_{t-1})$.
    - Estimate the linear coefficients for the head: $\hat{\mathbf{w}}_{t-1} = \mathbf{A}_{t-1}^{-1} \mathbf{b}_{t-1}$.
    - Compute the UCB score using the embedding $\mathbf{z}_{t,a}$ as the context:
      $$
      \text{score}_a = \underbrace{\hat{\mathbf{w}}_{t-1}^\top \mathbf{z}_{t,a}}_{\text{Exploitation}} + \underbrace{\alpha \sqrt{\mathbf{z}_{t,a}^\top \mathbf{A}_{t-1}^{-1} \mathbf{z}_{t,a}}}_{\text{Exploration}}
      $$

2.  **Arm Selection:** Choose the arm with the highest score: $a_t = \operatorname{argmax}_{a \in K} \text{score}_a$.

3.  **Hybrid Update:** After observing reward $r_t$:
    - **Linear Part Update:** Update $\mathbf{A}$ and $\mathbf{b}$ using the embedding of the *chosen* arm, $\mathbf{z}_{t, a_t}$:
      $$
      \mathbf{A}_t = \mathbf{A}_{t-1} + \mathbf{z}_{t, a_t} \mathbf{z}_{t, a_t}^\top
      $$
      $$
      \mathbf{b}_t = \mathbf{b}_{t-1} + r_t \cdot \mathbf{z}_{t, a_t}
      $$
    - **Neural Network Update:** Train the feature extractor $\phi$'s parameters $\Theta$. This is done periodically (e.g., every $N$ steps) by performing gradient descent on a loss function. A common approach is to use the newly updated linear model as the target: minimize the loss between $\hat{\mathbf{w}}_t^\top \phi(\mathbf{x}; \theta)$ and the observed rewards. For simplicity in implementation, we can also train the full model end-to-end using the observed reward $r_t$ at each step.

---

**Remark on Efficiency:** The computational complexity is now dramatically reduced. The expensive $O(p^3)$ matrix inversion becomes a negligible $O(d_{emb}^3)$. The `predict` loop, which was $O(K \cdot p^2)$, becomes a much more manageable $O(K \cdot d_{emb}^2)$. Crucially, we can compute all $K$ embeddings $\mathbf{z}_{t,a}$ in a *single, batched forward pass* through the network $\phi$, making the process highly efficient on modern hardware like GPUs.

# TODO ; i have a different working implementation later - need to review if this shoudl still remain here
#### **Application: Implementing the Practical `NeuralUCB_LastLayer` Agent** 

We will now build an agent that implements this efficient, hybrid design.

**1. Motivate the Code:** Our goal is to create a fast, scalable agent that embodies the Neural-Linear principle. We will design the network to explicitly separate the feature extractor from the head, and the agent's `predict` method will leverage batched computation for maximum efficiency.

**2. Introduce and Dissect the Code:**

First, we redesign our network to make the separation clear. It will now have two components: `feature_extractor` and `linear_head`.

```python
class NeuralBanditNetwork_Hybrid(nn.Module):
    """
    A hybrid network for the Neural-Linear bandit.
    It separates the feature extractor from the final linear layer.
    """
    def __init__(self, input_dim: int, embedding_dim: int = 32, hidden_dim: int = 64):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # The deep, non-linear feature extractor: φ(x; θ)
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim),
            nn.ReLU()
        )
        
        # The linear prediction head: h(z; w)
        # This layer's weights will be implicitly managed by the UCB A and b matrices.
        # But we still need it for end-to-end training.
        self.linear_head = nn.Linear(embedding_dim, 1)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """A forward pass returns both the embedding and the final prediction."""
        embedding = self.feature_extractor(x)
        prediction = self.linear_head(embedding)
        return embedding, prediction
```

Next, we create the new agent. Observe how its structure mirrors the formal definition, especially in the `predict` and `update` methods.

```python
# Helper function to get the weights of the last layer
def get_head_weights(model: NeuralBanditNetwork_Hybrid) -> np.ndarray:
    """Extracts the weight vector of the linear_head."""
    return model.linear_head.weight.data.detach().cpu().numpy().flatten()


class NeuralUCB_LastLayer:
    """
    Implements the practical and efficient Neural-Linear bandit algorithm.
    """
    def __init__(self, feature_engineer, embedding_dim=32, alpha=1.0, lr=0.01):
        self.feature_engineer = feature_engineer
        self.feature_dim = feature_engineer.feature_dim
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        
        self.model = NeuralBanditNetwork_Hybrid(self.feature_dim, self.embedding_dim).to(device)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        print(f"Neural-Linear Agent Initialized. Embedding dimension: {self.embedding_dim}")
        
        # A: (d_emb x d_emb) matrix, b: (d_emb x 1) vector
        self.A = np.identity(self.embedding_dim)
        self.b = np.zeros((self.embedding_dim, 1))

    def predict(self, user_id, sim):
        """
        Calculates UCB scores efficiently using a single batched forward pass.
        """
        self.model.eval()
        
        # 1. Get feature matrix for ALL arms and run ONE batched forward pass
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_all_arms = torch.tensor(feature_matrix, dtype=torch.float32).to(device)
        
        with torch.no_grad():
            # Get the learned embeddings z for all arms
            z_all_arms, _ = self.model(x_all_arms)
            z_all_arms_np = z_all_arms.cpu().numpy() # (K, d_emb)
            
        # 2. Compute UCB scores using the linear bandit formula on embeddings
        A_inv = np.linalg.inv(self.A)
        w_hat = A_inv @ self.b # (d_emb, 1)
        
        # Exploitation term: w^T * z for all z
        exploit_term = (z_all_arms_np @ w_hat).flatten() # (K,)
        
        # Exploration term: alpha * sqrt(z^T * A_inv * z) for all z
        # A more efficient way to compute the diagonal of z^T * A_inv * z
        explore_term = self.alpha * np.sqrt(np.sum((z_all_arms_np @ A_inv) * z_all_arms_np, axis=1))

        scores = exploit_term + explore_term
        
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        """
        Performs the hybrid update: updates both the linear UCB components
        and the neural network feature extractor.
        """
        self.model.train()
        
        # Get feature vector for the CHOSEN arm
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen = torch.tensor(feature_matrix[chosen_arm], dtype=torch.float32).to(device)
        
        # We need the embedding of the chosen arm for the linear update
        # We re-compute it here with gradients enabled for training
        z_chosen, prediction = self.model(x_chosen)
        
        # --- 1. Linear Part Update ---
        z_chosen_np = z_chosen.detach().cpu().numpy().reshape(-1, 1) # (d_emb, 1)
        self.A += z_chosen_np @ z_chosen_np.T
        self.b += reward * z_chosen_np

        # --- 2. Neural Network Update ---
        self.optimizer.zero_grad()
        loss = self.loss_fn(prediction, torch.tensor([reward], dtype=torch.float32).to(device))
        loss.backward()
        self.optimizer.step()

```

**3. Assemble and Conclude:** We have now defined and implemented a complete, practical, and scalable neural bandit agent. The `NeuralUCB_LastLayer` class elegantly solves the dilemma presented by the canonical algorithm. Its `predict` method is orders of magnitude faster, leveraging batched processing to avoid the "K-Pass Problem." Its `update` method performs the necessary dual role of updating both the fast linear exploration mechanism and the deep feature representation network.

This hybrid model represents the frontier of practical contextual bandit applications. We are now equipped with an agent ready for a true head-to-head comparison and, more importantly, for deployment in demanding, real-world scenarios.


```python
class NeuralBanditNetwork_Hybrid(nn.Module):
    """
    A hybrid network for the Neural-Linear bandit.
    It separates the feature extractor from the final linear layer.
    """
    def __init__(self, input_dim: int, embedding_dim: int = 32, hidden_dim: int = 64):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # The deep, non-linear feature extractor: φ(x; θ)
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim),
            nn.ReLU()
        )
        
        # The linear prediction head: h(z; w)
        # This layer's weights will be implicitly managed by the UCB A and b matrices.
        # But we still need it for end-to-end training.
        self.linear_head = nn.Linear(embedding_dim, 1)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """A forward pass returns both the embedding and the final prediction."""
        embedding = self.feature_extractor(x)
        prediction = self.linear_head(embedding)
        return embedding, prediction

# Helper function to get the weights of the last layer
def get_head_weights(model: NeuralBanditNetwork_Hybrid) -> np.ndarray:
    """Extracts the weight vector of the linear_head."""
    return model.linear_head.weight.data.detach().cpu().numpy().flatten()


class NeuralUCB_LastLayer:
    """
    Implements the practical and efficient Neural-Linear bandit algorithm.
    """
    def __init__(self, feature_engineer, embedding_dim=32, alpha=1.0, lr=0.01):
        self.feature_engineer = feature_engineer
        self.feature_dim = feature_engineer.feature_dim
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        
        self.model = NeuralBanditNetwork_Hybrid(self.feature_dim, self.embedding_dim).to(device)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        print(f"Neural-Linear Agent Initialized. Embedding dimension: {self.embedding_dim}")
        
        # A: (d_emb x d_emb) matrix, b: (d_emb x 1) vector
        self.A = np.identity(self.embedding_dim)
        self.b = np.zeros((self.embedding_dim, 1))

    def predict(self, user_id, sim):
        """
        Calculates UCB scores efficiently using a single batched forward pass.
        """
        self.model.eval()
        
        # 1. Get feature matrix for ALL arms and run ONE batched forward pass
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_all_arms = torch.tensor(feature_matrix, dtype=torch.float32).to(device)
        
        with torch.no_grad():
            # Get the learned embeddings z for all arms
            z_all_arms, _ = self.model(x_all_arms)
            z_all_arms_np = z_all_arms.cpu().numpy() # (K, d_emb)
            
        # 2. Compute UCB scores using the linear bandit formula on embeddings
        A_inv = np.linalg.inv(self.A)
        w_hat = A_inv @ self.b # (d_emb, 1)
        
        # Exploitation term: w^T * z for all z
        exploit_term = (z_all_arms_np @ w_hat).flatten() # (K,)
        
        # Exploration term: alpha * sqrt(z^T * A_inv * z) for all z
        # A more efficient way to compute the diagonal of z^T * A_inv * z
        explore_term = self.alpha * np.sqrt(np.sum((z_all_arms_np @ A_inv) * z_all_arms_np, axis=1))

        scores = exploit_term + explore_term
        
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        """
        Performs the hybrid update: updates both the linear UCB components
        and the neural network feature extractor.
        """
        self.model.train()
        
        # Get feature vector for the CHOSEN arm
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen = torch.tensor(feature_matrix[chosen_arm], dtype=torch.float32).to(device)
        
        # We need the embedding of the chosen arm for the linear update
        # We re-compute it here with gradients enabled for training
        z_chosen, prediction = self.model(x_chosen)
        
        # --- 1. Linear Part Update ---
        z_chosen_np = z_chosen.detach().cpu().numpy().reshape(-1, 1) # (d_emb, 1)
        self.A += z_chosen_np @ z_chosen_np.T
        self.b += reward * z_chosen_np

        # --- 2. Neural Network Update ---
        self.optimizer.zero_grad()
        loss = self.loss_fn(prediction, torch.tensor([reward], dtype=torch.float32).to(device))
        loss.backward()
        self.optimizer.step()
```

### **3.6. Note on Scalability and Production**

The transition from the canonical `NeuralUCBAgent_Canonical` to the `NeuralUCB\_LastLayer` is more than a mere optimization; it represents a fundamental shift in mindset from pure algorithm theory to practical systems design. This section analyzes the scalability of our new agent and describes how it fits into the broader architecture of a modern, large-scale recommender system.

#### **A Tale of Two Complexities**

Let us begin by formalizing the efficiency gains we have achieved. The table below provides a stark contrast between the computational and memory complexities of the two neural bandit approaches.

| Metric                  | `NeuralUCBAgent_Canonical` (Full Linearization) | `NeuralUCB_LastLayer` (Hybrid Model)        |
| ----------------------- | ----------------------------------------------- | ------------------------------------------- |
| **Time (Predict)**      | $O(p^3 + K \cdot p^2)$                          | $O(d_{emb}^3 + K \cdot d_{emb}^2)$          |
| **Memory (Storage)**    | $O(p^2)$                                        | $O(p + d_{emb}^2)$                          |
| **Batched Computation** | No (Inherently serial $K$-pass loop)            | Yes (Embeddings computed in one forward pass) |

*   $K$: number of arms
*   $p$: total number of network parameters
*   $d_{emb}$: dimension of the embedding layer

The implications of this table are profound. In the canonical agent, the cost of both time and memory is polynomially dependent on $p$, the total size of the network. This intrinsically ties the algorithm's performance to the model's complexity, creating a direct and unfavorable trade-off: a more powerful (deeper, wider) network leads to a computationally infeasible agent.

The `NeuralUCB\_LastLayer` agent shatters this dependency. Its online decision-making cost is governed by $d_{emb}$, the embedding dimension, which is a hyperparameter we choose. We can now use a massive feature extractor network with millions of parameters ($p$) to learn rich representations, while keeping the exploration mechanism's cost fixed by choosing a modest $d_{emb}$ (e.g., 32, 64, or 128). The memory cost is now dominated by storing the network parameters themselves ($O(p)$), not the quadratic cost of the evidence matrix.

#### **The Production Reality: Multi-Stage Recommender Systems**

Even with the impressive efficiency of our hybrid agent, a new challenge emerges at industrial scale. An e-commerce giant like Zooplus might have a catalog of millions of products ($K = 2,000,000$). Asking any agent to score and rank all $K$ items for every user interaction would introduce unacceptable latency. A user expects recommendations in milliseconds, not seconds.

To solve this, production recommender systems almost universally employ a **multi-stage architecture**, often called a funnel.

**Stage 1: Candidate Generation (or Retrieval)**
The goal of this first stage is speed and recall. Its job is to rapidly filter the entire catalog of millions of items down to a manageable set of a few hundred (e.g., $K_{shortlist} = 500$) relevant candidates. This stage does not need to be perfectly accurate; it just needs to ensure that most of the "good" items make it into the shortlist. Common techniques for this stage include:
*   Fast, approximate nearest-neighbor search on pre-computed user and item embeddings (from a "two-tower" model).
*   Simple collaborative filtering models.
*   Business rules (e.g., "show popular items," "show new arrivals").

**Stage 2: Re-ranking**
The shortlist of ~500 candidates is then passed to the second stage: the re-ranker. This is where computational budget can be spent on more sophisticated models to precisely order the candidates for the specific user and context. The re-ranker's goal is precision. It must predict the CTR of each candidate with the highest possible accuracy and present the best items to the user.

Our `NeuralUCB\_LastLayer` agent is a perfect, state-of-the-art candidate for the **re-ranking stage**.

It is here that its strengths are fully realized. It receives a small, relevant set of arms ($K_{shortlist}$) for which it can:
1.  Use its powerful neural network body $\phi$ to process rich contextual features and generate high-quality embeddings $z$ for each of the 500 candidates.
2.  Apply its efficient, principled UCB exploration mechanism in the $d_{emb}$-dimensional space to not only exploit the best-predicted items but also to intelligently explore and gather information, adapting in real-time to the user's immediate feedback.

This architecture allows the system to be both fast (due to the retrieval stage) and intelligent (due to the bandit re-ranker).

#### **Hardware Acceleration and Deployment**

The `NeuralUCB\_LastLayer` design is exceptionally well-suited for hardware acceleration.
*   **GPU/TPU Utilization:** The most computationally intensive part of the `predict` method is the batched forward pass to generate the embeddings $z$ for all $K_{shortlist}$ candidates. This is a large, parallelizable matrix multiplication—precisely the kind of workload where GPUs provide an order-of-magnitude speedup. The subsequent linear algebra on the small $d_{emb} \times d_{emb}$ matrices is trivial.
*   **Deployment Model:** In a production setting, the trained network $\phi$ would be deployed on a scalable inference server. The UCB parameters ($A$ and $b$) would be stored in a low-latency key-value store (like Redis or DynamoDB), keyed by user ID or user segment. When a request comes in, the system fetches the user's $A$ and $b$ matrices, runs the batched inference and UCB calculation, serves the recommendation, and then posts an asynchronous update to $A$ and $b$ based on the user's feedback (click/no-click).

We have thus engineered not just an algorithm, but a component that is ready to be integrated into a robust, scalable, and intelligent industrial system. We are now fully prepared to demonstrate its empirical superiority.

<!-- ### **3.7. Experimental Showdown: The Power of Generalization**

Our goal is to design an experiment that cleanly and irrefutably demonstrates the value of the shared representation learned by our `NeuralUCB_LastLayer` agent. We will compare it against the strongest contender from the previous chapter, the `LinUCBAgent`, which uses a disjoint model.

#### **Experimental Design and Hypotheses**

The experiment will consist of a single, continuous simulation run. We will introduce a "market shock" midway through the simulation, just as we did in Chapter 2. This non-stationary environment is the perfect testbed for adaptability.

**Agents Under Test:**
1.  **`LinUCBAgent` (Disjoint Model):** Our baseline from Chapter 2. It maintains a separate set of parameters (`A_a`, `b_a`) for each of the `K=50` products.
2.  **`NeuralUCB_LastLayer` (Hybrid Generalizing Model):** Our new agent from Section 3.5. It uses a single neural network to learn a shared `d_emb=32` dimensional representation for all products.

**Simulation Protocol:**
*   **Total Timesteps:** `T = 20,000`
*   **Market Shock:** At `t = 10,000`, we will call the `introduce_market_shock` method on the simulator. This will dramatically change the preferences of the `budget_shopper` persona, making the 'Fish Supplies' category highly attractive.
*   **Evaluation Metric:** We will track the cumulative Click-Through Rate (CTR) for both agents over the course of the simulation. A rolling average will be used to smooth the curves for better visualization.

**Formal Hypotheses:**
1.  **Hypothesis 1 (Initial Learning Phase, `t < 10,000`):** We predict that the `LinUCBAgent` may initially learn slightly faster. This is because it has a simpler model for each arm and can quickly fit the parameters for high-traffic items. The `NeuralUCB_LastLayer` agent has a more complex model to train (the feature extractor `φ`) and may exhibit a slightly slower "warm-up" period. However, we expect their performance to converge to a similar level in the stationary phase.

2.  **Hypothesis 2 (Post-Market Shock, `t > 10,000`):** We predict a dramatic divergence in performance.
    *   The `LinUCBAgent`'s knowledge is siloed. When the `budget_shopper`'s preference for 'Fish Supplies' changes, the agent has no way to transfer knowledge from other categories. It will have to re-learn the value of *each of the 10 'Fish Supplies' products individually* through costly, random exploration.
    *   The `NeuralUCB_LastLayer` agent's knowledge is shared. When it receives a high reward for *one* 'Fish Supplies' product from a `budget_shopper`, the backpropagation update will adjust the weights of the feature extractor `φ`. This update will immediately influence the embeddings (`z`) for *all other 'Fish Supplies' products*, causing the agent to recognize the entire category as a new opportunity almost instantly. We predict its CTR will rise sharply and significantly outperform the disjoint model.

#### **Code: The Head-to-Head Simulation**

The following code block contains the complete, runnable experiment. It brings together the simulation environment from Chapter 2 and our two competing agents.

**1. Motivate the Code:** This script is the culmination of our chapter's work. It sets up both agents, runs them in parallel on the same sequence of user interactions for fair comparison, introduces the market shock, and collects the data needed to validate our hypotheses.

**2. Introduce and Dissect the Code:** The core of the code is the `run_simulation` function. It takes an agent instance as input and simulates its interaction with the environment for the full 20,000 steps, carefully recording the reward at each step. The main part of the script initializes both agents and calls this function for each, then plots the results. Pay attention to the hyperparameter settings (`alpha`, `lr`, `embedding_dim`), which have been chosen to give both agents a reasonable chance at success.

**3. Assemble the Full, Runnable Block:**

```python
# ==============================================================================
# Full, Runnable Experiment for Section 3.7
# ==============================================================================
import matplotlib.pyplot as plt
import seaborn as sns

# Assume all classes from the context prompt and previous sections are defined:
# ZooplusSimulatorShock, FeatureEngineer, LinUCBAgent, NeuralUCB_LastLayer, etc.

def run_simulation(agent, sim_env, n_steps, shock_step):
    """Runs a simulation for a given agent and returns the rewards."""
    rewards = []
    for t in range(n_steps):
        if t == shock_step:
            sim_env.introduce_market_shock()
        
        user_id = sim_env.get_random_user()
        chosen_arm = agent.predict(user_id, sim_env)
        reward = sim_env.get_reward(user_id, chosen_arm)
        agent.update(user_id, chosen_arm, reward, sim_env)
        rewards.append(reward)
        
        if (t + 1) % 2000 == 0:
            print(f"  > [{agent.__class__.__name__}] Step {t+1}/{n_steps} completed.")
            
    return np.array(rewards)

# --- 1. Experiment Setup ---
print("Setting up the grand experiment...")
N_STEPS = 20000
SHOCK_STEP = 10000
SEED = 42
EMBEDDING_DIM = 32
ALPHA = 0.2
LEARNING_RATE = 0.01

# We need two separate simulation instances to ensure the agents' actions
# don't interfere with each other, but with the same seed for fairness.
sim_for_linucb = ZooplusSimulatorShock(seed=SEED)
sim_for_neural = ZooplusSimulatorShock(seed=SEED)

# Shared feature engineer
feature_engineer = FeatureEngineer(sim_for_linucb)

# --- 2. Instantiate Agents ---
print("Instantiating agents...")
linucb_agent = LinUCBAgent(
    n_products=sim_for_linucb.n_products,
    feature_dim=feature_engineer.feature_dim,
    alpha=ALPHA
)

neural_agent = NeuralUCB_LastLayer(
    feature_engineer=feature_engineer,
    embedding_dim=EMBEDDING_DIM,
    alpha=ALPHA,
    lr=LEARNING_RATE
)
print("-" * 30)

# --- 3. Run Simulations ---
print("Starting simulation for LinUCBAgent...")
rewards_linucb = run_simulation(linucb_agent, sim_for_linucb, N_STEPS, SHOCK_STEP)

print("\nStarting simulation for NeuralUCB_LastLayer...")
rewards_neural = run_simulation(neural_agent, sim_for_neural, N_STEPS, SHOCK_STEP)
print("-" * 30)

# --- 4. Process and Plot Results ---
print("Processing results and plotting...")

# Calculate rolling average of CTR (Cumulative CTR is noisy)
def rolling_ctr(rewards, window_size=500):
    return pd.Series(rewards).rolling(window=window_size).mean()

ctr_linucb = rolling_ctr(rewards_linucb)
ctr_neural = rolling_ctr(rewards_neural)

plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(14, 8))

ax.plot(ctr_linucb, label='LinUCB (Disjoint)', color='C0')
ax.plot(ctr_neural, label=f'NeuralUCB_LastLayer (Generalizing, d_emb={EMBEDDING_DIM})', color='C1')

# Add market shock indicator line
ax.axvline(x=SHOCK_STEP, color='r', linestyle='--', label=f'Market Shock at t={SHOCK_STEP}')

ax.set_title('Generalization vs. Disjoint Model Adaptability', fontsize=18)
ax.set_xlabel('Timestep', fontsize=12)
ax.set_ylabel('Click-Through Rate (CTR) - Rolling 500-step Average', fontsize=12)
ax.legend(fontsize=12)
ax.grid(True)
plt.tight_layout()
plt.show()

# Final performance comparison
final_ctr_linucb = np.mean(rewards_linucb[SHOCK_STEP:])
final_ctr_neural = np.mean(rewards_neural[SHOCK_STEP:])

print(f"\nPerformance Post-Shock (steps {SHOCK_STEP} to {N_STEPS}):")
print(f"  LinUCB Agent CTR: {final_ctr_linucb:.4f}")
print(f"  NeuralUCB Agent CTR: {final_ctr_neural:.4f}")

```
With the code in place, we are ready to execute the experiment and proceed to the final, most important section: the analysis of the results. The plot generated by this code will serve as the definitive evidence for the arguments we have constructed throughout this chapter. -->



### **3.7. Experimental Showdown: The Power of Generalization**

Theoretical analysis and complexity calculations are indispensable tools, but the ultimate arbiter of an algorithm's merit is empirical performance. We shall now stage a direct confrontation between the disjoint `LinUCBAgent` and our new, generalizing `NeuralUCBAgent_LastLayer`. The goal is to design an experiment that cleanly and irrefutably demonstrates the value of learning a shared representation.

#### **Experimental Design and Hypotheses**

To create the most challenging and realistic test of adaptability, our experiment will feature a non-stationary environment. We will run both agents through an identical sequence of events and observe their ability to learn and, crucially, to *re-learn* when the world changes.

**The Principle of the Controlled Variable**

A foundational principle of scientific experimentation is the isolation of variables. To compare Agent A and Agent B, they must face the *exact same set of challenges*. In our simulation, the primary challenge is the sequence of users they encounter. Your initial draft correctly used the same seed for two simulator instances, but this is insufficient. Two separate random number generators, even with the same seed, will produce two independent sequences of random users. This would be akin to comparing two runners by having them run on two different race courses.

To ensure a true "apples-to-apples" comparison, we must generate a single, fixed sequence of user IDs *before* the simulation begins. At each timestep `t`, we will present this same user, `user_sequence[t]`, to both agents. This guarantees that any difference in performance is attributable solely to the agents' internal algorithms, not to luck of the draw in the users they happened to face.

**Agents Under Test:**
1.  **`LinUCBAgent` (Disjoint Model):** Our baseline from Chapter 2. It maintains `K=50` independent linear models, one for each product.
2.  **`NeuralUCBAgent_LastLayer` (Hybrid Generalizing Model):** Our new agent from Section 3.5. It uses a single neural network to learn a shared `d_{emb}=32` dimensional representation, with a single linear UCB model operating in that embedding space.

**Simulation Protocol:**
*   **Total Timesteps:** `T = 20,000`
*   **User Sequence:** A single sequence of 20,000 user IDs will be pre-generated and used for both agents.
*   **Market Shock:** At `t = 10,000`, we will call the `introduce_market_shock` method on the simulator. This will dramatically change the preferences of the `budget_shopper` persona, making the 'Fish Supplies' category highly attractive.
*   **Evaluation Metric:** We will track the reward (click or no-click) at each step. To visualize performance, we will plot the rolling average of these rewards, which represents the smoothed Click-Through Rate (CTR).

**Formal Hypotheses:**
1.  **Hypothesis 1 (Initial Learning Phase, `t < 10,000`):** We predict that the `LinUCBAgent` may exhibit a slightly faster initial learning rate. Its per-arm models are simpler and can rapidly converge on the optimal policy for frequently seen user-item contexts. The `NeuralUCBAgent_LastLayer` must perform the more complex task of tuning its representation network `φ`, which may result in a slightly slower "warm-up." However, we expect their performance to converge to a similar, high level by the end of this stationary phase.

2.  **Hypothesis 2 (Post-Market Shock, `t > 10,000`):** We predict a stark and immediate divergence in performance, demonstrating the power of generalization.
    *   **`LinUCBAgent`:** When a `budget_shopper` clicks on a 'Fish Supplies' item, the reward information is used to update *only* the parameters for that specific product. The agent's knowledge about the other nine 'Fish Supplies' items remains unchanged. It is blind to the category-wide trend and must wastefully re-explore each of the ten fish-related products one by one.
    *   **`NeuralUCBAgent_LastLayer`:** When a high reward is observed for one 'Fish Supplies' item, the agent performs its dual update. The backpropagation step adjusts the parameters `\Theta_\phi` of the feature extractor. This update, driven by a single positive example, refines the representation of the *entire 'Fish Supplies' category* in the `d_{emb}`-dimensional embedding space. The agent immediately generalizes this new insight. When presented with the next `budget_shopper`, it will "see" that all fish-related products now have more promising embeddings `z` and will prioritize exploring them. Its CTR should rise dramatically as it rapidly exploits the new market dynamic.

#### **Code: The Head-to-Head Simulation**

The following code block has been revised to implement the rigorously controlled experiment.

**1. Motivate the Code:** This script is the crucible for our theory. It will execute the controlled experiment by setting up both agents, pre-generating the user sequence, running both agents through this identical sequence, and collecting the results required to validate our hypotheses. The inclusion of detailed logging will allow the student to observe the experiment's progress in real time.

**2. Introduce and Dissect the Code:** The structure has been modified for clarity and correctness.
*   First, we define our constants and initialize the environment and agents.
*   **The crucial change:** We create a `user_sequence` array *before* the main loop.
*   The main simulation is now a single loop from `t = 0` to `N_STEPS`. Inside the loop, we fetch `user_id = user_sequence[t]`. This same `user_id` is passed to `linucb_agent.predict` and `neural_agent.predict`. This enforces our controlled experiment principle.
*   We then update both agents based on their individual actions and rewards.
*   Logging has been enhanced to provide progress updates, including a snapshot of the current smoothed CTR, giving a more dynamic view of the learning process.

**3. Assemble the Full, Runnable Block:**

```python
# ==============================================================================
# Revised, Runnable Experiment for Section 3.7
# ==============================================================================
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm # Or from tqdm import tqdm for standard scripts

# Assume all classes from the context prompt and previous sections are defined:
# ZooplusSimulatorShock, FeatureEngineer, LinUCBAgent, NeuralUCB_LastLayer, etc.

# --- 1. Experiment Setup ---
print(">>> STAGE 1: SETTING UP THE EXPERIMENT <<<")
N_STEPS = 20000
SHOCK_STEP = 10000
SEED = 42
EMBEDDING_DIM = 32
ALPHA = 0.2
LEARNING_RATE = 0.01
ROLLING_WINDOW = 500

# We need only one simulation environment now.
# The agents will interact with this single source of truth.
sim_env = ZooplusSimulatorShock(seed=SEED)

# Shared feature engineer
feature_engineer = FeatureEngineer(sim_env)
print("-" * 50)

# --- 2. Instantiate Agents ---
print(">>> STAGE 2: INSTANTIATING AGENTS <<<")
linucb_agent = LinUCBAgent(
    n_products=sim_env.n_products,
    feature_dim=feature_engineer.feature_dim,
    alpha=ALPHA
)

neural_agent = NeuralUCBAgent_LastLayer(
    feature_engineer=feature_engineer,
    embedding_dim=EMBEDDING_DIM,
    alpha=ALPHA,
    lr=LEARNING_RATE
)
print("-" * 50)

# --- 3. Pre-generate the Controlled User Sequence ---
print(f">>> STAGE 3: PRE-GENERATING USER SEQUENCE ({N_STEPS} users) <<<")
# This is the CRITICAL step for a fair, controlled experiment.
rng = np.random.default_rng(SEED)
user_sequence = rng.integers(0, sim_env.n_users, size=N_STEPS)
print("User sequence created.")
print("-" * 50)

# --- 4. Run The Simulation ---
print(">>> STAGE 4: RUNNING SIMULATION <<<")
rewards_linucb = []
rewards_neural = []

# Use tqdm for a nice progress bar
for t in tqdm(range(N_STEPS), desc="Simulating Timesteps"):
    # Introduce market shock at the specified step
    if t == SHOCK_STEP:
        sim_env.introduce_market_shock()

    # Get the SAME user for both agents from our pre-generated sequence
    user_id = user_sequence[t]

    # --- LinUCB Agent's turn ---
    chosen_arm_linucb = linucb_agent.predict(user_id, sim_env)
    reward_linucb = sim_env.get_reward(user_id, chosen_arm_linucb)
    linucb_agent.update(user_id, chosen_arm_linucb, reward_linucb, sim_env)
    rewards_linucb.append(reward_linucb)

    # --- NeuralUCB Agent's turn ---
    chosen_arm_neural = neural_agent.predict(user_id, sim_env)
    reward_neural = sim_env.get_reward(user_id, chosen_arm_neural)
    neural_agent.update(user_id, chosen_arm_neural, reward_neural, sim_env)
    rewards_neural.append(reward_neural)

    # Log progress periodically
    if (t + 1) % 2500 == 0:
        # Calculate current rolling CTR for logging
        current_ctr_linucb = np.mean(rewards_linucb[-ROLLING_WINDOW:])
        current_ctr_neural = np.mean(rewards_neural[-ROLLING_WINDOW:])
        print(f"  > Step {t+1}/{N_STEPS}: "
              f"LinUCB CTR={current_ctr_linucb:.3f}, "
              f"NeuralUCB CTR={current_ctr_neural:.3f}")

print("Simulation complete.")
print("-" * 50)

# --- 5. Process and Plot Results ---
print(">>> STAGE 5: PROCESSING AND PLOTTING RESULTS <<<")

# Calculate rolling average of CTR
ctr_linucb = pd.Series(rewards_linucb).rolling(window=ROLLING_WINDOW, min_periods=1).mean()
ctr_neural = pd.Series(rewards_neural).rolling(window=ROLLING_WINDOW, min_periods=1).mean()

plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(14, 8))

ax.plot(ctr_linucb, label='LinUCB (Disjoint Model)', color='C0', alpha=0.8)
ax.plot(ctr_neural, label=f'NeuralUCB (Generalizing Model, d_emb={EMBEDDING_DIM})', color='C1', linewidth=2)

# Add market shock indicator line
ax.axvline(x=SHOCK_STEP, color='r', linestyle='--', linewidth=2, label=f'Market Shock at t={SHOCK_STEP}')

ax.set_title('Generalization vs. Disjoint Model Adaptability', fontsize=18, pad=15)
ax.set_xlabel('Timestep', fontsize=12)
ax.set_ylabel(f'Click-Through Rate (CTR) - {ROLLING_WINDOW}-step Rolling Average', fontsize=12)
ax.legend(fontsize=12, loc='upper left')
ax.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

# Final performance comparison
final_ctr_linucb = np.mean(rewards_linucb[SHOCK_STEP:])
final_ctr_neural = np.mean(rewards_neural[SHOCK_STEP:])

print(f"\n--- Performance Analysis Post-Shock (steps {SHOCK_STEP} to {N_STEPS}) ---")
print(f"  LinUCB Agent Average CTR: {final_ctr_linucb:.4f}")
print(f"  NeuralUCB Agent Average CTR: {final_ctr_neural:.4f}")
print(f"  Performance Lift: {(final_ctr_neural - final_ctr_linucb) / final_ctr_linucb:.2%}")
```

With this rigorously designed experiment now codified, we are prepared to execute it. The resulting plot will provide the visual evidence needed to confirm our hypotheses and draw our final conclusions for this chapter.




```python
# to unify the structure of the code let us also update a bit our LinUCB agent
# ==============================================================================
# The Refactored LinUCBAgent
# ==============================================================================
class LinUCBAgent:
    """
    Implements the LinUCB algorithm with a disjoint model for each arm.
    
    REVISED: This agent has been refactored to be self-contained. It now holds
    a reference to the feature_engineer and conforms to the same .predict() and
    .update() API as the NeuralUCB agents, making experiments cleaner.
    """
    def __init__(self, n_products: int, feature_engineer: FeatureEngineer, alpha: float = 1.0):
        self.n_products = n_products
        self.alpha = alpha
        
        # --- REFACTOR ---
        # Store the feature engineer and get feature_dim from it.
        self.feature_engineer = feature_engineer
        self.feature_dim = self.feature_engineer.feature_dim
        
        # A: (d x d) matrix for each arm
        self.A = [np.identity(self.feature_dim) for _ in range(n_products)]
        # b: (d x 1) vector for each arm
        self.b = [np.zeros((self.feature_dim, 1)) for _ in range(n_products)]
        # No need to print initialization here, we'll do it in the main script.

    def predict(self, user_id: int, sim: ZooplusSimulatorShock) -> int:
        """
        Calculates the UCB score for each arm and chooses the best one.
        
        REVISED: Now takes user_id and sim, and creates features internally.
        """
        # --- REFACTOR ---
        # The agent is now responsible for creating its own features.
        feature_vectors = self.feature_engineer.create_features(user_id, sim)
        
        scores = np.zeros(self.n_products)
        
        # This is a naive implementation that re-calculates the inverse
        # at every step. For small d, this is acceptable. We could optimize
        # by caching inverses, but we keep it simple for clarity.
        for arm_idx in range(self.n_products):
            A_inv = np.linalg.inv(self.A[arm_idx])
            theta_hat = A_inv @ self.b[arm_idx]
            x = feature_vectors[arm_idx].reshape(-1, 1) # (d x 1)
            
            exploit_term = (theta_hat.T @ x).item()
            explore_term = self.alpha * np.sqrt(x.T @ A_inv @ x).item()
            
            scores[arm_idx] = exploit_term + explore_term
            
        return np.argmax(scores)

    def update(self, user_id: int, chosen_arm: int, reward: int, sim: ZooplusSimulatorShock):
        """
        Updates the model parameters for the chosen arm.
        
        REVISED: Now takes user_id and sim, and creates features internally.
        """
        # --- REFACTOR ---
        # The agent is now responsible for creating its own features.
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        feature_vector = feature_matrix[chosen_arm]
        
        x = feature_vector.reshape(-1, 1) # (d x 1)
        self.A[chosen_arm] += x @ x.T
        self.b[chosen_arm] += reward * x
```


```python
class NeuralUCBNetwork_LastLayer(nn.Module):
    """
    A network designed for the Neural-Linear bandit. It separates the
    feature extractor from the final linear layer.
    """
    def __init__(self, input_dim: int, embedding_dim: int = 32, hidden_dim: int = 64):
        super().__init__()
        # The feature extractor part 'phi'
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        # The final linear layer is handled by the bandit's UCB logic,
        # so it's not explicitly defined as an nn.Linear layer here.
        # This network only produces the embeddings 'z'.

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Returns the learned embedding 'z' for the input 'x'."""
        return self.feature_extractor(x)
```


```python
class NeuralUCBAgent_LastLayer:
    """
    Implements the practical and efficient Neural-Linear bandit algorithm.
    """
    def __init__(self, feature_engineer, embedding_dim=32, alpha=1.0, lr=0.01):
        self.feature_engineer = feature_engineer
        self.feature_dim = feature_engineer.feature_dim
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        
        self.model = NeuralUCBNetwork_LastLayer(self.feature_dim, self.embedding_dim).to(device)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # A: (d_emb x d_emb) matrix, b: (d_emb x 1) vector
        self.A = np.identity(self.embedding_dim)
        self.b = np.zeros((self.embedding_dim, 1))
        print(f"Neural-Linear Agent Initialized. Embedding dimension: {self.embedding_dim}")

    def predict(self, user_id, sim):
        """
        Calculates UCB scores efficiently using a single batch forward pass.
        """
        self.model.eval()
        
        # 1. Get feature matrix for all arms
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_all = torch.tensor(feature_matrix, dtype=torch.float32).to(device)
        
        # 2. Get all embeddings 'z' in a single, efficient forward pass
        with torch.no_grad():
            z_all = self.model(x_all).cpu().numpy() # (K x d_emb)
        
        # 3. Perform the fast linear UCB calculation in NumPy
        A_inv = np.linalg.inv(self.A)
        theta_hat = A_inv @ self.b # (d_emb x 1)
        
        # Predicted rewards (exploitation term) for all arms
        predicted_rewards = z_all @ theta_hat # (K x 1)
        
        # Exploration bonuses for all arms
        # (z @ A_inv) is (K x d_emb), so we multiply element-wise with z
        # and sum along the d_emb axis.
        exploration_bonuses = self.alpha * np.sqrt(np.sum((z_all @ A_inv) * z_all, axis=1))
        
        scores = predicted_rewards.flatten() + exploration_bonuses
        
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        """
        Performs the dual update: updates A/b for the linear part and trains
        the feature extractor network via backpropagation.
        """
        self.model.train()
        
        # Get feature vector for the chosen arm
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen = torch.tensor(feature_matrix[chosen_arm], dtype=torch.float32).to(device)
        
        # --- Part 1: Update the Linear UCB Components (A and b) ---
        # Re-compute the chosen embedding 'z' with gradients enabled
        z_chosen = self.model(x_chosen)
        z_chosen_np = z_chosen.detach().cpu().numpy().reshape(-1, 1) # (d_emb x 1)
        
        self.A += z_chosen_np @ z_chosen_np.T
        self.b += reward * z_chosen_np
        
        # --- Part 2: Update the Neural Network Feature Extractor ---
        # Estimate the linear parameters for the training step
        A_inv = np.linalg.inv(self.A)
        theta_hat = A_inv @ self.b
        theta_hat_torch = torch.tensor(theta_hat, dtype=torch.float32).to(device)
        
        # The network's prediction is the dot product of the embedding and theta
        prediction = z_chosen @ theta_hat_torch
        
        # Backpropagate the loss to train the feature extractor
        self.optimizer.zero_grad()
        loss = self.loss_fn(prediction.squeeze(), torch.tensor(reward, dtype=torch.float32).to(device))
        loss.backward()
        self.optimizer.step()
```


```python
# ==============================================================================
# FINAL SCRIPT: Fully Functional Experiment for Section 3.7
# ==============================================================================
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm # Use this for standard .py scripts

# --- Ensure all required classes are defined in your environment ---
# ZooplusSimulatorShock, FeatureEngineer, the NEW LinUCBAgent, NeuralUCBAgent_LastLayer

# --- 1. Experiment Setup ---
print(">>> STAGE 1: SETTING UP THE EXPERIMENT <<<")
N_STEPS = 20000
SHOCK_STEP = 10000
SEED = 42
EMBEDDING_DIM = 32
ALPHA = 0.2
LEARNING_RATE = 0.01
ROLLING_WINDOW = 500

sim_env = ZooplusSimulatorShock(seed=SEED)
feature_engineer = FeatureEngineer(sim_env)
print("-" * 50)

# --- 2. Instantiate Agents (using the refactored LinUCBAgent) ---
print(">>> STAGE 2: INSTANTIATING AGENTS <<<")
linucb_agent = LinUCBAgent(
    n_products=sim_env.n_products,
    feature_engineer=feature_engineer, # Pass the engineer object
    alpha=ALPHA
)
print("Initialized LinUCBAgent (Disjoint Model).")

neural_agent = NeuralUCBAgent_LastLayer(
    feature_engineer=feature_engineer,
    embedding_dim=EMBEDDING_DIM,
    alpha=ALPHA,
    lr=LEARNING_RATE
)
print("Initialized NeuralUCB_LastLayer (Generalizing Model).")
print("-" * 50)

# --- 3. Pre-generate the Controlled User Sequence ---
print(f">>> STAGE 3: PRE-GENERATING USER SEQUENCE ({N_STEPS} users) <<<")
rng = np.random.default_rng(SEED)
user_sequence = rng.integers(0, sim_env.n_users, size=N_STEPS)
print("User sequence created.")
print("-" * 50)

# --- 4. Run The Simulation ---
print(">>> STAGE 4: RUNNING SIMULATION <<<")
rewards_linucb = []
rewards_neural = []

for t in tqdm(range(N_STEPS), desc="Simulating Timesteps"):
    if t == SHOCK_STEP:
        sim_env.introduce_market_shock()

    user_id = user_sequence[t]

    # --- LinUCB Agent's turn ---
    chosen_arm_linucb = linucb_agent.predict(user_id, sim_env)
    reward_linucb = sim_env.get_reward(user_id, chosen_arm_linucb)
    linucb_agent.update(user_id, chosen_arm_linucb, reward_linucb, sim_env)
    rewards_linucb.append(reward_linucb)

    # --- NeuralUCB Agent's turn ---
    chosen_arm_neural = neural_agent.predict(user_id, sim_env)
    reward_neural = sim_env.get_reward(user_id, chosen_arm_neural)
    neural_agent.update(user_id, chosen_arm_neural, reward_neural, sim_env)
    rewards_neural.append(reward_neural)

    if (t + 1) % 2500 == 0:
        current_ctr_linucb = np.mean(rewards_linucb[-ROLLING_WINDOW:])
        current_ctr_neural = np.mean(rewards_neural[-ROLLING_WINDOW:])
        print(f"  > Step {t+1}/{N_STEPS}: "
              f"LinUCB CTR={current_ctr_linucb:.3f}, "
              f"NeuralUCB CTR={current_ctr_neural:.3f}")

print("Simulation complete.")
print("-" * 50)

# --- 5. Process and Plot Results ---
print(">>> STAGE 5: PROCESSING AND PLOTTING RESULTS <<<")

ctr_linucb = pd.Series(rewards_linucb).rolling(window=ROLLING_WINDOW, min_periods=1).mean()
ctr_neural = pd.Series(rewards_neural).rolling(window=ROLLING_WINDOW, min_periods=1).mean()

plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(14, 8))
ax.plot(ctr_linucb, label='LinUCB (Disjoint Model)', color='C0', alpha=0.8)
ax.plot(ctr_neural, label=f'NeuralUCB (Generalizing Model, d_emb={EMBEDDING_DIM})', color='C1', linewidth=2)
ax.axvline(x=SHOCK_STEP, color='r', linestyle='--', linewidth=2, label=f'Market Shock at t={SHOCK_STEP}')
ax.set_title('Generalization vs. Disjoint Model Adaptability', fontsize=18, pad=15)
ax.set_xlabel('Timestep', fontsize=12)
ax.set_ylabel(f'Click-Through Rate (CTR) - {ROLLING_WINDOW}-step Rolling Average', fontsize=12)
ax.legend(fontsize=12, loc='upper left')
ax.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

final_ctr_linucb = np.mean(rewards_linucb[SHOCK_STEP:])
final_ctr_neural = np.mean(rewards_neural[SHOCK_STEP:])

print(f"\n--- Performance Analysis Post-Shock (steps {SHOCK_STEP} to {N_STEPS}) ---")
print(f"  LinUCB Agent Average CTR: {final_ctr_linucb:.4f}")
print(f"  NeuralUCB Agent Average CTR: {final_ctr_neural:.4f}")
print(f"  Performance Lift: {(final_ctr_neural - final_ctr_linucb) / final_ctr_linucb:.2%}")
```

    >>> STAGE 1: SETTING UP THE EXPERIMENT <<<
    Zooplus Simulator Initialized.
    Feature Engineer Initialized. Feature dimension: 9
    --------------------------------------------------
    >>> STAGE 2: INSTANTIATING AGENTS <<<
    Initialized LinUCBAgent (Disjoint Model).
    Neural-Linear Agent Initialized. Embedding dimension: 32
    Initialized NeuralUCB_LastLayer (Generalizing Model).
    --------------------------------------------------
    >>> STAGE 3: PRE-GENERATING USER SEQUENCE (20000 users) <<<
    User sequence created.
    --------------------------------------------------
    >>> STAGE 4: RUNNING SIMULATION <<<


    Simulating Timesteps:  13%|█▎        | 2560/20000 [00:08<00:56, 306.93it/s]

      > Step 2500/20000: LinUCB CTR=0.486, NeuralUCB CTR=0.290


    Simulating Timesteps:  25%|██▌       | 5055/20000 [00:16<00:47, 317.78it/s]

      > Step 5000/20000: LinUCB CTR=0.516, NeuralUCB CTR=0.314


    Simulating Timesteps:  38%|███▊      | 7551/20000 [00:26<00:43, 285.47it/s]

      > Step 7500/20000: LinUCB CTR=0.502, NeuralUCB CTR=0.320


    Simulating Timesteps:  50%|█████     | 10062/20000 [00:34<00:31, 313.49it/s]

      > Step 10000/20000: LinUCB CTR=0.496, NeuralUCB CTR=0.346
    
    --- MARKET SHOCK! ---
    Updating 'budget_shopper' persona preferences for 'Fish Supplies'.
    Environment reward function has been updated.
    


    Simulating Timesteps:  63%|██████▎   | 12541/20000 [00:43<00:24, 306.49it/s]

      > Step 12500/20000: LinUCB CTR=0.450, NeuralUCB CTR=0.576


    Simulating Timesteps:  75%|███████▌  | 15049/20000 [00:51<00:15, 319.02it/s]

      > Step 15000/20000: LinUCB CTR=0.450, NeuralUCB CTR=0.586


    Simulating Timesteps:  88%|████████▊ | 17550/20000 [00:59<00:08, 303.65it/s]

      > Step 17500/20000: LinUCB CTR=0.434, NeuralUCB CTR=0.622


    Simulating Timesteps: 100%|██████████| 20000/20000 [01:07<00:00, 294.61it/s]


      > Step 20000/20000: LinUCB CTR=0.640, NeuralUCB CTR=0.612
    Simulation complete.
    --------------------------------------------------
    >>> STAGE 5: PROCESSING AND PLOTTING RESULTS <<<



    
![png](Chapter%203%20v3_files/Chapter%203%20v3_21_17.png)
    


    
    --- Performance Analysis Post-Shock (steps 10000 to 20000) ---
      LinUCB Agent Average CTR: 0.4650
      NeuralUCB Agent Average CTR: 0.5440
      Performance Lift: 16.99%


### **Notes 3.2: On Training Stability and the Replay Buffer**

In our `NeuralUCBAgent_LastLayer`, we update the feature extractor network $\phi$ after every single user interaction. This is known as **pure online learning**. While this allows for maximum responsiveness, it can be problematic for deep, non-convex models like neural networks. The stream of incoming data is often highly correlated; a user might interact with several similar items consecutively, or a popular item might appear many times in a short period. Training on this non-i.i.d. (independent and identically distributed) stream of data can cause the network's weights to oscillate or drift, leading to unstable performance and forgetting past knowledge. This is a phenomenon known as **catastrophic forgetting**.

To build a more robust and stable agent, we can introduce two key mechanisms: a **replay buffer** and **batched, periodic training**.

#### **Intuition: Learning from a Coherent Collection of Memories**

Imagine our agent as a student. The pure online learning approach is akin to the student trying to learn a complex subject by reading one random, isolated sentence at a time. If they happen to read several sentences about a niche topic, they might over-correct their entire understanding of the subject based on that fleeting, correlated burst of information.

A more effective student would collect notes over time. They would then sit down periodically to review a diverse, shuffled set of these notes. By studying a batch of varied examples together, they can form a more stable, coherent, and robust mental model that isn't swayed by the last single piece of information they saw.

This is precisely what a replay buffer enables.
*   **The Replay Buffer:** A memory that stores a large history of recent interactions (`context`, `reward`).
*   **Batched, Periodic Training:** Instead of updating the network after every interaction, we do so periodically (e.g., every 100 steps). At that time, we sample a random *batch* of memories from the buffer and perform a single, more stable gradient descent step on this batch. This breaks the temporal correlations in the data stream and provides a more stable, averaged gradient for the update.

#### **Rigor: The Hybrid Update Schedule**

It is absolutely critical to understand that this batched update strategy applies *only* to the slow-moving representation learner, the neural network `φ`. The fast-moving linear bandit component, which is responsible for quantifying uncertainty for the *very next* decision, **must** be updated online.

***
**Definition 3.4: Hybrid Update Schedule with Replay Buffer**

Let `B` be a replay buffer of fixed capacity `C` storing experience tuples $(x_t, r_t)$. Let $T_{train}$ be the frequency of training (e.g., update every 100 steps), and let `M` be the minibatch size.

At each timestep $t$, upon observing the interaction $(x_{t, a_t}, r_t)$ for the chosen arm $a_t$:

1.  **Online Linear Update (Always):** The UCB components are updated immediately using the embedding of the chosen arm, $z_{t, a_t} = φ(x_{t, a_t})$:
    $$ \mathbf{A}_t = \mathbf{A}_{t-1} + \mathbf{z}_{t, a_t} \mathbf{z}_{t, a_t}^\top $$
    $$ \mathbf{b}_t = \mathbf{b}_{t-1} + r_t \cdot \mathbf{z}_{t, a_t} $$

2.  **Store Experience:** The tuple $(x_{t, a_t}, r_t)$ is added to the replay buffer `B`.

3.  **Periodic Neural Update (Conditional):** If $t$ is a multiple of $T_{train}$:
    a. Sample a random minibatch of `M` experiences ${(x_i, r_i)}_{i=1}^M$ from `B`.
    b. Perform a single batched gradient descent step on the neural network $\phi$ using this minibatch to minimize the batch loss.

***

This hybrid schedule gives us the best of both worlds: the bandit's uncertainty estimates are always up-to-date, while the deep representation network learns in a stable and efficient manner.

#### **Application: A More Stable Neural-Linear Agent**

Let us now codify this improved agent.

```python
# ==============================================================================
# CORRECTED CLASS: The Robust NeuralUCB Agent
# ==============================================================================
import collections
import random

class NeuralUCBNetwork_WithHead(nn.Module):
    """REVISED: Network now includes an explicit linear head for stable training."""
    def __init__(self, input_dim: int, embedding_dim: int = 32, hidden_dim: int = 64):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        # Explicit head for generating predictions for training
        self.linear_head = nn.Linear(embedding_dim, 1)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """Returns both the embedding 'z' for UCB decisions and the final prediction for training."""
        embedding = self.feature_extractor(x)
        prediction = self.linear_head(embedding)
        return embedding, prediction

class NeuralUCBAgent_Stable:
    """
    The FINAL, robust version of the Neural-Linear bandit.
    - Uses a replay buffer for stable, batched training.
    - Uses a network with an explicit head for a clean training signal.
    """
    def __init__(self, feature_engineer, embedding_dim=32, alpha=0.3, lr=0.01,
                 buffer_size=10000, batch_size=64, train_every_steps=50, **kwargs):
        self.feature_engineer = feature_engineer
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        self.buffer_size = buffer_size
        self.batch_size = batch_size
        self.train_every_steps = train_every_steps
        
        self.model = NeuralUCBNetwork_WithHead(feature_engineer.feature_dim, embedding_dim).to(device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        self.A = np.identity(self.embedding_dim)
        self.b = np.zeros((self.embedding_dim, 1))
        
        self.replay_buffer = collections.deque(maxlen=self.buffer_size)
        self.step_counter = 0

    def predict(self, user_id, sim):
        self.model.eval()
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_all = torch.tensor(feature_matrix, dtype=torch.float32).to(device)
        
        with torch.no_grad():
            # We only need the embeddings for the UCB decision
            z_all, _ = self.model(x_all)
            z_all_np = z_all.cpu().numpy()
        
        A_inv = np.linalg.inv(self.A)
        theta_hat = A_inv @ self.b
        predicted_rewards = z_all_np @ theta_hat
        exploration_bonuses = self.alpha * np.sqrt(np.sum((z_all_np @ A_inv) * z_all_np, axis=1))
        scores = predicted_rewards.flatten() + exploration_bonuses
        
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        self.step_counter += 1
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen_np = feature_matrix[chosen_arm]
        
        # Online Linear Update (using the embedding)
        with torch.no_grad():
            x_tensor = torch.tensor(x_chosen_np, dtype=torch.float32).to(device)
            z_chosen_np, _ = self.model(x_tensor)
            z_chosen_np = z_chosen_np.cpu().numpy().reshape(-1, 1)
        self.A += z_chosen_np @ z_chosen_np.T
        self.b += reward * z_chosen_np

        # Store experience for neural net training
        self.replay_buffer.append((x_chosen_np, reward))
        
        # Periodic Neural Update
        if self.step_counter % self.train_every_steps == 0:
            self._train_from_batch()

    def _train_from_batch(self):
        if len(self.replay_buffer) < self.batch_size:
            return
        self.model.train()
        minibatch = random.sample(self.replay_buffer, self.batch_size)
        x_batch, r_batch = zip(*minibatch)
        x_batch_tensor = torch.tensor(np.array(x_batch), dtype=torch.float32).to(device)
        r_batch_tensor = torch.tensor(np.array(r_batch), dtype=torch.float32).to(device).view(-1, 1)

        # CORRECTED TRAINING: Use the network's own end-to-end prediction
        _, predictions = self.model(x_batch_tensor)
        
        loss = self.loss_fn(predictions, r_batch_tensor)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

This `NeuralUCBAgent_Stable` class is a drop-in replacement for `NeuralUCBAgent_LastLayer`. While running it would likely produce a smoother and potentially faster-converging learning curve, it would not change the fundamental conclusion of our experiment: the generalizing agent, whether trained online or via a replay buffer, will dramatically outperform the disjoint agent in a non-stationary environment. We therefore leave it to the ambitious student to run this comparison as an optional exercise.


```python
# ==============================================================================
# CORRECTED CLASS: The Robust NeuralUCB Agent
# ==============================================================================
import collections
import random

class NeuralUCBNetwork_WithHead(nn.Module):
    """REVISED: Network now includes an explicit linear head for stable training."""
    def __init__(self, input_dim: int, embedding_dim: int = 32, hidden_dim: int = 64):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        # Explicit head for generating predictions for training
        self.linear_head = nn.Linear(embedding_dim, 1)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """Returns both the embedding 'z' for UCB decisions and the final prediction for training."""
        embedding = self.feature_extractor(x)
        prediction = self.linear_head(embedding)
        return embedding, prediction

class NeuralUCBAgent_Stable:
    """
    The FINAL, robust version of the Neural-Linear bandit.
    - Uses a replay buffer for stable, batched training.
    - Uses a network with an explicit head for a clean training signal.
    """
    def __init__(self, feature_engineer, embedding_dim=32, alpha=0.3, lr=0.01,
                 buffer_size=10000, batch_size=64, train_every_steps=50, **kwargs):
        self.feature_engineer = feature_engineer
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        self.buffer_size = buffer_size
        self.batch_size = batch_size
        self.train_every_steps = train_every_steps
        
        self.model = NeuralUCBNetwork_WithHead(feature_engineer.feature_dim, embedding_dim).to(device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        self.A = np.identity(self.embedding_dim)
        self.b = np.zeros((self.embedding_dim, 1))
        
        self.replay_buffer = collections.deque(maxlen=self.buffer_size)
        self.step_counter = 0

    def predict(self, user_id, sim):
        self.model.eval()
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_all = torch.tensor(feature_matrix, dtype=torch.float32).to(device)
        
        with torch.no_grad():
            # We only need the embeddings for the UCB decision
            z_all, _ = self.model(x_all)
            z_all_np = z_all.cpu().numpy()
        
        A_inv = np.linalg.inv(self.A)
        theta_hat = A_inv @ self.b
        predicted_rewards = z_all_np @ theta_hat
        exploration_bonuses = self.alpha * np.sqrt(np.sum((z_all_np @ A_inv) * z_all_np, axis=1))
        scores = predicted_rewards.flatten() + exploration_bonuses
        
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        self.step_counter += 1
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_chosen_np = feature_matrix[chosen_arm]
        
        # Online Linear Update (using the embedding)
        with torch.no_grad():
            x_tensor = torch.tensor(x_chosen_np, dtype=torch.float32).to(device)
            z_chosen_np, _ = self.model(x_tensor)
            z_chosen_np = z_chosen_np.cpu().numpy().reshape(-1, 1)
        self.A += z_chosen_np @ z_chosen_np.T
        self.b += reward * z_chosen_np

        # Store experience for neural net training
        self.replay_buffer.append((x_chosen_np, reward))
        
        # Periodic Neural Update
        if self.step_counter % self.train_every_steps == 0:
            self._train_from_batch()

    def _train_from_batch(self):
        if len(self.replay_buffer) < self.batch_size:
            return
        self.model.train()
        minibatch = random.sample(self.replay_buffer, self.batch_size)
        x_batch, r_batch = zip(*minibatch)
        x_batch_tensor = torch.tensor(np.array(x_batch), dtype=torch.float32).to(device)
        r_batch_tensor = torch.tensor(np.array(r_batch), dtype=torch.float32).to(device).view(-1, 1)

        # CORRECTED TRAINING: Use the network's own end-to-end prediction
        _, predictions = self.model(x_batch_tensor)
        
        loss = self.loss_fn(predictions, r_batch_tensor)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```


```python
# ==============================================================================
# FINAL SCRIPT: Fully Functional Experiment for Section 3.7
# ==============================================================================
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm # Use this for standard .py scripts

# --- Ensure all required classes are defined in your environment ---
# ZooplusSimulatorShock, FeatureEngineer, the NEW LinUCBAgent, NeuralUCBAgent_LastLayer

# --- 1. Experiment Setup ---
print(">>> STAGE 1: SETTING UP THE EXPERIMENT <<<")
N_STEPS = 20000
SHOCK_STEP = 10000
SEED = 42
EMBEDDING_DIM = 32
ALPHA = 0.2
LEARNING_RATE = 0.01
ROLLING_WINDOW = 500

sim_env = ZooplusSimulatorShock(seed=SEED)
feature_engineer = FeatureEngineer(sim_env)
print("-" * 50)

# --- 2. Instantiate Agents (using the refactored LinUCBAgent) ---
print(">>> STAGE 2: INSTANTIATING AGENTS <<<")
linucb_agent = LinUCBAgent(
    n_products=sim_env.n_products,
    feature_engineer=feature_engineer, # Pass the engineer object
    alpha=ALPHA
)
print("Initialized LinUCBAgent (Disjoint Model).")

neural_agent = NeuralUCBAgent_Stable(
    feature_engineer=feature_engineer,
    embedding_dim=EMBEDDING_DIM,
    alpha=ALPHA,
    lr=LEARNING_RATE
)
print("Initialized NeuralUCB_LastLayer (Generalizing Model).")
print("-" * 50)

# --- 3. Pre-generate the Controlled User Sequence ---
print(f">>> STAGE 3: PRE-GENERATING USER SEQUENCE ({N_STEPS} users) <<<")
rng = np.random.default_rng(SEED)
user_sequence = rng.integers(0, sim_env.n_users, size=N_STEPS)
print("User sequence created.")
print("-" * 50)

# --- 4. Run The Simulation ---
print(">>> STAGE 4: RUNNING SIMULATION <<<")
rewards_linucb = []
rewards_neural = []

for t in tqdm(range(N_STEPS), desc="Simulating Timesteps"):
    if t == SHOCK_STEP:
        sim_env.introduce_market_shock()

    user_id = user_sequence[t]

    # --- LinUCB Agent's turn ---
    chosen_arm_linucb = linucb_agent.predict(user_id, sim_env)
    reward_linucb = sim_env.get_reward(user_id, chosen_arm_linucb)
    linucb_agent.update(user_id, chosen_arm_linucb, reward_linucb, sim_env)
    rewards_linucb.append(reward_linucb)

    # --- NeuralUCB Agent's turn ---
    chosen_arm_neural = neural_agent.predict(user_id, sim_env)
    reward_neural = sim_env.get_reward(user_id, chosen_arm_neural)
    neural_agent.update(user_id, chosen_arm_neural, reward_neural, sim_env)
    rewards_neural.append(reward_neural)

    if (t + 1) % 2500 == 0:
        current_ctr_linucb = np.mean(rewards_linucb[-ROLLING_WINDOW:])
        current_ctr_neural = np.mean(rewards_neural[-ROLLING_WINDOW:])
        print(f"  > Step {t+1}/{N_STEPS}: "
              f"LinUCB CTR={current_ctr_linucb:.3f}, "
              f"NeuralUCB CTR={current_ctr_neural:.3f}")

print("Simulation complete.")
print("-" * 50)

# --- 5. Process and Plot Results ---
print(">>> STAGE 5: PROCESSING AND PLOTTING RESULTS <<<")

ctr_linucb = pd.Series(rewards_linucb).rolling(window=ROLLING_WINDOW, min_periods=1).mean()
ctr_neural = pd.Series(rewards_neural).rolling(window=ROLLING_WINDOW, min_periods=1).mean()

plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(14, 8))
ax.plot(ctr_linucb, label='LinUCB (Disjoint Model)', color='C0', alpha=0.8)
ax.plot(ctr_neural, label=f'NeuralUCB (Generalizing Model, d_emb={EMBEDDING_DIM})', color='C1', linewidth=2)
ax.axvline(x=SHOCK_STEP, color='r', linestyle='--', linewidth=2, label=f'Market Shock at t={SHOCK_STEP}')
ax.set_title('Generalization vs. Disjoint Model Adaptability', fontsize=18, pad=15)
ax.set_xlabel('Timestep', fontsize=12)
ax.set_ylabel(f'Click-Through Rate (CTR) - {ROLLING_WINDOW}-step Rolling Average', fontsize=12)
ax.legend(fontsize=12, loc='upper left')
ax.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

final_ctr_linucb = np.mean(rewards_linucb[SHOCK_STEP:])
final_ctr_neural = np.mean(rewards_neural[SHOCK_STEP:])

print(f"\n--- Performance Analysis Post-Shock (steps {SHOCK_STEP} to {N_STEPS}) ---")
print(f"  LinUCB Agent Average CTR: {final_ctr_linucb:.4f}")
print(f"  NeuralUCB Agent Average CTR: {final_ctr_neural:.4f}")
print(f"  Performance Lift: {(final_ctr_neural - final_ctr_linucb) / final_ctr_linucb:.2%}")
```

    >>> STAGE 1: SETTING UP THE EXPERIMENT <<<
    Zooplus Simulator Initialized.
    Feature Engineer Initialized. Feature dimension: 9
    --------------------------------------------------
    >>> STAGE 2: INSTANTIATING AGENTS <<<
    Initialized LinUCBAgent (Disjoint Model).
    Initialized NeuralUCB_LastLayer (Generalizing Model).
    --------------------------------------------------
    >>> STAGE 3: PRE-GENERATING USER SEQUENCE (20000 users) <<<
    User sequence created.
    --------------------------------------------------
    >>> STAGE 4: RUNNING SIMULATION <<<


    Simulating Timesteps:  13%|█▎        | 2553/20000 [00:06<00:39, 439.03it/s]

      > Step 2500/20000: LinUCB CTR=0.486, NeuralUCB CTR=0.478


    Simulating Timesteps:  25%|██▌       | 5066/20000 [00:12<00:39, 373.71it/s]

      > Step 5000/20000: LinUCB CTR=0.516, NeuralUCB CTR=0.454


    Simulating Timesteps:  38%|███▊      | 7575/20000 [00:18<00:28, 435.93it/s]

      > Step 7500/20000: LinUCB CTR=0.502, NeuralUCB CTR=0.438


    Simulating Timesteps:  50%|█████     | 10057/20000 [00:24<00:24, 404.58it/s]

      > Step 10000/20000: LinUCB CTR=0.496, NeuralUCB CTR=0.420
    
    --- MARKET SHOCK! ---
    Updating 'budget_shopper' persona preferences for 'Fish Supplies'.
    Environment reward function has been updated.
    


    Simulating Timesteps:  63%|██████▎   | 12553/20000 [00:30<00:17, 437.54it/s]

      > Step 12500/20000: LinUCB CTR=0.450, NeuralUCB CTR=0.568


    Simulating Timesteps:  75%|███████▌  | 15055/20000 [00:35<00:11, 430.31it/s]

      > Step 15000/20000: LinUCB CTR=0.450, NeuralUCB CTR=0.578


    Simulating Timesteps:  88%|████████▊ | 17575/20000 [00:41<00:05, 447.77it/s]

      > Step 17500/20000: LinUCB CTR=0.434, NeuralUCB CTR=0.608


    Simulating Timesteps: 100%|██████████| 20000/20000 [00:47<00:00, 424.03it/s]


      > Step 20000/20000: LinUCB CTR=0.640, NeuralUCB CTR=0.584
    Simulation complete.
    --------------------------------------------------
    >>> STAGE 5: PROCESSING AND PLOTTING RESULTS <<<



    
![png](Chapter%203%20v3_files/Chapter%203%20v3_24_17.png)
    


    
    --- Performance Analysis Post-Shock (steps 10000 to 20000) ---
      LinUCB Agent Average CTR: 0.4650
      NeuralUCB Agent Average CTR: 0.5716
      Performance Lift: 22.92%


### **3.8. Analysis of Results and Chapter Conclusion**

We have journeyed from the simple disjoint LinUCB model to a sophisticated, stable, and scalable Neural-Linear agent. We have dissected the theory, confronted the computational realities, and engineered a practical solution. It is now time for the final act: a comprehensive, all-to-all comparison to empirically validate the hierarchy of intelligence we have constructed.

#### **The Grand Challenge: A Multi-Shock Gauntlet**

We will subject our four primary agent archetypes to a long and arduous test designed to rigorously evaluate their performance across different paradigms of learning.

**The Contenders:**
1.  **The Random Agent:** Our ultimate baseline. It makes decisions without knowledge or learning.
2.  **The Static MLP Agent (Chapter 1):** The embodiment of the batched learning paradigm. It possesses significant pre-trained knowledge but is incapable of adaptation.
3.  **The LinUCB Agent (Chapter 2):** Our first adaptive agent. It learns online but its knowledge is siloed within disjoint, non-generalizing models.
4.  **The Stable NeuralUCB Agent (Chapter 3):** The culmination of our work. It combines the generalization power of a deep feature extractor with the principled, efficient exploration of a linear bandit, stabilized by a replay buffer.

**The Arena:**
*   **Simulation Horizon:** `N_STEPS = 40,000` to allow for long-term learning and adaptation.
*   **The Gauntlet:** We will introduce **two** distinct market shocks to test for repeated adaptability:
    *   **Shock 1 (t=10,000):** The `budget_shopper` persona develops a strong preference for 'Fish Supplies'.
    *   **Shock 2 (t=25,000):** The `cat_connoisseur` persona suddenly develops a strong preference for 'Dog Toys'—a highly counter-intuitive shift.
*   **The Control:** A single, pre-generated sequence of 40,000 users will be used for all agents, ensuring every agent faces the exact same sequence of challenges.

#### **A Standardized Agent Interface: The Cornerstone of Fair Comparison**

To build a clean experimental harness, we must first ensure all our agents "speak the same language." We mandate that every agent must implement the following two methods:

*   `predict(self, user_id: int, sim: ZooplusSimulatorShock) -> int`: Takes the current user and simulation state and returns the index of the chosen arm.
*   `update(self, user_id: int, chosen_arm: int, reward: int, sim: ZooplusSimulatorShock)`: Allows the agent to learn from the outcome of its decision.

Our `LinUCBAgent` (as refactored) and `NeuralUCBAgent_Stable` already conform to this interface. We must now create simple wrapper classes for our Random and Static MLP agents to make them compliant.

#### **Code: The Unified Experimental Harness**

**1. Motivate the Code:** This single, comprehensive script will define all agent wrappers, set up the multi-shock experiment, run all four agents through the identical gauntlet, and generate a final, comparative plot that tells the complete story of our journey.

**2. Introduce and Dissect the Code:**
*   **Part A: The Agent Wrappers.** We start by defining simple classes for `RandomAgent` and `StaticMLPAgent`. These classes implement the standard `predict`/`update` interface. The `update` method for these non-learning agents will simply `pass`. This is an application of the **Adapter Pattern**, a crucial software engineering concept that allows incompatible interfaces to work together.
*   **Part B: The Experimental Setup.** We define all constants, including the path to the pre-trained MLP model and the schedule for our two market shocks.
*   **Part C: The Simulation Loop.** This is the heart of the script. It is a single, clean loop that iterates through the pre-generated user sequence. Inside the loop, it cycles through our dictionary of agents, calling `agent.predict()` and `agent.update()` for each one. Because we standardized the interface, this loop is beautifully simple, with no need for messy `if/elif` statements to handle different agent types.

**3. Assemble the Full, Runnable Block:**

```python
# ==============================================================================
# FINAL SCRIPT: The Grand Challenge - A Full, Comparative Experiment
# ==============================================================================
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os

# Assume all core classes are defined: ZooplusSimulatorShock, FeatureEngineer,
# MLPRecommender, LinUCBAgent (refactored), NeuralUCBAgent_Stable, etc.

# --- Part A: Standardized Agent Wrappers ---

class RandomAgent:
    """A simple agent that chooses products randomly. Conforms to the standard API."""
    def __init__(self, n_products: int, **kwargs):
        self.n_products = n_products
        self.rng = np.random.default_rng(seed=42)

    def predict(self, user_id: int, sim: ZooplusSimulatorShock) -> int:
        return self.rng.integers(self.n_products)

    def update(self, user_id: int, chosen_arm: int, reward: int, sim: ZooplusSimulatorShock):
        pass # This agent does not learn.

class StaticMLPAgent:
    """A wrapper for the pre-trained MLP model from Chapter 1. Conforms to the standard API."""
    def __init__(self, model_path: str, n_users: int, n_products: int, **kwargs):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found at: {model_path}. "
                                    "Please ensure the batched model is saved and the path is correct.")
        self.model = MLPRecommender(n_users, n_products).to(device)
        self.model.load_state_dict(torch.load(model_path, map_location=device))
        self.model.eval()
        self.n_products = n_products

    def predict(self, user_id: int, sim: ZooplusSimulatorShock) -> int:
        scores = get_batch_model_predictions(self.model, user_id, self.n_products, device)
        return np.argmax(scores)

    def update(self, user_id: int, chosen_arm: int, reward: int, sim: ZooplusSimulatorShock):
        pass # This agent does not adapt.

# --- Part B: Experiment Setup ---
print(">>> STAGE 1: SETTING UP THE GRAND CHALLENGE <<<")
N_STEPS = 40000
SHOCK_SCHEDULE = {10000: "Shock 1", 25000: "Shock 2"}
SEED = 42
ROLLING_WINDOW = 500
# IMPORTANT: Update this path to point to your saved model file from Chapter 1/2.
MLP_MODEL_PATH = "batch_recommender_model_ch2.pth" 

sim_env = ZooplusSimulatorShock(seed=SEED)
feature_engineer = FeatureEngineer(sim_env)

# --- Define all agents and their initialization parameters ---
agent_configs = {
    "Random Baseline": {
        "class": RandomAgent,
        "params": {"n_products": sim_env.n_products}
    },
    "Static MLP (Chap 1)": {
        "class": StaticMLPAgent,
        "params": {"model_path": MLP_MODEL_PATH, "n_users": sim_env.n_users, "n_products": sim_env.n_products}
    },
    "LinUCB (Disjoint)": {
        "class": LinUCBAgent,
        "params": {"n_products": sim_env.n_products, "feature_engineer": feature_engineer, "alpha": 0.2}
    },
    "NeuralUCB (Stable, Generalizing)": {
        "class": NeuralUCBAgent_Stable,
        "params": {"feature_engineer": feature_engineer, "embedding_dim": 32, "alpha": 0.3, "lr": 0.01, "train_every_steps": 50}
    }
}

agents = {name: config["class"](**config["params"]) for name, config in agent_configs.items()}
print("All agents instantiated.")
print("-" * 50)

# --- Pre-generate the controlled user sequence ---
print(f">>> STAGE 2: PRE-GENERATING USER SEQUENCE ({N_STEPS} users) <<<")
rng = np.random.default_rng(SEED)
user_sequence = rng.integers(0, sim_env.n_users, size=N_STEPS)
print("User sequence created.")
print("-" * 50)


# --- Part C: The Unified Simulation Loop ---
print(">>> STAGE 3: RUNNING SIMULATION GAUNTLET <<<")
rewards = {name: [] for name in agents.keys()}

for t in tqdm(range(N_STEPS), desc="Simulating Timesteps"):
    if t in SHOCK_SCHEDULE:
        sim_env.introduce_market_shock()
        # For the second shock, we'll redefine the shock behavior
        if t == 25000:
             sim_env.personas['cat_connoisseur']['Dog Toy'] = 0.75

    user_id = user_sequence[t]

    for name, agent in agents.items():
        chosen_arm = agent.predict(user_id, sim_env)
        reward = sim_env.get_reward(user_id, chosen_arm)
        agent.update(user_id, chosen_arm, reward, sim_env)
        rewards[name].append(reward)

print("Simulation complete.")
print("-" * 50)

# --- Part D: Processing and Plotting Results ---
print(">>> STAGE 4: PROCESSING AND PLOTTING FINAL RESULTS <<<")
ctr_results = {name: pd.Series(r).rolling(window=ROLLING_WINDOW).mean() for name, r in rewards.items()}

plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(16, 9))
colors = {
    "Random Baseline": "gray",
    "Static MLP (Chap 1)": "purple",
    "LinUCB (Disjoint)": "C0",
    "NeuralUCB (Stable, Generalizing)": "C1"
}

for name, ctr_data in ctr_results.items():
    ax.plot(ctr_data, label=name, color=colors[name],
            linestyle=':' if name in ["Random Baseline", "Static MLP (Chap 1)"] else '-',
            linewidth=2 if name in ["LinUCB (Disjoint)", "NeuralUCB (Stable, Generalizing)"] else 1.5,
            alpha=0.8)

for t_shock, shock_name in SHOCK_SCHEDULE.items():
    ax.axvline(x=t_shock, color='r', linestyle='--', linewidth=2, label=f'{shock_name} (t={t_shock})' if t_shock == 10000 else None)

ax.set_title('Grand Challenge: A Comparison of Recommender System Paradigms', fontsize=20, pad=20)
ax.set_xlabel('Timestep', fontsize=14)
ax.set_ylabel(f'Click-Through Rate (CTR) - {ROLLING_WINDOW}-step Rolling Average', fontsize=14)
ax.legend(fontsize=12, loc='best')
ax.grid(True, which='both', linestyle='--', linewidth=0.5)
ax.set_ylim(bottom=0)
ax.tick_params(axis='both', which='major', labelsize=12)
plt.tight_layout()
plt.show()

# --- Final Performance Summary ---
for name, r in rewards.items():
    overall_ctr = np.mean(r)
    post_shock1_ctr = np.mean(r[10000:25000])
    post_shock2_ctr = np.mean(r[25000:])
    print(f"\n--- Performance for: {name} ---")
    print(f"  Overall CTR: {overall_ctr:.4f}")
    print(f"  CTR after Shock 1: {post_shock1_ctr:.4f}")
    print(f"  CTR after Shock 2: {post_shock2_ctr:.4f}")
```

#### **Analysis of Results and Chapter Conclusion**

The plot generated by this experiment serves as the definitive visual summary of this book's central theses.
*   The **Random** agent establishes the floor of performance.
*   The **Static MLP** starts strong, leveraging its offline training, but its performance curve is tragically flat. It is a fossil, perfectly adapted to a world that no longer exists, completely blind to the market shocks. Its relative performance degrades over time as the adaptive agents learn and surpass it.
*   The **LinUCB** agent demonstrates the power of online learning. It starts from nothing and quickly learns the initial environment. After each shock, its performance dips, but it begins the arduous process of recovery. However, its disjoint nature is its Achilles' heel; the recovery is slow, as it must re-explore each of the ten products in the newly-favored category individually.
*   The **NeuralUCB** agent represents the pinnacle of our design. It learns effectively in the initial phase. Critically, after the first market shock, its CTR dips but then recovers with astonishing speed. A single high-reward signal from one 'Fish Supplies' item informs its understanding of the *entire category*, and it rapidly adapts its strategy. It demonstrates this remarkable adaptability a second time after the counter-intuitive second shock, proving its mechanism for generalization is robust.

**Conclusion:** We began this chapter with a simple, scalable, but non-generalizing online learner. We have concluded by engineering a sophisticated agent that marries the representation learning of deep networks with the principled exploration of contextual bandits. The final experiment is unequivocal: in dynamic, real-world environments, the ability to **generalize from sparse feedback** is not merely an advantage; it is the defining characteristic of an intelligent system. The Neural-Linear architecture provides a powerful, practical, and scalable framework for achieving this essential capability, representing a significant step towards building truly adaptive artificial intelligence.


```python
# ==============================================================================
#
# Foundations of Machine Learning: A Functional Analysis Perspective
# Chapter 3: The Grand Challenge - Full, Runnable Experiment
# Professor Alistair Finch, Stanford University
#
# ==============================================================================

# ==============================================================================
# SECTION 0: ALL REQUIRED IMPORTS
# ==============================================================================
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
import collections
import random

# ==============================================================================
# SECTION 1: DEVICE CONFIGURATION
# ==============================================================================
if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")
    print("✅ Using MPS (Apple Silicon) device for PyTorch.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("✅ Using CUDA device for PyTorch.")
else:
    device = torch.device("cpu")
    print("✅ Using CPU device for PyTorch.")

# ==============================================================================
# SECTION 2: CORE SIMULATION AND MODEL CLASSES (FROM PREVIOUS CHAPTERS)
# ==============================================================================

class ZooplusSimulatorShock:
    """The simulation environment, capable of introducing market shocks."""
    def __init__(self, n_products=50, n_users=1000, seed=42):
        self.rng = np.random.default_rng(seed)
        self.n_products = n_products
        self.n_users = n_users
        self.products = self._create_product_catalog()
        self.personas = self._create_user_personas()
        self.user_to_persona_map = self._assign_users_to_personas()

    def _create_product_catalog(self):
        categories = ['Fish Supplies', 'Cat Food', 'Dog Food', 'Dog Toy', 'Cat Toy']
        num_per_category = self.n_products // len(categories)
        cat_list = [cat for cat in categories for _ in range(num_per_category)]
        cat_list.extend(self.rng.choice(categories, self.n_products - len(cat_list)))
        return pd.DataFrame({
            'product_id': range(self.n_products),
            'category': self.rng.permutation(cat_list)
        }).set_index('product_id')

    def _create_user_personas(self):
        return {
            'new_puppy_parent': {'Dog Food': 0.40, 'Dog Toy': 0.50, 'Cat Food': 0.10, 'Cat Toy': 0.05, 'Fish Supplies': 0.02},
            'cat_connoisseur':  {'Dog Food': 0.05, 'Dog Toy': 0.02, 'Cat Food': 0.55, 'Cat Toy': 0.45, 'Fish Supplies': 0.05},
            'budget_shopper':   {'Dog Food': 0.25, 'Dog Toy': 0.15, 'Cat Food': 0.40, 'Cat Toy': 0.20, 'Fish Supplies': 0.20},
            'fish_hobbyist':    {'Dog Food': 0.02, 'Dog Toy': 0.02, 'Cat Food': 0.10, 'Cat Toy': 0.08, 'Fish Supplies': 0.60}
        }

    def _assign_users_to_personas(self):
        return {user_id: self.rng.choice(list(self.personas.keys())) for user_id in range(self.n_users)}

    def get_reward(self, user_id, product_id):
        persona = self.user_to_persona_map[user_id]
        category = self.products.loc[product_id, 'category']
        click_prob = self.personas[persona].get(category, 0.01)
        return self.rng.binomial(1, click_prob)

    def introduce_market_shock(self):
        print(f"  > [SIMULATOR] Applying market shock...")
        self.personas['budget_shopper']['Fish Supplies'] = 0.80
        self.personas['budget_shopper']['Dog Food'] = 0.10
        self.personas['budget_shopper']['Cat Food'] = 0.15

class FeatureEngineer:
    """Helper class to create one-hot encoded feature vectors."""
    def __init__(self, sim):
        self.user_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        self.product_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        user_personas = np.array(list(sim.personas.keys())).reshape(-1, 1)
        product_categories = np.array(sim.products['category'].unique()).reshape(-1, 1)
        self.user_encoder.fit(user_personas)
        self.product_encoder.fit(product_categories)
        self.feature_dim = self.user_encoder.get_feature_names_out().shape[0] + \
                           self.product_encoder.get_feature_names_out().shape[0]

    def create_features(self, user_id, sim):
        persona = sim.user_to_persona_map[user_id]
        user_one_hot = self.user_encoder.transform([[persona]])
        all_prod_cats = sim.products['category'].values.reshape(-1, 1)
        prod_one_hots = self.product_encoder.transform(all_prod_cats)
        user_features = np.tile(user_one_hot, (sim.n_products, 1))
        return np.concatenate([user_features, prod_one_hots], axis=1)

class MLPRecommender(nn.Module):
    """The 'Embedding + MLP' baseline model from Chapter 1."""
    def __init__(self, n_users, n_products, embedding_dim=32):
        super().__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.product_embedding = nn.Embedding(n_products, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 64)
        self.fc2 = nn.Linear(64, 32)
        self.output = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, user_ids, product_ids):
        user_embeds = self.user_embedding(user_ids)
        product_embeds = self.product_embedding(product_ids)
        x = torch.cat([user_embeds, product_embeds], dim=1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.output(x)
        return self.sigmoid(x)

def get_batch_model_predictions(model, user_id, n_products, device):
    """Utility function to get the trained batched model's predicted CTR."""
    model.eval()
    with torch.no_grad():
        user_tensor = torch.tensor([user_id] * n_products, dtype=torch.long).to(device)
        product_tensor = torch.tensor(range(n_products), dtype=torch.long).to(device)
        scores = model(user_tensor, product_tensor).squeeze().cpu().numpy()
    return scores

class LinUCBAgent:
    """The REFACTORED LinUCB agent from Chapter 2, conforming to the standard API."""
    def __init__(self, n_products, feature_engineer, alpha=1.0, **kwargs):
        self.n_products = n_products
        self.feature_engineer = feature_engineer
        self.feature_dim = feature_engineer.feature_dim
        self.alpha = alpha
        self.A = [np.identity(self.feature_dim) for _ in range(n_products)]
        self.b = [np.zeros((self.feature_dim, 1)) for _ in range(n_products)]

    def predict(self, user_id, sim):
        feature_vectors = self.feature_engineer.create_features(user_id, sim)
        scores = np.zeros(self.n_products)
        for i in range(self.n_products):
            A_inv = np.linalg.inv(self.A[i])
            theta = A_inv @ self.b[i]
            x = feature_vectors[i].reshape(-1, 1)
            scores[i] = (theta.T @ x).item() + self.alpha * np.sqrt(x.T @ A_inv @ x).item()
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        x = self.feature_engineer.create_features(user_id, sim)[chosen_arm].reshape(-1, 1)
        self.A[chosen_arm] += x @ x.T
        self.b[chosen_arm] += reward * x

class NeuralUCBNetwork_WithHead(nn.Module):
    """The network for our robust Neural-Linear agent, with an explicit prediction head."""
    def __init__(self, input_dim: int, embedding_dim: int = 32, hidden_dim: int = 64):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        self.linear_head = nn.Linear(embedding_dim, 1)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        embedding = self.feature_extractor(x)
        prediction = self.linear_head(embedding)
        return embedding, prediction

class NeuralUCBAgent_Stable:
    """The FINAL, robust version of the Neural-Linear bandit."""
    def __init__(self, feature_engineer, embedding_dim=32, alpha=0.3, lr=0.01,
                 buffer_size=10000, batch_size=64, train_every_steps=50, **kwargs):
        self.feature_engineer = feature_engineer
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        self.buffer_size = buffer_size
        self.batch_size = batch_size
        self.train_every_steps = train_every_steps
        self.model = NeuralUCBNetwork_WithHead(feature_engineer.feature_dim, embedding_dim).to(device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        self.A = np.identity(embedding_dim)
        self.b = np.zeros((embedding_dim, 1))
        self.replay_buffer = collections.deque(maxlen=self.buffer_size)
        self.step_counter = 0

    def predict(self, user_id, sim):
        self.model.eval()
        feature_matrix = self.feature_engineer.create_features(user_id, sim)
        x_all = torch.tensor(feature_matrix, dtype=torch.float32).to(device)
        with torch.no_grad():
            z_all, _ = self.model(x_all)
            z_all_np = z_all.cpu().numpy()
        A_inv = np.linalg.inv(self.A)
        theta_hat = A_inv @ self.b
        scores = (z_all_np @ theta_hat).flatten() + self.alpha * np.sqrt(np.sum((z_all_np @ A_inv) * z_all_np, axis=1))
        return np.argmax(scores)

    def update(self, user_id, chosen_arm, reward, sim):
        self.step_counter += 1
        x_chosen_np = self.feature_engineer.create_features(user_id, sim)[chosen_arm]
        with torch.no_grad():
            z_chosen_np, _ = self.model(torch.tensor(x_chosen_np, dtype=torch.float32).to(device))
            z_chosen_np = z_chosen_np.cpu().numpy().reshape(-1, 1)
        self.A += z_chosen_np @ z_chosen_np.T
        self.b += reward * z_chosen_np
        self.replay_buffer.append((x_chosen_np, reward))
        if self.step_counter % self.train_every_steps == 0:
            self._train_from_batch()

    def _train_from_batch(self):
        if len(self.replay_buffer) < self.batch_size: return
        self.model.train()
        minibatch = random.sample(self.replay_buffer, self.batch_size)
        x_batch, r_batch = zip(*minibatch)
        x_batch_tensor = torch.tensor(np.array(x_batch), dtype=torch.float32).to(device)
        r_batch_tensor = torch.tensor(np.array(r_batch), dtype=torch.float32).to(device).view(-1, 1)
        _, predictions = self.model(x_batch_tensor)
        loss = self.loss_fn(predictions, r_batch_tensor)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

# ==============================================================================
# SECTION 3: STANDARDIZED AGENT WRAPPERS
# ==============================================================================

class RandomAgent:
    """A simple agent that chooses products randomly."""
    def __init__(self, n_products, **kwargs):
        self.n_products = n_products
        self.rng = np.random.default_rng(seed=42)
    def predict(self, user_id, sim): return self.rng.integers(self.n_products)
    def update(self, user_id, chosen_arm, reward, sim): pass

class StaticMLPAgent:
    """A wrapper for the pre-trained MLP model."""
    def __init__(self, model_path, n_users, n_products, **kwargs):
        self.model = MLPRecommender(n_users, n_products).to(device)
        self.model.load_state_dict(torch.load(model_path, map_location=device))
        self.model.eval()
        self.n_products = n_products
    def predict(self, user_id, sim): return np.argmax(get_batch_model_predictions(self.model, user_id, self.n_products, device))
    def update(self, user_id, chosen_arm, reward, sim): pass

# ==============================================================================
# SECTION 4: THE GRAND CHALLENGE EXPERIMENT HARNESS
# ==============================================================================

def create_dummy_mlp_model_if_not_exists(path, n_users, n_products):
    """Creates a dummy model file. This makes the script runnable out-of-the-box."""
    if not os.path.exists(path):
        print(f"⚠️  Model file not found at '{path}'.")
        print("   Creating a dummy (randomly initialized) model to allow script to run.")
        dummy_model = MLPRecommender(n_users, n_products)
        torch.save(dummy_model.state_dict(), path)

if __name__ == '__main__':
    # --- Experiment Setup ---
    print(">>> STAGE 1: SETTING UP THE GRAND CHALLENGE (Corrected) <<<")
    N_STEPS = 40000
    SHOCK_SCHEDULE = {10000: "Shock 1: Budget Shopper loves Fish", 25000: "Shock 2: Cat Connoisseur loves Dog Toys"}
    SEED = 42
    ROLLING_WINDOW = 500
    MLP_MODEL_PATH = "batch_recommender_model_ch2.pth"

    # --- Create dummy model if needed ---
    create_dummy_mlp_model_if_not_exists(MLP_MODEL_PATH, n_users=1000, n_products=50)

    # --- Define all agent configurations ---
    agent_configs = {
        "Random Baseline": {"class": RandomAgent, "params": {"n_products": 50}},
        "Static MLP (Chap 1)": {"class": StaticMLPAgent, "params": {"model_path": MLP_MODEL_PATH, "n_users": 1000, "n_products": 50}},
        "LinUCB (Disjoint)": {"class": LinUCBAgent, "params": {"n_products": 50, "feature_engineer": None, "alpha": 0.2}},
        "NeuralUCB (Stable, Generalizing)": {"class": NeuralUCBAgent_Stable, "params": {"feature_engineer": None, "embedding_dim": 32, "alpha": 0.3, "lr": 0.01}}
    }

    # --- CORRECTED SETUP: Create sandboxed environments for each agent ---
    sim_instances = {name: ZooplusSimulatorShock(seed=SEED) for name in agent_configs.keys()}
    feature_engineers = {name: FeatureEngineer(sim) for name, sim in sim_instances.items()}
    agents = {}
    for name, config in agent_configs.items():
        params = config["params"]
        if "feature_engineer" in params:
            params["feature_engineer"] = feature_engineers[name]
        agents[name] = config["class"](**params)
    print("✅ All agents and their sandboxed environments instantiated.")
    print("-" * 60)

    # --- Pre-generate the controlled user sequence ---
    print(f">>> STAGE 2: PRE-GENERATING USER SEQUENCE ({N_STEPS} users) <<<")
    rng = np.random.default_rng(SEED)
    user_sequence = rng.integers(0, sim_instances["Random Baseline"].n_users, size=N_STEPS)
    print("✅ User sequence created.")
    print("-" * 60)

    # --- The Corrected Simulation Loop ---
    print(">>> STAGE 3: RUNNING SIMULATION GAUNTLET <<<")
    rewards = {name: [] for name in agents.keys()}
    for t in tqdm(range(N_STEPS), desc="Simulating Timesteps"):
        if t in SHOCK_SCHEDULE:
            print(f"\nTime {t}: Introducing {SHOCK_SCHEDULE[t]} to all environments...")
            for sim in sim_instances.values():
                if t == 10000: sim.introduce_market_shock()
                if t == 25000: sim.personas['cat_connoisseur']['Dog Toy'] = 0.75
        user_id = user_sequence[t]
        for name, agent in agents.items():
            sim_env = sim_instances[name]
            chosen_arm = agent.predict(user_id, sim_env)
            reward = sim_env.get_reward(user_id, chosen_arm)
            agent.update(user_id, chosen_arm, reward, sim_env)
            rewards[name].append(reward)
    print("\n✅ Simulation complete.")
    print("-" * 60)

    # --- Processing and Plotting Results ---
    print(">>> STAGE 4: PROCESSING AND PLOTTING FINAL RESULTS <<<")
    ctr_results = {name: pd.Series(r).rolling(window=ROLLING_WINDOW, min_periods=1).mean() for name, r in rewards.items()}
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(16, 9))
    colors = {"Random Baseline": "gray", "Static MLP (Chap 1)": "purple", "LinUCB (Disjoint)": "C0", "NeuralUCB (Stable, Generalizing)": "C1"}
    styles = {"Random Baseline": ":", "Static MLP (Chap 1)": ":", "LinUCB (Disjoint)": "-", "NeuralUCB (Stable, Generalizing)": "-"}
    widths = {"Random Baseline": 1.5, "Static MLP (Chap 1)": 1.5, "LinUCB (Disjoint)": 2.5, "NeuralUCB (Stable, Generalizing)": 2.5}
    for name, ctr_data in ctr_results.items():
        ax.plot(ctr_data, label=name, color=colors[name], linestyle=styles[name], linewidth=widths[name], alpha=0.9)
    # Plot shock lines
    is_first_shock_line = True
    for t_shock, shock_name in SHOCK_SCHEDULE.items():
        ax.axvline(x=t_shock, color='r', linestyle='--', linewidth=2, label='Market Shock' if is_first_shock_line else None)
        is_first_shock_line = False
    ax.set_title('Grand Challenge: A Comparison of Recommender System Paradigms', fontsize=20, pad=20)
    ax.set_xlabel('Timestep', fontsize=14)
    ax.set_ylabel(f'Click-Through Rate (CTR) - {ROLLING_WINDOW}-step Rolling Average', fontsize=14)
    ax.legend(fontsize=12, loc='best')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)
    ax.set_ylim(bottom=0)
    ax.tick_params(axis='both', which='major', labelsize=12)
    plt.tight_layout()
    plt.show()

    # --- Final Performance Summary ---
    print("-" * 60)
    print(">>> FINAL PERFORMANCE ANALYSIS <<<")
    for name, r in rewards.items():
        overall_ctr = np.mean(r)
        post_shock1_ctr = np.mean(r[10000:25000])
        post_shock2_ctr = np.mean(r[25000:])
        print(f"\n--- Performance for: {name} ---")
        print(f"  Overall CTR (steps 0-{N_STEPS}): {overall_ctr:.4f}")
        print(f"  CTR after Shock 1 (steps 10k-25k): {post_shock1_ctr:.4f}")
        print(f"  CTR after Shock 2 (steps 25k-40k): {post_shock2_ctr:.4f}")
    print("-" * 60)
```

    ✅ Using MPS (Apple Silicon) device for PyTorch.
    >>> STAGE 1: SETTING UP THE GRAND CHALLENGE (Corrected) <<<
    ✅ All agents and their sandboxed environments instantiated.
    ------------------------------------------------------------
    >>> STAGE 2: PRE-GENERATING USER SEQUENCE (40000 users) <<<
    ✅ User sequence created.
    ------------------------------------------------------------
    >>> STAGE 3: RUNNING SIMULATION GAUNTLET <<<


    Simulating Timesteps:  25%|██▌       | 10043/40000 [00:34<01:32, 323.72it/s]

    
    Time 10000: Introducing Shock 1: Budget Shopper loves Fish to all environments...
      > [SIMULATOR] Applying market shock...
      > [SIMULATOR] Applying market shock...
      > [SIMULATOR] Applying market shock...
      > [SIMULATOR] Applying market shock...


    Simulating Timesteps:  63%|██████▎   | 25062/40000 [01:24<00:47, 315.19it/s]

    
    Time 25000: Introducing Shock 2: Cat Connoisseur loves Dog Toys to all environments...


    Simulating Timesteps: 100%|██████████| 40000/40000 [02:14<00:00, 296.77it/s]


    
    ✅ Simulation complete.
    ------------------------------------------------------------
    >>> STAGE 4: PROCESSING AND PLOTTING FINAL RESULTS <<<



    
![png](Chapter%203%20v3_files/Chapter%203%20v3_26_7.png)
    


    ------------------------------------------------------------
    >>> FINAL PERFORMANCE ANALYSIS <<<
    
    --- Performance for: Random Baseline ---
      Overall CTR (steps 0-40000): 0.2253
      CTR after Shock 1 (steps 10k-25k): 0.2091
      CTR after Shock 2 (steps 25k-40k): 0.2534
    
    --- Performance for: Static MLP (Chap 1) ---
      Overall CTR (steps 0-40000): 0.4564
      CTR after Shock 1 (steps 10k-25k): 0.4411
      CTR after Shock 2 (steps 25k-40k): 0.4417
    
    --- Performance for: LinUCB (Disjoint) ---
      Overall CTR (steps 0-40000): 0.5257
      CTR after Shock 1 (steps 10k-25k): 0.4943
      CTR after Shock 2 (steps 25k-40k): 0.5869
    
    --- Performance for: NeuralUCB (Stable, Generalizing) ---
      Overall CTR (steps 0-40000): 0.5139
      CTR after Shock 1 (steps 10k-25k): 0.5109
      CTR after Shock 2 (steps 25k-40k): 0.6589
    ------------------------------------------------------------



```python

```


```python

```
